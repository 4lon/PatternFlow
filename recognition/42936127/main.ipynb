{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/caelanreid/PatternFlow/tree/topic-recognition/recognition/42936127\n",
    "^REPO\n",
    "\n",
    "\n",
    "VQVAE Implementation adapted from this tutorial from keras https://keras.io/examples/generative/vq_vae/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11328 files belonging to 1 classes.\n",
      "Using 7930 files for training.\n",
      "Using 3398 files for validation.\n",
      "loaded model\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "seed = 123\n",
    "batch_size = 64\n",
    "img_height = 256\n",
    "img_width = 256\n",
    "image_shape = (img_height, img_width, 3)\n",
    "\n",
    "from modules import *\n",
    "\n",
    "load_model = True\n",
    "\n",
    "\n",
    "train_ds, val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    \"keras_png_slices_data/slices/\", \n",
    "    labels = None,\n",
    "    validation_split = 0.3,\n",
    "    subset = \"both\",\n",
    "    seed = seed,\n",
    "    image_size = (img_height, img_width)\n",
    ")\n",
    "\n",
    "\n",
    "if(load_model):\n",
    "    vqvae_trainer = VQVAETrainer(0.05, latent_dim=16, num_embeddings=128, image_shape = image_shape)\n",
    "    vqvae_trainer.vqvae = keras.models.load_model(\"saved_models\")\n",
    "    vqvae_trainer.compile(optimizer=keras.optimizers.Adam())\n",
    "    print(\"loaded model\")\n",
    "else:\n",
    "    #data_variance = tf.math.reduce_variance(train_ds)\n",
    "    vqvae_trainer = VQVAETrainer(0.05, latent_dim=16, num_embeddings=128, image_shape = image_shape)\n",
    "    vqvae_trainer.compile(optimizer=keras.optimizers.Adam())\n",
    "    vqvae_trainer.fit(\n",
    "        x = train_ds,\n",
    "        validation_data = val_ds,\n",
    "        epochs = 1,\n",
    "        use_multiprocessing = True,\n",
    "        verbose = 1\n",
    "    )\n",
    "\n",
    "    vqvae_trainer.vqvae.save(\"saved_models\")\n",
    "\n",
    "    vqvae = vqvae_trainer.vqvae\n",
    "    print(\"trained and saved model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "Tensor(\"vqvae_1/StatefulPartitionedCall:0\", shape=(None, 256, 256, 1), dtype=float32)\n",
      "Tensor(\"vqvae_1/StatefulPartitionedCall:0\", shape=(None, 256, 256, 1), dtype=float32)\n",
      "248/248 [==============================] - 23s 92ms/step - loss: 2635.1303 - reconstruction_loss: 1837.2906 - vqvae_loss: 9.8869e-04 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_vq_loss: 0.0000e+00\n",
      "Epoch 2/6\n",
      "248/248 [==============================] - 23s 92ms/step - loss: 811.9381 - reconstruction_loss: 773.9625 - vqvae_loss: 9.6280e-04 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_vq_loss: 0.0000e+00\n",
      "Epoch 3/6\n",
      "248/248 [==============================] - 23s 93ms/step - loss: 698.4777 - reconstruction_loss: 681.5889 - vqvae_loss: 9.5595e-04 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_vq_loss: 0.0000e+00\n",
      "Epoch 4/6\n",
      "248/248 [==============================] - 23s 92ms/step - loss: 645.7414 - reconstruction_loss: 632.8278 - vqvae_loss: 9.5216e-04 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_vq_loss: 0.0000e+00\n",
      "Epoch 5/6\n",
      "248/248 [==============================] - 22s 90ms/step - loss: 599.5610 - reconstruction_loss: 590.7867 - vqvae_loss: 9.2903e-04 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_vq_loss: 0.0000e+00\n",
      "Epoch 6/6\n",
      "248/248 [==============================] - 24s 95ms/step - loss: 571.4910 - reconstruction_loss: 564.9542 - vqvae_loss: 9.2264e-04 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_vq_loss: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as rescaling_1_layer_call_fn, rescaling_1_layer_call_and_return_conditional_losses, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models/assets\n"
     ]
    }
   ],
   "source": [
    "vqvae_trainer.fit(\n",
    "    x = train_ds,\n",
    "    validation_data = val_ds,\n",
    "    epochs = 6,\n",
    "    use_multiprocessing = True,\n",
    "    verbose = 1\n",
    ")\n",
    "vqvae.save(\"saved_models\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
