{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "https://thispersondoesnotexist.com/\n",
    "\n",
    "https://github.com/NVlabs/stylegan\n",
    "https://arxiv.org/abs/1812.04948\n",
    "https://github.com/NVlabs/stylegan2\n",
    "https://arxiv.org/abs/1912.04958\n",
    "\n",
    "https://machinelearningmastery.com/introduction-to-style-generative-adversarial-network-stylegan/\n",
    "StyleGan | Lecture 71 (Part 1) | Applied Deep Learning\n",
    "https://www.youtube.com/watch?v=hfFAUFsglLc\n",
    "AI generated faces - StyleGAN explained!\n",
    "https://www.youtube.com/watch?v=4LNO8nLxF4Y\n",
    "\n",
    "Affine appears to be == self-similar transformation via width/height shift, shear, zoom, etc.?\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/apply_affine_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.5.0\n",
      "Tensorflow CUDA is available.\n",
      "Tensorflow set GPU memory growth to True.\n",
      "Tensorflow is executing eagerly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Suppress tensorflow logging:\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(f'Tensorflow version: {tf.__version__}')\n",
    "print(f'Tensorflow CUDA {\"is\" if tf.test.is_built_with_cuda() else \"is not\"} available.')\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print('Tensorflow set GPU memory growth to True.')\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "print(f'Tensorflow {\"is\" if tf.executing_eagerly() else \"is not\"} executing eagerly.')\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.layers import Input, Add, Dense, Flatten, Reshape, LeakyReLU, Conv2D, Conv2DTranspose\n",
    "from tensorflow.keras.layers import Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling, Normalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import imageio\n",
    "import time\n",
    "\n",
    "dark1 = '#191b26'\n",
    "dark2 = '#151722'\n",
    "white1 = '#fff'\n",
    "white2 = '#b3b5be'\n",
    "red1 = '#cd6152'\n",
    "red2 = '#cd6152'\n",
    "green1 = '#67a39a'\n",
    "green2 = '#253037'\n",
    "blue1 = '#4a64fd'\n",
    "blue2 = '#1b1f39'\n",
    "\n",
    "mpl.rcParams['text.color'] = white2\n",
    "mpl.rcParams['axes.labelcolor'] = white2\n",
    "mpl.rcParams['axes.facecolor'] = dark1\n",
    "mpl.rcParams['axes.edgecolor'] = white2\n",
    "mpl.rcParams['figure.facecolor'] = dark2\n",
    "mpl.rcParams['xtick.color'] = white2\n",
    "mpl.rcParams['ytick.color'] = white2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Define constants.\n",
    "BATCH_SIZE = 1024\n",
    "IMAGE_WIDTH, IMAGE_HEIGHT = 28, 28\n",
    "IMAGE_SIZE = (IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "\n",
    "GRAPH_INTERVAL = 50\n",
    "PRINT_INTERVAL = 25\n",
    "\n",
    "EPOCHS = 999\n",
    "LINEAR_SIZE = 4*4*512       # Size of Dense layer in Generator.\n",
    "LATENT_DIMS = 100           # Number of random numbers to use at Generator input.\n",
    "NUM_GENERATED_SAMPLES = 8   # Number of sample fake images to generate to visualise training.\n",
    "\n",
    "# This set of latent inputs will be used for each training graph output,\n",
    "# so we can visualise progress on the same latent samples.\n",
    "SEED = tf.random.normal([NUM_GENERATED_SAMPLES, LATENT_DIMS])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataloader = tfds.load('mnist', as_supervised=True)\n",
    "image_dataset = dataloader['train']\n",
    "# Cast [0,255] images to [-1,1].\n",
    "image_dataset = image_dataset.map(lambda image, label: Rescaling(scale=1./127.5, offset=-1)(image))\n",
    "image_dataset = image_dataset.shuffle(BATCH_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "batch = image_dataset.take(1)\n",
    "image = list(batch.as_numpy_iterator())[0][0]\n",
    "\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Set up generator and discrimator.\n",
    "Display sample fake image from generator (untrained).\n",
    "Display sample prediction from discriminator (untrained)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def make_generator_model():\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "\n",
    "    inputs = Input(shape=(4*4*512,), dtype=tf.float32)\n",
    "    reshape1 = Reshape(target_shape=(4,4,512))(inputs)\n",
    "\n",
    "    conv1 = Conv2DTranspose(filters=128, kernel_size=(3,3), strides=(2,2),\n",
    "                            padding='same', kernel_initializer=init)(reshape1)\n",
    "    act1 = LeakyReLU(alpha=0.2)(conv1)\n",
    "    # assert ??\n",
    "\n",
    "    conv2 = Conv2DTranspose(filters=1, kernel_size=(3,3), strides=(2,2),\n",
    "                            padding='same', kernel_initializer=init, activation='tanh')(act1)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=conv2, name='synthesiser')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def make_discriminator_model():\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(32, (4,4), strides=(2,2), padding='same', kernel_initializer=init,\n",
    "                            input_shape=(IMAGE_WIDTH,IMAGE_HEIGHT,1)))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Conv2D(32, (4,4), strides=(2,2), padding='same', kernel_initializer=init))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Conv2D(32, (4,4), strides=(2,2), padding='same', kernel_initializer=init))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def generate_latent_coordinates(batches, dimensions):\n",
    "    return tf.random.normal([batches, dimensions])\n",
    "\n",
    "generator = make_generator_model()\n",
    "discriminator = make_discriminator_model()\n",
    "\n",
    "ones = tf.ones((1, 4*4*512))\n",
    "\n",
    "# Demonstrate generator and discriminator on 1 random noise sample.\n",
    "generated_image = generator(ones, training=False)\n",
    "# decision = discriminator(generated_image)\n",
    "img = generated_image.numpy()[0]\n",
    "# plt.title(f'Decision: {decision[0][0]}')\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "def disc_loss(real_output, fake_output):\n",
    "    \"\"\"\n",
    "    How well the discriminator can distinguish real from fake images.\n",
    "    :param real_output: Classifications of real images.\n",
    "    :param fake_output: Classifications of fake images.\n",
    "    :return: Loss on real images, loss on fake images.\n",
    "    \"\"\"\n",
    "    # Compare predictions on real images to array of ones.\n",
    "    real_loss = BinaryCrossentropy()(tf.ones_like(real_output), real_output)\n",
    "    # Compare predictions on fake images to array of zeroes.\n",
    "    fake_loss = BinaryCrossentropy()(tf.zeros_like(fake_output), fake_output)\n",
    "    return real_loss, fake_loss\n",
    "\n",
    "\n",
    "def gen_loss(fake_output):\n",
    "    \"\"\"\n",
    "    How well the generator can fool the discriminator.\n",
    "    We get this loss from the discriminator, and pass it to the generator.\n",
    "    :param fake_output: Discriminator classification of fake images.\n",
    "    :return: Loss of discriminator on fake images, inverted.\n",
    "    \"\"\"\n",
    "    # Compare discriminator decisions to array of ones.\n",
    "    return BinaryCrossentropy()(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "\n",
    "gen_optimizer = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "disc_optimizer = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "\n",
    "checkpoint_dir = 'C:/Pred_Models'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=gen_optimizer,\n",
    "                                 discriminator_optimizer=disc_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_gan_progress(g_losses, d_real_losses, d_fake_losses, model, epoch, batch, test_input):\n",
    "    \"\"\"\n",
    "    Plot tensorflow train and validation metrics and loss in one chart.\n",
    "    :param history: Dictionary containing the accuracy and loss history.\n",
    "    \"\"\"\n",
    "    predictions = model(test_input, training=False)\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 11))\n",
    "    fig.suptitle(f'Epoch {epoch:03} | Batch {batch:03}')\n",
    "\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(3, 4, i + 1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(predictions[i].numpy(), cmap='gray')\n",
    "\n",
    "    plt.subplot(3, 4, (9,12))\n",
    "    plt.title(f'GAN generator and discriminator losses')\n",
    "    plt.xlabel('Batch')\n",
    "    plt.ylabel('Loss')\n",
    "    if len(g_losses) > 50:\n",
    "        plt.plot(g_losses[len(g_losses)-50:], c=green1, label=f'gen loss')\n",
    "        plt.plot(d_real_losses[len(d_real_losses)-50:], c='purple', label=f'disc real loss')\n",
    "        plt.plot(d_fake_losses[len(d_fake_losses)-50:], c='#ff884d', label=f'disc fake loss')\n",
    "    else:\n",
    "        plt.plot(g_losses, c=green1, label=f'gen loss')\n",
    "        plt.plot(d_real_losses, c='purple', label=f'disc real loss')\n",
    "        plt.plot(d_fake_losses, c='#ff884d', label=f'disc fake loss')\n",
    "    plt.tight_layout()\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "    plt.savefig(f'C:/Pred_Images/preds_at_epoch{epoch:04}_batch{batch:04}.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    \"\"\"\n",
    "    Conduct forward and backward pass, updating weights of each model\n",
    "    by their loss.\n",
    "    :param images: Batch of images (either real or fake)\n",
    "    :return: Losses, for plotting only.\n",
    "    \"\"\"\n",
    "    # Random latent space input for generator.\n",
    "    noise = generate_latent_coordinates(BATCH_SIZE, LATENT_DIMS)\n",
    "\n",
    "    # Track gradients of each model.\n",
    "    # ie. Track what happened in what order during forward pass.\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        # Forward pass.\n",
    "        generated_images = generator(noise, training=True)\n",
    "\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        g_loss = gen_loss(fake_output)\n",
    "        d_real_loss, d_fake_loss = disc_loss(real_output, fake_output)\n",
    "        d_loss = d_real_loss + d_fake_loss\n",
    "\n",
    "    # Backward pass.\n",
    "    # Calculate gradient for each models trainable weights.\n",
    "    gradients_of_generator = gen_tape.gradient(g_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "\n",
    "    # Update generator and discriminator weights with gradients.\n",
    "    gen_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    disc_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "    return g_loss, d_real_loss, d_fake_loss\n",
    "\n",
    "\n",
    "def train(dataset, epochs):\n",
    "    g_losses, d_real_losses, d_fake_losses = [], [], []\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        for i, image_batch in enumerate(dataset):\n",
    "            # Start training step, tracking losses.\n",
    "            g_loss, d_real_loss, d_fake_loss = train_step(image_batch)\n",
    "            g_loss, d_real_loss, d_fake_loss = g_loss.numpy(), d_real_loss.numpy(), d_fake_loss.numpy()\n",
    "\n",
    "            # Update output graph every n batches.\n",
    "            if i % GRAPH_INTERVAL == 0:\n",
    "                g_losses.append(g_loss)\n",
    "                d_real_losses.append(d_real_loss)\n",
    "                d_fake_losses.append(d_fake_loss)\n",
    "                display.clear_output(wait=True)\n",
    "                plot_gan_progress(g_losses, d_real_losses, d_fake_losses, generator, epoch, i, SEED)\n",
    "            if i % PRINT_INTERVAL == 0:\n",
    "                print(f'Epoch {epoch:04}: batch {i:04}/{len(dataset)} | g_loss: {g_loss} | '\n",
    "                      f'd_fake_loss: {d_fake_loss} | d_real_loss: {d_real_loss}', end='\\r')\n",
    "\n",
    "        # Save the model every 15 epochs\n",
    "        if (epoch + 1) % 15 == 0:\n",
    "            checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "\n",
    "        print('Time for epoch {} is {} sec\\n'.format(epoch + 1, time.time() - start))\n",
    "\n",
    "\n",
    "train(image_dataset, EPOCHS)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# checkpoint_dir = 'C:/Pred_Models'\n",
    "# checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "# checkpoint = tf.train.Checkpoint(generator_optimizer=gen_optimizer,\n",
    "#                                  discriminator_optimizer=disc_optimizer,\n",
    "#                                  generator=generator,\n",
    "#                                  discriminator=discriminator)\n",
    "#\n",
    "# checkpoint.restore(\"C:/Pred_models/ckpt-16.index\")\n",
    "\n",
    "predictions = generator(generate_latent_coordinates(1, 100), training=False)\n",
    "plt.axis('off')\n",
    "plt.imshow(predictions[0].numpy(), cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "predictions = generator(generate_latent_coordinates(1, 100), training=False)\n",
    "plt.axis('off')\n",
    "plt.imshow(predictions[0].numpy(), cmap='gray')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "base",
   "language": "python",
   "display_name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}