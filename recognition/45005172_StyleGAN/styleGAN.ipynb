{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from PIL import Image\n",
    "import math\n",
    "import os\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 256\n",
    "LATENT_SIZE = 512\n",
    "BATCH_SIZE = 12\n",
    "\n",
    "LAYERS = int(math.log2(IMG_SIZE) - 1)\n",
    "MIX_PROB = 0.9\n",
    "CHA = 48\n",
    "\n",
    "INITIALIZER = 'he_normal'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise(num):\n",
    "    return np.random.normal(0.0, 1.0, size = [num, LATENT_SIZE]).astype('float32')\n",
    "\n",
    "def get_noise(num):\n",
    "    return [noise(num)] * LAYERS\n",
    "\n",
    "def get_mixed_noise(num):\n",
    "    rand = int(random.random() * LAYERS)\n",
    "    p1 = [noise(num)] * rand\n",
    "    p2 = [noise(num)] * (LAYERS - rand)\n",
    "    return p1 + [] + p2\n",
    "\n",
    "def img_dim(size):\n",
    "    return np.random.uniform(0.0, 1.0, size = [size, IMG_SIZE, IMG_SIZE, 1]).astype('float32')\n",
    "\n",
    "def pixel_norm(x, epsilon = 1e-7):\n",
    "    mean = tf.keras.backend.mean(x, axis=[1, 2], keepdims=True)\n",
    "    std = tf.keras.backend.std(x, axis=[1, 2], keepdims=True) + epsilon\n",
    "    return (x - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_loss(sample, output, weights):\n",
    "    grad = tf.keras.backend.gradients(output, sample)[0]\n",
    "    grad_sq = tf.keras.backend.square(grad)\n",
    "    grad_loss = tf.keras.backend.sum(grad_sq, axis=np.arange(1, len(grad_sq.shape)))\n",
    "    return tf.keras.backend.mean(grad_loss * weights)\n",
    "\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return tf.keras.backend.mean(y_true * y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fade_in(alpha, a, b):\n",
    "    alpha = tf.reshape(alpha, [-1, 1, 1, 1])\n",
    "    alpha = tf.clip(alpha - b, 0, 1)\n",
    "    return a * alpha + ((1.0 - alpha) * (1.0 - a))\n",
    "\n",
    "def AdaIN(input_shapes):\n",
    "    y = pixel_norm(input_shapes[0])\n",
    "    #shape = [-1, 1, 1, y.shape[-1]]\n",
    "    scale = tf.reshape(input_shapes[1], (-1, 1, 1, y.shape[-1])) + 1.0\n",
    "    bias = tf.reshape(input_shapes[2], (-1, 1, 1, y.shape[-1]))\n",
    "    return y * scale + bias \n",
    "\n",
    "def fit(x):\n",
    "    h = x[1].shape[1]\n",
    "    w = x[1].shape[2]\n",
    "    return x[0][:, :h, :w, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gen_block(input_tensor, style, inoise, filters, up_sample = True):\n",
    "    if up_sample:\n",
    "        block = tf.keras.layers.UpSampling2D()(input_tensor)\n",
    "    else:\n",
    "        block = tf.keras.layers.Activation('linear')(input_tensor)\n",
    "\n",
    "    beta = tf.keras.layers.Dense(filters)(style)\n",
    "    delta = tf.keras.layers.Lambda(fit)([inoise, block])\n",
    "    delta = tf.keras.layers.Dense(filters, kernel_initializer='zeros')(delta)\n",
    "    gamma = tf.keras.layers.Dense(filters)(style)\n",
    "\n",
    "    block = tf.keras.layers.Conv2D(filters=filters, kernel_size=3, padding='same', \\\n",
    "        kernel_initializer='he_normal')(block)\n",
    "    block = tf.keras.layers.add([block, delta])\n",
    "    block = tf.keras.layers.Lambda(AdaIN)([block, gamma, beta])\n",
    "\n",
    "    return tf.keras.layers.LeakyReLU(0.2)(block)\n",
    "\n",
    "def get_desc_block(input_tensor, fil, pool = True):\n",
    "    block = tf.keras.layers.Conv2D(filters=fil, kernel_size=3, \\\n",
    "        padding='same', kernel_initializer='he_normal')(input_tensor)\n",
    "    block = tf.keras.layers.LeakyReLU(0.2)(block)\n",
    "\n",
    "    if pool:\n",
    "        block = tf.keras.layers.AveragePooling2D()(block)\n",
    "\n",
    "    return block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(object):\n",
    "    def __init__(self, steps = 1, learn_rate = 1e-4, decay = 1e-5):\n",
    "        self.desc = None\n",
    "        self.gen = None\n",
    "        self.style = None\n",
    "\n",
    "        self.g_model = None\n",
    "\n",
    "        self.L_Rate = learn_rate\n",
    "        self.steps = steps\n",
    "        self.beta = 0.99\n",
    "\n",
    "        self.discriminator()\n",
    "        self.generator()\n",
    "\n",
    "        # self.g_model = tf.keras.models.model_from_json(self.gen.to_json())\n",
    "        # self.g_model.set_weights(self.gen.get_weights())\n",
    "        \n",
    "        # self.s_model = tf.keras.models.model_from_json(self.style.to_json())\n",
    "        # self.s_model.set_weights(self.style.get_weights())\n",
    "\n",
    "    def discriminator(self):\n",
    "        if self.desc:\n",
    "            return self.desc\n",
    "        \n",
    "        input_tensor = tf.keras.layers.Input(shape = [IMG_SIZE, IMG_SIZE, 3])\n",
    "\n",
    "        #x = tf.keras.layers.Input(shape = [IMG_SIZE, IMG_SIZE, 3])\n",
    "        x = get_desc_block(input_tensor, 1*CHA)\n",
    "        x = get_desc_block(x, 2*CHA)\n",
    "        x = get_desc_block(x, 3*CHA)\n",
    "        x = get_desc_block(x, 4*CHA)\n",
    "        x = get_desc_block(x, 6*CHA)\n",
    "        x = get_desc_block(x, 8*CHA)\n",
    "        x = get_desc_block(x, 16*CHA, False)\n",
    "        x = tf.keras.layers.Flatten()(x)\n",
    "        x = tf.keras.layers.Dense(16*CHA, kernel_initializer='he_normal')(x)\n",
    "        x = tf.keras.layers.LeakyReLU(0.2)(x)\n",
    "        x = tf.keras.layers.Dense(1, kernel_initializer='he_normal')(x)\n",
    "\n",
    "        self.desc = tf.keras.models.Model(inputs=input_tensor, outputs=x)\n",
    "\n",
    "        return self.desc\n",
    "\n",
    "    def generator(self):\n",
    "        if self.gen:\n",
    "            return self.gen\n",
    "        \n",
    "        # Style Mapping\n",
    "        self.style = tf.keras.Sequential()\n",
    "        self.style.add(tf.keras.layers.Dense(512, input_shape=[LATENT_SIZE]))\n",
    "        self.style.add(tf.keras.layers.LeakyReLU(0.2))\n",
    "        self.style.add(tf.keras.layers.Dense(512))\n",
    "        self.style.add(tf.keras.layers.LeakyReLU(0.2))\n",
    "        self.style.add(tf.keras.layers.Dense(512))\n",
    "        self.style.add(tf.keras.layers.LeakyReLU(0.2))\n",
    "        self.style.add(tf.keras.layers.Dense(512))\n",
    "        self.style.add(tf.keras.layers.LeakyReLU(0.2))  \n",
    "            # tf.keras.layers.Dense(512, input_shape=[LATENT_SIZE]),\n",
    "            # tf.keras.layers.LeakyReLU(0.2),\n",
    "            # tf.keras.layers.Dense(512),\n",
    "            # tf.keras.layers.LeakyReLU(0.2),\n",
    "            # tf.keras.layers.Dense(512),\n",
    "            # tf.keras.layers.LeakyReLU(0.2),\n",
    "            # tf.keras.layers.Dense(512),\n",
    "            # tf.keras.layers.LeakyReLU(0.2)\n",
    "\n",
    "        # Actual Generator\n",
    "        input_style = []\n",
    "\n",
    "        for i in range(LAYERS):\n",
    "            input_style.append(tf.keras.Input([LATENT_SIZE]))\n",
    "\n",
    "        input_noise = tf.keras.layers.Input([IMG_SIZE, IMG_SIZE, 1])\n",
    "\n",
    "        x = tf.keras.layers.Lambda(lambda x: x[:, :128])(input_style[0])\n",
    "        x = tf.keras.layers.Dense(4*4*4*CHA, activation='relu', kernel_initializer='he_normal')(x)\n",
    "        x = tf.keras.layers.Reshape([4, 4, 4*CHA])(x)\n",
    "        x = get_gen_block(x, input_style[0], input_noise, 16*CHA, up_sample=False)\n",
    "        x = get_gen_block(x, input_style[1], input_noise, 8*CHA)\n",
    "        x = get_gen_block(x, input_style[2], input_noise, 6*CHA)\n",
    "        x = get_gen_block(x, input_style[3], input_noise, 4*CHA)\n",
    "        x = get_gen_block(x, input_style[4], input_noise, 3*CHA)\n",
    "        x = get_gen_block(x, input_style[5], input_noise, 2*CHA)\n",
    "        x = get_gen_block(x, input_style[6], input_noise, 1*CHA)\n",
    "        x = tf.keras.layers.Conv2D(filters=3, kernel_size=1, padding='same', kernel_initializer='he_normal')(x)\n",
    "\n",
    "        self.gen = tf.keras.models.Model(inputs = input_style + [input_noise], outputs = x)\n",
    "\n",
    "        return self.gen\n",
    "    \n",
    "    def gen_model(self):\n",
    "        input_style = []\n",
    "        style = []\n",
    "\n",
    "        for i in range(LAYERS):\n",
    "            input_style.append(tf.keras.layers.Input([LATENT_SIZE]))\n",
    "            style.append(self.style(input_style[-1]))\n",
    "\n",
    "        input_noise = tf.keras.layers.Input([IMG_SIZE, IMG_SIZE, 1])\n",
    "\n",
    "        x = self.gen(style+[input_noise])\n",
    "        self.g_model = tf.keras.models.Model(inputs = input_style + [input_noise], outputs = x)\n",
    "\n",
    "        return self.g_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Optimisers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4, beta_1=0, beta_2=0.9)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=4*1e-4, beta_1=0, beta_2=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleGAN(object):\n",
    "    def __init__(self, steps = 1, learn_rate = 1e-4, decay = 1e-5):\n",
    "        self.GAN = GAN(steps = steps, learn_rate = learn_rate, decay = decay)\n",
    "        \n",
    "        self.generator = self.GAN.gen_model()\n",
    "        self.discriminiator = self.GAN.discriminator()\n",
    "        \n",
    "        self.weight = np.array([10] * BATCH_SIZE).astype('float32')\n",
    "\n",
    "    def train(self, train_set):\n",
    "        # Randomly train alternating styles\n",
    "        if random.random() < MIX_PROB:\n",
    "            style = get_mixed_noise(BATCH_SIZE)\n",
    "        else:\n",
    "            style = get_noise(BATCH_SIZE)\n",
    "\n",
    "        \n",
    "        d_loss, g_loss, div = self.train_step(train_set.astype('float32'), style, img_dim(12), self.weight)\n",
    "\n",
    "        new_weight = 5/(np.array(div) + 1e-7)\n",
    "        self.weight = self.weight[0] * 0.9 + 0.1 * new_weight\n",
    "        self.weight = np.clip([self.weight] * BATCH_SIZE, 0.01, 10000.0).astype('float32')\n",
    "\n",
    "        # Print progress after models after 100 steps\n",
    "        if self.GAN.steps%1 == 0:\n",
    "            print(\"\\n==============================\")\n",
    "            print(\"Epoch: \", self.GAN.steps)\n",
    "            print(\"Discriminator Loss: \", d_loss)\n",
    "            print(\"Generator Loss: \", g_loss)\n",
    "            print(\"==============================\\n\")\n",
    "\n",
    "            #Save images in /Generated-img after every 500 epochs\n",
    "            if self.GAN.steps%500 == 0:\n",
    "                self.save_image(self.GAN.steps/500)\n",
    "\n",
    "        self.GAN.steps += 1\n",
    "        if self.GAN.steps < 2:\n",
    "            print(self.GAN.steps)\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, images, style, noise, weight):\n",
    "        # =======================DEBUG===============\n",
    "        print(images.shape)\n",
    "        with tf.GradientTape() as g_tape, tf.GradientTape() as d_tape:\n",
    "            generated_img = self.GAN.g_model(style + [noise], training=True)\n",
    "            real_output = self.GAN.desc(images, training=True)\n",
    "            generated_output = self.GAN.desc(generated_img, training=True)\n",
    "\n",
    "            generator_loss = tf.keras.backend.mean(generated_output)\n",
    "            divergence = tf.keras.backend.mean(tf.keras.backend.relu(1+real_output) \\\n",
    "                + tf.keras.backend.relu(1-generated_output))\n",
    "            discriminator_loss = divergence + gradient_loss(images, real_output, weight)\n",
    "\n",
    "        gradients_of_generator = g_tape.gradient(generator_loss, self.GAN.g_model.trainable_variables)\n",
    "        gradients_of_discriminator = d_tape.gradient(discriminator_loss, \\\n",
    "            self.GAN.desc.trainable_variables)\n",
    "\n",
    "        generator_optimizer.apply_gradients(zip(gradients_of_generator, \\\n",
    "            self.GAN.g_model.trainable_variables))\n",
    "        discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, \\\n",
    "            self.GAN.desc.trainable_variables))\n",
    "        \n",
    "        return discriminator_loss, generator_loss, divergence\n",
    "\n",
    "    def save_image(self, image_num):\n",
    "        noise1 = get_noise(64)\n",
    "        noise2 = img_dim(64)\n",
    "\n",
    "        # g_model = self.GAN.gen_model()\n",
    "        generated_images = self.GAN.g_model.predict(noise1 + [noise2], batch_size = BATCH_SIZE)\n",
    "\n",
    "        result = []\n",
    "\n",
    "        # for i in range(0, 64, 8):\n",
    "        #     result.append(np.concatenate(generated_images[i:i+8], axis=1))\n",
    "        result.append(np.concatenate(generated_images[0:1], axis = 1))\n",
    "        x = np.concatenate(result, axis = 0)\n",
    "        x = np.clip(x, 0.0, 1.0)\n",
    "\n",
    "        images = Image.fromarray(np.uint8(x*255))\n",
    "        #images = Image.fromarray(np.array(x))\n",
    "\n",
    "        #np.save(\"Generated_img/img-\"+str(image_num)+\".png\", images)\n",
    "        images.save(\"Generated_img/img-\"+str(image_num)+\".png\")\n",
    "        if image_num == 0:\n",
    "            plt.imshow(images)\n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert OASIS Brain data to .npy\n",
    "Converts the OASIS Brain .png images to .npy arrays for training efficiency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data directory for OASIS Brain \n",
    "def convert_to_npy(dir_path):\n",
    "    segment_length = (1024 ** 3) // (IMG_SIZE*IMG_SIZE*3)\n",
    "\n",
    "    file_names = []\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(dir_path):\n",
    "        for filename in filenames:\n",
    "            file_names.append(os.path.join(dirpath, filename))\n",
    "\n",
    "    np.random.shuffle(file_names)\n",
    "    \n",
    "    segment = []\n",
    "    ctr = 0\n",
    "    kn = 0\n",
    "    for fname in file_names:\n",
    "        img = Image.open(fname).convert(\"RGB\").resize((IMG_SIZE, IMG_SIZE), Image.BILINEAR)\n",
    "        img = np.array(img, dtype='uint8')\n",
    "        segment.append(img)\n",
    "        kn += 1\n",
    "\n",
    "        if kn >= segment_length:\n",
    "            np.save(\"OASIS-Brain-npy/image-\" + str(ctr) + \".npy\", np.array(segment))\n",
    "            segment = []\n",
    "            kn = 0\n",
    "            ctr += 1\n",
    "\n",
    "    np.save(\"OASIS-Brain-npy/image-\" + str(ctr) + \".npy\", np.array(segment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If data nor loaded as .npy run the next block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Uncomment this block to run code to convert image folder to .npy folders\n",
    "# path to original image directory will go to 'data_dir'\n",
    "\n",
    "# data_dir = \"keras_png_slices_data/keras_png_slices_train\"\n",
    "# convert_to_npy(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If folder with .npy exists run next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1184 images.\n"
     ]
    }
   ],
   "source": [
    "def load_data(data_path):\n",
    "    segment = []\n",
    "    images = []\n",
    "    for dirpath, dirnames, filenames in os.walk(data_path):\n",
    "        for filename in filenames:\n",
    "            segment.append(os.path.join(dirpath, filename))\n",
    "    \n",
    "    index = random.randint(0, len(segment) - 1)\n",
    "    images = np.load(segment[index])\n",
    "    return images\n",
    "\n",
    "images = load_data('OASIS-Brain-npy')\n",
    "print(str(len(images)) + \" images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Training Images\n",
    "### Training to be done on number of images = BATCH_SIZE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_batch(images, update):\n",
    "    if update > images.shape[0]:\n",
    "        images = load_data('OASIS-Brain-npy')\n",
    "\n",
    "    #randomly select #BATCH_SIZE numbers\n",
    "    print(images.shape)\n",
    "    indeces = np.random.randint(0, images.shape[0] - 1, BATCH_SIZE)\n",
    "    train_set = []\n",
    "    for i in indeces:\n",
    "        train_set.append(images[i])\n",
    "\n",
    "    return np.array(train_set).astype('float32') / 255.0\n",
    "# image_indices = np.random.randint(0, train.shape[0] - 1, [4])\n",
    "# real_images = train[image_indices]\n",
    "\n",
    "# for i in range(4):\n",
    "#     plt.figure(i)\n",
    "#     plt.imshow(real_images[i])\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[12,144,128,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node model_5/model_4/conv2d_27/Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_predict_function_6283]\n\nFunction call stack:\npredict_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-2870fff5d9d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStyleGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mupdate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGAN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m1000001\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_training_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-bbee64b27bb4>\u001b[0m in \u001b[0;36msave_image\u001b[0;34m(self, image_num)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# g_model = self.GAN.gen_model()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mgenerated_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGAN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnoise2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf-env/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m     87\u001b[0m           method.__name__))\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[0;32m~/miniconda3/envs/tf-env/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1266\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1268\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1269\u001b[0m             \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1270\u001b[0m             \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf-env/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf-env/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    648\u001b[0m               *args, **kwds)\n\u001b[1;32m    649\u001b[0m       \u001b[0;31m# If we did not create any variables the trace we have is good enough.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_concrete_stateful_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfn_with_cond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minner_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf-env/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1659\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \"\"\"\n\u001b[0;32m-> 1661\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1662\u001b[0m         (t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m~/miniconda3/envs/tf-env/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/miniconda3/envs/tf-env/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    591\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    594\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf-env/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[12,144,128,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node model_5/model_4/conv2d_27/Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_predict_function_6283]\n\nFunction call stack:\npredict_function\n"
     ]
    }
   ],
   "source": [
    "model = StyleGAN()\n",
    "model.save_image(0)\n",
    "update = 0\n",
    "while model.GAN.steps <= 1000001:\n",
    "    train_set = get_training_batch(images, update)\n",
    "    # ========= DEBUG ==========\n",
    "    #print(train_set.shape)\n",
    "    update += BATCH_SIZE\n",
    "    model.train(train_set)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "784a2de71e312217db3d3563a6c7078fbbff23ed7ca221264310ecd9339103c7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('tf-env': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
