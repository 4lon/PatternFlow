{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fa6846d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib as plt\n",
    "\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# Tensorflow addons for instance normalization as described in Improved Unet Paper\n",
    "import tensorflow_addons as tfa\n",
    "import os\n",
    "\n",
    "import pydot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b9015dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "INSTANCE_NORMALIZATION_ARGS = dict(\n",
    "    axis=3,                             # Axis being normalised\n",
    "    center=True,                        # Signal to add beta as an offset to the normalised tensor\n",
    "    scale=True,                         # Signal to multiply by gamma\n",
    "    beta_initializer='random_uniform',  \n",
    "    gamma_initializer='random_uniform') \n",
    "\n",
    "LEAKY_ALPHA = 0.01\n",
    "# Optimiser Information\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "acd0fe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Creation\n",
    "def build_model(input_size = (512,512,3)):\n",
    "    inputs = Input(input_size)\n",
    "    \n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2,2))(conv1)\n",
    "    \n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2,2))(conv2)\n",
    "    \n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2,2))(conv3)\n",
    "    \n",
    "    conv4 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool3)\n",
    "    conv4 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2,2))(drop4)\n",
    "    \n",
    "    conv5 = Conv2D(1024, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool4)\n",
    "    conv5 = Conv2D(1024, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "    \n",
    "    upsam6 = UpSampling2D(size=(2,2))(drop5)\n",
    "    up6 = Conv2D(512, 2, activation='relu', padding='same', kernel_initializer='he_normal')(upsam6)\n",
    "    merge6 = concatenate([drop4,up6], axis=3)\n",
    "    conv6 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge6)\n",
    "    conv6 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv6)\n",
    "    \n",
    "    upsam7 = UpSampling2D(size=(2,2))(conv6)\n",
    "    up7 = Conv2D(256, 2, activation='relu', padding='same', kernel_initializer='he_normal')(upsam7)\n",
    "    merge7 = concatenate([conv3,up7], axis=3)\n",
    "    conv7 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge7)\n",
    "    conv7 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv7)\n",
    "    \n",
    "    upsam8 = UpSampling2D(size=(2,2))(conv7)\n",
    "    up8 = Conv2D(128, 2, activation='relu', padding='same', kernel_initializer='he_normal')(upsam8)\n",
    "    merge8 = concatenate([conv2,up8], axis=3)\n",
    "    conv8 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge8)\n",
    "    conv8 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv8)\n",
    "    \n",
    "    upsam9 = UpSampling2D(size=(2,2))(conv8)\n",
    "    up9 = Conv2D(64, 2, activation='relu', padding='same', kernel_initializer='he_normal')(upsam9)\n",
    "    merge9 = concatenate([conv1,up9], axis=3)\n",
    "    conv9 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge9)\n",
    "    conv9 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv9)\n",
    "    \n",
    "    conv10 = Conv2D(1, 1, activation='sigmoid')(conv9)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=conv10)\n",
    "    model.summary()\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics='Accuracy')\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "41be687b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_module(input, out_filter):\n",
    "    # First Convolution block\n",
    "    c1 = Conv2D(filters=out_filter, kernel_size=(3,3), padding='same')(input)\n",
    "    c2 = tfa.layers.InstanceNormalization(**INSTANCE_NORMALIZATION_ARGS)(c1)\n",
    "    c3 = LeakyReLU(alpha=LEAKY_ALPHA)(c2)\n",
    "    \n",
    "    # DropOut\n",
    "    c4 = Dropout(0.3)(c3)\n",
    "    \n",
    "    # Secound Convolution block\n",
    "    c5 = Conv2D(filters=out_filter, kernel_size=(3,3), padding='same')(c4)\n",
    "    c6 = tfa.layers.InstanceNormalization(**INSTANCE_NORMALIZATION_ARGS)(c5)\n",
    "    c7 = LeakyReLU(alpha=LEAKY_ALPHA)(c6)\n",
    "    \n",
    "    # Preactivation residual add\n",
    "    c8 = Add()([input,c7])\n",
    "    \n",
    "    return c8\n",
    "\n",
    "# Module that recombines the features following concatenation and reduces the number of feature maps for memory\n",
    "def localization_module(input, out_filter):\n",
    "    # First Convolution block\n",
    "    l1 = Conv2D(filters=out_filter, kernel_size=(3,3), padding='same')(input)\n",
    "    l2 = tfa.layers.InstanceNormalization(**INSTANCE_NORMALIZATION_ARGS)(l1)\n",
    "    l3 = LeakyReLU(alpha=LEAKY_ALPHA)(l2)\n",
    "    \n",
    "    # Secound Convolution block, of shape (1x1x1)\n",
    "    l4 = Conv2D(filters=out_filter, kernel_size=(1,1), padding='same')(l3)\n",
    "    l5 = tfa.layers.InstanceNormalization(**INSTANCE_NORMALIZATION_ARGS)(l4)\n",
    "    l6 = LeakyReLU(alpha=LEAKY_ALPHA)(l5)\n",
    "    \n",
    "    return l6\n",
    "\n",
    "# Upsamples features from a lower 'level' of the UNet to a higher spatial information\n",
    "def upsampling_module(input, out_filter):\n",
    "    # Upsample \n",
    "    u1 = UpSampling2D(size=(2, 2))(input)\n",
    "    \n",
    "    # Convolutional block\n",
    "    u2 = Conv2D(filters=out_filter, kernel_size=(3,3), padding='same')(u1)\n",
    "    u3 = tfa.layers.InstanceNormalization(**INSTANCE_NORMALIZATION_ARGS)(u2)\n",
    "    u4 = LeakyReLU(alpha=LEAKY_ALPHA)(u3)\n",
    "    \n",
    "    return u4\n",
    "\n",
    "# Connects context_modueles to reduce the resolution of the feature maps and allow for more features while aggregating\n",
    "def context_connector(input, out_filter):\n",
    "    cc1 = Conv2D(filters=out_filter, kernel_size=(3,3), strides=2, padding='same')(input)\n",
    "    cc2 = tfa.layers.InstanceNormalization(**INSTANCE_NORMALIZATION_ARGS)(cc1)\n",
    "    cc3 = LeakyReLU(alpha=LEAKY_ALPHA)(cc2)\n",
    "    return cc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "947ea920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def improved_unet(input_size = (512,512,3)):\n",
    "    input = Input(shape=input_size)\n",
    "    \n",
    "    # Context Pathway\n",
    "    # Layer 1\n",
    "    x1 = Conv2D(filters=16, kernel_size=(3,3), padding='same')(input)\n",
    "    x2 = tfa.layers.InstanceNormalization(**INSTANCE_NORMALIZATION_ARGS)(x1)\n",
    "    x3 = LeakyReLU(alpha=LEAKY_ALPHA)(x2) \n",
    "    x4 = context_module(x3, 16)\n",
    "    \n",
    "    # Layer 2\n",
    "    x5 = context_connector(x4, 32)\n",
    "    x6 = context_module(x5, 32)\n",
    "    \n",
    "    # Layer 3\n",
    "    x7 = context_connector(x5, 64)\n",
    "    x8 = context_module(x7, 64)\n",
    "    \n",
    "    # Layer 4\n",
    "    x9 = context_connector(x8, 128)\n",
    "    x10 = context_module(x9, 128)\n",
    "    \n",
    "    # Layer 5.1\n",
    "    x11 = context_connector(x10, 256)\n",
    "    x12 = context_module(x11, 256)\n",
    "    \n",
    "    # Begin Localization Pathway\n",
    "    # Layer 5.2\n",
    "    x13 = upsampling_module(x12, 128)\n",
    "    \n",
    "    # Layer 4\n",
    "    x14 = Concatenate()([x10, x13])\n",
    "    x15 = localization_module(x14, 128)\n",
    "    x16 = upsampling_module(x15, 64)\n",
    "    \n",
    "    # Layer 3\n",
    "    x17 = Concatenate()([x8, x16])\n",
    "    x18 = localization_module(x17, 64) # Segmentation 1 from here\n",
    "    x19 = upsampling_module(x18, 32)\n",
    "    \n",
    "    # Layer 3: Segmentation\n",
    "    seg1 = Activation('sigmoid')(x18)\n",
    "    seg1 = upsampling_module(seg1, 32)\n",
    "    \n",
    "    # Layer 2\n",
    "    x20 = Concatenate()([x6, x19])\n",
    "    x21 = localization_module(x20, 32) # Segmentation 2 from here\n",
    "    x22 = upsampling_module(x21, 16)\n",
    "    \n",
    "    # Layer 2: Segmentation\n",
    "    seg2 = Activation('sigmoid')(x21)\n",
    "    seg3 = Add()([seg1,seg2])\n",
    "    seg3 = upsampling_module(seg3, 32)\n",
    "    \n",
    "    # Layer 1\n",
    "    x23 = Concatenate()([x4, x22])\n",
    "    x24 = Conv2D(filters=32, kernel_size=(3,3), padding='same')(x23)\n",
    "    x25 = tfa.layers.InstanceNormalization(**INSTANCE_NORMALIZATION_ARGS)(x24)\n",
    "    x26 = LeakyReLU(alpha=LEAKY_ALPHA)(x25) \n",
    "    \n",
    "    # Layer 1: Segmentation\n",
    "    seg4 = Activation('sigmoid')(x26)\n",
    "    segFinal = Add()([seg3,seg4])\n",
    "    \n",
    "    # Output\n",
    "    output = Conv2D(filters=1, kernel_size=(1,1), activation='sigmoid', padding='same')(segFinal)\n",
    "    uNet = Model(inputs=input, outputs=output)\n",
    "    \n",
    "    return uNet\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a25c1cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "model = improved_unet()\n",
    "\n",
    "tf.keras.utils.plot_model(model, show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8772dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5d250d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
