{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "fa6846d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib as plt\n",
    "\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# Tensorflow addons for instance normalization as described in Improved Unet Paper\n",
    "import tensorflow_addons as tfa\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b9015dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "INSTANCE_NORMALIZATION_ARGS = dict(\n",
    "    axis=3,                             # Axis being normalised\n",
    "    center=True,                        # Signal to add beta as an offset to the normalised tensor\n",
    "    scale=True,                         # Signal to multiply by gamma\n",
    "    beta_initializer='random_uniform',  \n",
    "    gamma_initializer='random_uniform') \n",
    "\n",
    "LEAKY_ALPHA = 0.01\n",
    "# Optimiser Information\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "41be687b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_module(input, out_filter):\n",
    "    # First Convolution block\n",
    "    c1 = Conv2D(filters=out_filter, kernel_size=(3,3), padding='same')(input)\n",
    "    c2 = tfa.layers.InstanceNormalization(**INSTANCE_NORMALIZATION_ARGS)(c1)\n",
    "    c3 = LeakyReLU(alpha=LEAKY_ALPHA)(c2)\n",
    "    \n",
    "    # DropOut\n",
    "    c4 = Dropout(0.3)(c3)\n",
    "    \n",
    "    # Secound Convolution block\n",
    "    c5 = Conv2D(filters=out_filter, kernel_size=(3,3), padding='same')(c4)\n",
    "    c6 = tfa.layers.InstanceNormalization(**INSTANCE_NORMALIZATION_ARGS)(c5)\n",
    "    c7 = LeakyReLU(alpha=LEAKY_ALPHA)(c6)\n",
    "    \n",
    "    # Preactivation residual add\n",
    "    c8 = Add()([input,c7])\n",
    "    \n",
    "    return c8\n",
    "\n",
    "# Module that recombines the features following concatenation and reduces the number of feature maps for memory\n",
    "def localization_module(input, out_filter):\n",
    "    # First Convolution block\n",
    "    l1 = Conv2D(filters=out_filter, kernel_size=(3,3), padding='same')(input)\n",
    "    l2 = tfa.layers.InstanceNormalization(**INSTANCE_NORMALIZATION_ARGS)(l1)\n",
    "    l3 = LeakyReLU(alpha=LEAKY_ALPHA)(l2)\n",
    "    \n",
    "    # Secound Convolution block, of shape (1x1x1)\n",
    "    l4 = Conv2D(filters=out_filter, kernel_size=(1,1), padding='same')(l3)\n",
    "    l5 = tfa.layers.InstanceNormalization(**INSTANCE_NORMALIZATION_ARGS)(l4)\n",
    "    l6 = LeakyReLU(alpha=LEAKY_ALPHA)(l5)\n",
    "    \n",
    "    return l6\n",
    "\n",
    "# Upsamples features from a lower 'level' of the UNet to a higher spatial information\n",
    "def upsampling_module(input, out_filter):\n",
    "    # Upsample \n",
    "    u1 = UpSampling2D(size=(2, 2))(input)\n",
    "    \n",
    "    # Convolutional block\n",
    "    u2 = Conv2D(filters=out_filter, kernel_size=(3,3), padding='same')(u1)\n",
    "    u3 = tfa.layers.InstanceNormalization(**INSTANCE_NORMALIZATION_ARGS)(u2)\n",
    "    u4 = LeakyReLU(alpha=LEAKY_ALPHA)(u3)\n",
    "    \n",
    "    return u4\n",
    "\n",
    "# Connects context_modueles to reduce the resolution of the feature maps and allow for more features while aggregating\n",
    "def context_connector(input, out_filter):\n",
    "    cc1 = Conv2D(filters=out_filter, kernel_size=(3,3), strides=2, padding='same')(input)\n",
    "    cc2 = tfa.layers.InstanceNormalization(**INSTANCE_NORMALIZATION_ARGS)(cc1)\n",
    "    cc3 = LeakyReLU(alpha=LEAKY_ALPHA)(cc2)\n",
    "    return cc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "947ea920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def improved_unet(input_size = (512,512,3)):\n",
    "    input = Input(shape=input_size)\n",
    "    \n",
    "    # Context Pathway\n",
    "    # Layer 1\n",
    "    x1 = Conv2D(filters=16, kernel_size=(3,3), padding='same')(input)\n",
    "    x2 = tfa.layers.InstanceNormalization(**INSTANCE_NORMALIZATION_ARGS)(x1)\n",
    "    x3 = LeakyReLU(alpha=LEAKY_ALPHA)(x2) \n",
    "    x4 = context_module(x3, 16)\n",
    "    \n",
    "    # Layer 2\n",
    "    x5 = context_connector(x4, 32)\n",
    "    x6 = context_module(x5, 32)\n",
    "    \n",
    "    # Layer 3\n",
    "    x7 = context_connector(x5, 64)\n",
    "    x8 = context_module(x7, 64)\n",
    "    \n",
    "    # Layer 4\n",
    "    x9 = context_connector(x8, 128)\n",
    "    x10 = context_module(x9, 128)\n",
    "    \n",
    "    # Layer 5.1\n",
    "    x11 = context_connector(x10, 256)\n",
    "    x12 = context_module(x11, 256)\n",
    "    \n",
    "    # Begin Localization Pathway\n",
    "    # Layer 5.2\n",
    "    x13 = upsampling_module(x12, 128)\n",
    "    \n",
    "    # Layer 4\n",
    "    x14 = Concatenate()([x10, x13])\n",
    "    x15 = localization_module(x14, 128)\n",
    "    x16 = upsampling_module(x15, 64)\n",
    "    \n",
    "    # Layer 3\n",
    "    x17 = Concatenate()([x8, x16])\n",
    "    x18 = localization_module(x17, 64) # Segmentation 1 from here\n",
    "    x19 = upsampling_module(x18, 32)\n",
    "    \n",
    "    # Layer 3: Segmentation\n",
    "    seg1 = Activation('sigmoid')(x18)\n",
    "    seg1 = upsampling_module(seg1, 32)\n",
    "    \n",
    "    # Layer 2\n",
    "    x20 = Concatenate()([x6, x19])\n",
    "    x21 = localization_module(x20, 32) # Segmentation 2 from here\n",
    "    x22 = upsampling_module(x21, 16)\n",
    "    \n",
    "    # Layer 2: Segmentation\n",
    "    seg2 = Activation('sigmoid')(x21)\n",
    "    seg3 = Add()([seg1,seg2])\n",
    "    seg3 = upsampling_module(seg3, 32)\n",
    "    \n",
    "    # Layer 1\n",
    "    x23 = Concatenate()([x4, x22])\n",
    "    x24 = Conv2D(filters=32, kernel_size=(3,3), padding='same')(x23)\n",
    "    x25 = tfa.layers.InstanceNormalization(**INSTANCE_NORMALIZATION_ARGS)(x24)\n",
    "    x26 = LeakyReLU(alpha=LEAKY_ALPHA)(x25) \n",
    "    \n",
    "    # Layer 1: Segmentation\n",
    "    seg4 = Activation('sigmoid')(x26)\n",
    "    segFinal = Add()([seg3,seg4])\n",
    "    \n",
    "    # Output\n",
    "    output = Conv2D(filters=1, kernel_size=(1,1), activation='sigmoid', padding='same')(segFinal)\n",
    "    uNet = Model(inputs=input, outputs=output)\n",
    "    model = ThresholdedReLU(theta=0.5)(uNet)\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
