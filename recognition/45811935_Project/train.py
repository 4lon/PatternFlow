"""
    Performs all training/validating/testing/saving of models and plotting of results (i.e.
    losses and metrics during training/validation).

    Author: Adrian Rahul Kamal Rajkamal
    Student Number: 45811935
"""
from dataset import *
from modules import *
import keras.callbacks
from keras import optimizers
import tensorflow_probability as tfp
import matplotlib.pyplot as plt

""" Main data-related constants """
FILE_PATH = "./ADNI_AD_NC_2D/AD_NC/"
IMG_DIMENSION = 256
SEED = 42

""" Hyper-parameters """
NUM_EMBEDDINGS = 256
BATCH_SIZE = 32
LATENT_DIM = 32
PIXEL_SHIFT = 0.5
NUM_EPOCHS = 30
VALIDATION_SPLIT = 0.3
LEARNING_RATE = 0.001
OPTIMISER = optimizers.Adam(learning_rate=LEARNING_RATE)

# Check if GPU is available and if so use it, otherwise use CPU
gpu_used = len(tf.config.list_physical_devices('GPU'))
device = "/GPU:0" if gpu_used else "/CPU:0"

# Load and pre-process data
train_data = load_preprocess_image_data(path=FILE_PATH + "train", img_dim=IMG_DIMENSION,
                                        batch_size=BATCH_SIZE, validation_split=VALIDATION_SPLIT,
                                        subset="training", seed=SEED, shift=PIXEL_SHIFT)

val_data = load_preprocess_image_data(path=FILE_PATH + "train", img_dim=IMG_DIMENSION,
                                      batch_size=BATCH_SIZE, validation_split=VALIDATION_SPLIT,
                                      subset="validation", seed=SEED, shift=PIXEL_SHIFT)

test_data = load_preprocess_image_data(path=FILE_PATH + "test", img_dim=IMG_DIMENSION,
                                       batch_size=BATCH_SIZE, shift=PIXEL_SHIFT)

# Calculate (unbiased) training variance (across all pixels of all images)
full_train_data = train_data.unbatch()
num_train_images = full_train_data.reduce(np.int32(0), lambda x, _: x + 1).numpy()
num_train_pixels = num_train_images * IMG_DIMENSION ** 2

image_sum = full_train_data.reduce(np.float32(0), lambda x, y: x + y).numpy().flatten()
total_pixel_sum = image_sum.sum()
mean_pixel_train = total_pixel_sum / num_train_pixels

image_sse = full_train_data.reduce(np.float32(0), lambda x, y: x + (y - mean_pixel_train) ** 2)\
                           .numpy().flatten()
var_pixel_train = image_sse.sum() / (num_train_pixels - 1)

# Create VQ-VAE model
vqvae = VQVAE(tr_var=var_pixel_train, num_encoded=NUM_EMBEDDINGS, latent_dim=LATENT_DIM)

# Define SSIM metric and Callback


def mean_ssim(data, model):
    """
    Calculate mean SSIM over given dataset, comparing reconstructions generated by the given model.

    Args:
        data: The given dataset (e.g. training/validation/test)
        model: The given model

    Returns: mean SSIM over data.

    """
    ssim_sum = num_images = 0

    for image_batch in data:
        # Unshift the normalised images and reconstructions in this batch
        image_batch_unshifted = image_batch + PIXEL_SHIFT
        reconstruction_batch = model.predict(image_batch)
        reconstruction_batch_unshifted = reconstruction_batch + PIXEL_SHIFT

        # Calculate and sum over all SSIMs between images in this batch (max_val of 1 as the images
        # are still normalised by the constant of 255)
        total_batch_ssim = tf.math.reduce_sum(tf.image.ssim(image_batch_unshifted,
                                                            reconstruction_batch_unshifted,
                                                            max_val=1.0))
        # Add batch sum SSIM to total
        ssim_sum += total_batch_ssim
        num_images += image_batch.shape[0]
        # print("Number of images in a batch is:", num_images)

    # Return mean SSIM over entire dataset
    return ssim_sum / num_images


class SSIMTracking(keras.callbacks.Callback):
    """ Calculates and displays the mean SSIM after each epoch """
    def __init__(self, dataset, dataset_name="Training"):
        super(SSIMTracking, self).__init__()
        self._dataset = dataset
        self._dataset_name = dataset_name

    def on_epoch_end(self, epoch, logs=None):
        """ Calculate and display the mean SSIM on each epoch """
        print(f"Epoch {epoch}: {self._dataset_name} Mean SSIM: "
              f"{mean_ssim(self._dataset, self.model)}")


# Train VQ-VAE
vqvae.compile(optimizer=OPTIMISER)

with tf.device(device):
    history = vqvae.fit(train_data,
                        epochs=NUM_EPOCHS,
                        batch_size=BATCH_SIZE,
                        callbacks=[SSIMTracking(dataset=train_data),
                                   SSIMTracking(val_data, dataset_name="Validation")]
                        )




# Save

# Plot

# Test/generate --> maybe load saved model above and put this in predict.py

# Get trained VQ-VAE codebooks

# Create PixelCNN model

# Compile & Fit (with trained VQ-VAE codebooks)

# Save

# Plot

# Test/generate --> maybe load saved model above and put this in predict.py
