# **Knee MRI Image stylegan**
Student name: Mengyao Ma  
Student number: s4585787  

### Generative Adversarial Network 
![Gan structure](https://github.com/MMMMMYY/PatternFlow/blob/topic-recognition/recognition/45857876/images/gan.png)  
**GAN** is short for **Generative Adversarial Network** proposed by Ian Goodfellow in 2014. The main structure of GAN includes a **Generator(G)** and a **Discriminator (D)**.  For the generator, the input requires an n-dimensional vector, and the output is a picture with the pixel size of the picture, while the discriminator discriminates the pictures generated by the generator and labels them as "fake" or "real".   


### StyleGAN
**StyleGAN** is inspired by **style transfer**  to design a **new generator structure**.  In addtion, StyleGAN is **evolved** from **ProGAN** and uses a similar network structure. StyleGAN uses style to affect the posture and identity characteristics of the face, and noise to affect details such as hair strands, wrinkles, and skin tone. 


## The main StyleGAN structure  
[Traditional genertaor and StyleGAN generator](https://github.com/MMMMMYY/PatternFlow/blob/topic-recognition/recognition/45857876/images/20190325144840976.png)  
Through observation, it can be found that different layers and resolutions will affect different features. The lower the layer and resolution, the coarser the features it affects. We can divide these characteristics into three types:  
1. Rough-(resolution 0-8^2), affecting posture, general hairstyle, facial shape, etc.; 
2. Medium-(resolution 16^2-32^2), affecting finer facial features, hairstyles, opening of eyes or Closed, etc.; 
3. High-quality-(resolution 64^2^-1024^2^), affecting color (eyes, hair and skin) and microscopic features;  

In traditional gan, the generator only feeds the random variables as latent code into input layer.
### Mapping Network

To better solve the Feature unwrapping problem, the Mapping network is added into Generator. It consists of 8 fully connected layers. The Mapping network convert the latent code into 18 vectors after affin transform. These vectors A learned affin transform are added into the synthesis generator network(two for each resolution). The vectors A can control the style on certain resolution.

### AdaIN module

[AdaIN](https://github.com/MMMMMYY/PatternFlow/blob/topic-recognition/recognition/45857876/images/20190325144840976.png)   
At each resolution, two A will affect the generator twice, once after Upsampling and once after Convolution by using AdaIN.
Like the equation ablow, expand A into scaling factors y~ùë†,ùëñ~ and deviation factors y~ùëè,ùëñ~, and make a weighted sum of these two factors and the normalized convolution output to apply an influence.  

### Random nosie

To control the character detail and diversity of image generated, add a scaled noise to each channel before the Adain module. 

### Style mixing



## **Loss function applied**

In this task, I explore two loss functions to find the influence of loss function.  

### 1.  Logistic loss function
Output for logistic loss function for resolution 256 shows below:  
[resolution 256 epoch9](https://github.com/MMMMMYY/PatternFlow/blob/topic-recognition/recognition/45857876/images/logistic_256gen_6_9_1.png) 

### 2. Relativistic Average Hinge loss function
Output for logistic loss function for resolution 256 shows below:  
[resolution 256 epoch9](https://github.com/MMMMMYY/PatternFlow/blob/topic-recognition/recognition/45857876/images/rahingegen_6_9_1.png) 

## **Output of each resolution**


## **Style mixing output**

[test_image](https://github.com/MMMMMYY/PatternFlow/blob/topic-recognition/recognition/45857876/images/figure03-style-mixing.png)  

## Requirments

yacs
tqdm
numpy (only for visualization)
torchvision
torch

## Execute the code
Training:  
    python train.py --
Generate mixing image(test):  
    python generate_mixing.py --

## The default setting:  

general setting:  
    device = 'cuda'  
    number of preview samples = 36  
    make checkpoint every/epoch = 10  
    test loss:  
        epochs: [2,4,8,8,16,24,32]  
    style mixing:  
        epochs: [2,4,8,8,16,24,32,40]  

Generator setting:  
    latent size = 512  
    mapping layers = 4(8 in original paper, but 4 layers when latene size = 512)  
    blur_filter = [1, 2, 1]   

Discriminator setting:  
    enable equalized learning rate = True  
    blur_filter = [1, 2, 1]  

Generator Optimizer setting:
    optimizer = Adam  
    learning_rate = 0.003  
    betas = [0,0.99]  
    eps = defualt

Discriminator Optimizer setting:
    optimizer = Adam  
    learning_rate = 0.003  
    betas = [0,0.99]  
    eps = defualt
    
    

## Reference
Paper:  
A Style-Based Generator Architecture for Generative Adversarial Networks
Tero Karras (NVIDIA), Samuli Laine (NVIDIA), Timo Aila (NVIDIA)
https://arxiv.org/abs/1812.04948
Progressive Growing of GANs for Improved Quality, Stability, and Variation
Tero Karras, Timo Aila, Samuli Laine, Jaakko Lehtinen
https://arxiv.org/abs/1710.10196
Generative Adversarial Networks
Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio
https://arxiv.org/abs/1406.2661

Code:   
https://github.com/huangzh13/StyleGAN.pytorch  
https://github.com/lernapparat/lernapparat
https://github.com/NVlabs/stylegan
https://github.com/akanimax/pro_gan_pytorch
https://github.com/rosinality/style-based-gan-pytorch
https://github.com/goodfeli/adversarial
