# Generative VQVAE + DCGAN
A generative model using VQVAE and DCGAN\
Aryaman Sharma - 46066853 

## Usage
#### Dependencies:
- torch==1.9.0
- torchvision==0.10.0
- matplotlib==3.4.3
- Numpy==1.19.5
- tqdm==4.62.1
- Pillow==8.4.0
- skimage==0.18.0

To reproduce results using any dataset. Prepare dataset by structureing the data directory in accordance with torchvision.datasets.ImageFolder (https://pytorch.org/vision/stable/datasets.html). Modify data dir path in driver.ipynb and run all cells in order.\ 
Results may be volatile and vary with iterations. It is recommended to perform multiple independent runs of the notebook and selecting best performing models.

# Model Overview
![image](/images/vqvae.png.png)
![image](/images/vqvae_elements.png)
![image](/images/dcgan.png)

# Results on OASIS brain dataset
The VQVAE was trained only for two epochs to minimise number of unique values in codebook indices. (2 in the best case) Followed by training the DCGAN. Intermediate results from steps and training parameters are saved in driver.ipynb.

SSIM was >0.60 in 7809 out of 10000 training images used. Best SSIM archived was 0.698\
![image](/images/result.png)

# Scope for improvement
The main drawback with the current implementation is mapping the indices generated by the DCGAN to discrete elements to match codebook indices. Although training the VQVAE for higher epochs decreases reconstruction loss it also increases the number of unique elements in codebook indices. Two ways in which this problem can be resolved to get better reults:
- Optimise mapping algorithm: The current mapping relies on uniformly diving the output in intervals of same size. Can be improved and optimised by training the bin sizes improve mapping accuracy.
- Modify the DCGAN to output multichannel one hot encoded codebook indices rather than a image representation.