# OASIS Generative VQ-VAE
By Matthew Choy
> Creation of a generative model of the ADNI dataset using the VQVAE model ([paper](arxiv.org/abs/1711.00937))

# Files of Interest
What files to look at, and what purpose they serve.
- [modules.py](https://github.com/MattPChoy/PatternFlow/blob/topic-recognition/recognition/MattChoy-VQVAE/modules.py) Contains module definitions for the VQVAE (& Encoder, Decoder) as well as PixelCNN
- [train.py](https://github.com/MattPChoy/PatternFlow/blob/topic-recognition/recognition/MattChoy-VQVAE/train.py) Uses the modules defined in `modules.py` to train and save the VQVAE and PixelCNN models.
- [dataset.py](https://github.com/MattPChoy/PatternFlow/blob/topic-recognition/recognition/MattChoy-VQVAE/dataset.py) Loads and normalises dataset.
- [constants.py](https://github.com/MattPChoy/PatternFlow/blob/topic-recognition/recognition/MattChoy-VQVAE/constants.py) Defines constants and hyperparameters to ensure that this model can be adapted to other datasets more easily.
- [predict.py](https://github.com/MattPChoy/PatternFlow/blob/topic-recognition/recognition/MattChoy-VQVAE/predict.py) Loads the trained VQVAE and PixelCNN models located in `./vqvae` and `./pixel_cnn` respectively to generate OASIS-like brain scan images.
- [util.py](https://github.com/MattPChoy/PatternFlow/blob/topic-recognition/recognition/MattChoy-VQVAE/util.py) Defines helper methods for visualisation, data processing etc.

## Table of Contents
- [Algorithm Description](#description-of-algorithm)
- [How Does it Work?](#how-does-it-work)
- [Inputs, Outputs and Algorithm Performance](#inputs-outputs-and-algorithm-performance)
- [Implementation Details](#implementation-details)
  - [Dependencies](#dependencies)
- [Dataset](#dataset)
- [Data Pre-Processing](#data-pre-processing)
- [Training, Validation and Testing Splits](#training-validation-and-testing-splits)

# Description of Algorithm
## VQVAE
AutoEncoders are neural networks that transform inputs (in this case, images) into smaller dimensionality representations (such as the codes generated by this VQVAE). This VQVAE model contains an encoderm, given by the equation $e(x)$ where $x$ is the input image. It transforms the image into a smaller, continuous representation in the latent space.

This continuous latent representation is then passed through the VectorQuantiser module, which transforms it into a discretised latent representation by "snapping" it to the most representative entry in the codebook <a href="#note1">[Note 1]</a>. Let $e_i$ represent the $i$th entry in the codebook, which is closest entry found.

<p align="center">
  <img style="width:40vw;" src="./images/AutoEncoder-TowardsDataScience.png"/>
</p>
<p align="center">AutoEncoder, from <a href="https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73">TowardsDataScience</a></p>


This value can then be passed through the decoder module, represented by the equation $d(e)$ which takes a code as input, and outputs a reconstructed image. If the latent representation sufficiently describes key characteristics of the image, then the reconstructed image $\hat{x}$ should have high structural similiarity to the original image $x$. That is why the loss function that we're trying to optimise when training the VQVAE is the `reconstruction loss` and is given by the following equation.
$$\mathcal{L}(x, \hat{x})=|x-\hat{x}|$$

<a id="note1" name="note1">[Note 1]</a> Conceptually, we perform an $\text{argmin}$ operation in which we try to minimise the sum of differences between each entry in the latent representation, and the particular entry in the codebook.

## How does it work?
- Approximately a paragraph
- Include a visualisation

## Inputs, Outputs and Algorithm Performance
### VQVAE
Through the training process, I have logged the reconstruction loss, given by the difference between the original image from the dataset, $x$ and the reconstructed image which has been passed through the encoder, decoder and vector quantiser modules $\hat{x}=d(e(x))$. At the end of training, the reconstruction loss of the model is minimal, at $0.05%$ on the validation set.
$$\mathcal{L}(x, \hat{x})=|x-\hat{x}|$$
<p align="center">
  <img style="width:40vw;" src="./images/loss.png"/>
</p>
Images fed into the model are of dimensionality $(\text{batchsize}, 256, 256, 1)$ and are taken from the OASIS brain dataset. Some sample images include those shown below:
<p align="center">
  <img style="width:60vw" src="./images/sample_inputs.png"/>
</p>

The VQVAE's `Encoder` and `VectorQuantiser` module transform the input $x$ into a codebook sample $e_i$, as shown below.
<p align="center">
  <img style="width:60vw" src="./images/vqvae_reconstructions.png"/>
</p>

Following this, the codebook samples are fed through the decoder to reconstruct the image - Since the reproductions are structurally similar to the original images, this proves that the codes generated by the `Encoder` and `VectorQuantiser` modules are accurate representations of the input data, at smaller dimensionality.

Furthermore, we compute the structural similarity between the original image and the images reconstructed by the VQVAE model.

<p align="center">
  <img style="width:60vw" src="./images/SSIM.png"/>
</p>

### PixelCNN
We can then feed these latent representations through the trained PixelCNN module to generate novel images.

<p align="center">
  <img style="width:30vw" src="./images/pixelcnn_reconstructions.png"/>
</p>
## Implementation Details
### Dependencies
This project leverages a few non-standard libraries, including TensorFlow. Their versions are listed here:
```bash
tensorflow==2.9.1
tensorflow-probability==0.16.0
numpy==1.23.1
matplotlib==3.5.2
keras==2.9.0
Pillow==9.2.0
```

### Dataset
- The dataset used for this task was downloaded from BlackBoard and pre-processed into folders. It is available for easy download via my GitHub repository [GitHub.com/MattPChoy/OASIS](https://github.com/MattPChoy/OASIS)
### Data Pre-Processing
- The data was unzipped from the above .zip file, and moved into the project's root folder (PatternFlow/recognition/MattChoy-VQVAE/data/OASIS)
- Upon loading the data, it is normalised via dividing by 255.0 such that each pixel value is in the range $[0, 1]$.

### Training, Validation and Testing Splits
- I have used the test/train/validation split from the original dataset provided, which contains the following counts:

|                   | Train       | Validate     | Test     |
| ----------------- | ----------- | ------------ | -------- |
| Number of Samples | 9,664 (86%) | 1,120  (10%) | 544 (4%) |
