{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Methods.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjZ_XxgC57LQ"
      },
      "source": [
        "import tensorflow as tf \n",
        "import pathlib\n",
        "import numpy as np\n",
        "from matplotlib import pyplot\n",
        "from matplotlib import image\n",
        "import glob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPJNfJkC2R08"
      },
      "source": [
        "def download_oasis ():\n",
        "    \n",
        "    #download oasis brain MRI data\n",
        "    dataset_url = \"https://cloudstor.aarnet.edu.au/plus/s/n5aZ4XX1WBKp6HZ/download\"\n",
        "    data_dir = tf.keras.utils.get_file(origin=dataset_url,fname='oa-sis' ,untar=True)\n",
        "    data_dir = pathlib.Path(data_dir)\n",
        "    \n",
        "    # unzip data to current directory \n",
        "    print (data_dir)\n",
        "    ! unzip /root/.keras/datasets/oa-sis.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsEL-j0w5Gd-"
      },
      "source": [
        "def load_training (path):\n",
        "    # load training images (non segmented) in the path and store in numpy array\n",
        "    image_list = []\n",
        "    for filename in glob.glob(path+'/*.png'): \n",
        "        im=image.imread (filename)\n",
        "        image_list.append(im)\n",
        "\n",
        "    print('train_X shape:',np.array(image_list).shape)\n",
        "    train_set = np.array(image_list, dtype=np.float32)\n",
        "    return train_set\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnIbIVop69Dn"
      },
      "source": [
        "def process_training (data_set):\n",
        "    # the method normalizes training images and adds 4th dimention \n",
        "\n",
        "    train_set = data_set\n",
        "    train_set = (train_set - np.mean(train_set))/ np.std(train_set)\n",
        "    train_set= (train_set- np.amin(train_set))/ np.amax(train_set- np.amin(train_set))\n",
        "    train_set = train_set [:,:,:,np.newaxis]\n",
        "    \n",
        "    return train_set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQRjoY8HFUoi"
      },
      "source": [
        "def load_labels (path):\n",
        "    # loads labels images and map pixel values to class indices and convert image data type to unit8 \n",
        "\n",
        "    n_classes = 4\n",
        "    image_list =[]\n",
        "    for filename in glob.glob(path+'/*.png'): \n",
        "        im=image.imread (filename)\n",
        "        one_hot = np.zeros((im.shape[0], im.shape[1]))\n",
        "        for i, unique_value in enumerate(np.unique(im)):\n",
        "          one_hot[:, :][im == unique_value] = i\n",
        "        image_list.append(one_hot)\n",
        "\n",
        "    print('train_y shape:',np.array(image_list).shape)\n",
        "    labels = np.array(image_list, dtype=np.uint8)\n",
        "    \n",
        "    pyplot.imshow(labels[2])\n",
        "    pyplot.show()\n",
        "\n",
        "    return labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xsmg4eHmLJY5"
      },
      "source": [
        "def process_labels(seg_data):\n",
        "    # one hot encode label data and convert to numpy array\n",
        "    onehot_Y = []\n",
        "    for n in range(seg_data.shape[0]): \n",
        "      im = seg_data[n]\n",
        "      n_classes = 4\n",
        "      one_hot = np.zeros((im.shape[0], im.shape[1], n_classes),dtype=np.uint8)\n",
        "      for i, unique_value in enumerate(np.unique(im)):\n",
        "          one_hot[:, :, i][im == unique_value] = 1\n",
        "      onehot_Y.append(one_hot)\n",
        "    \n",
        "    onehot_Y =np.array(onehot_Y)\n",
        "    print (onehot_Y.dtype)\n",
        "    #print (np.unique(onehot_validate_Y))\n",
        "    print (onehot_Y.shape)\n",
        "\n",
        "    return onehot_Y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAFXCuq8hFaV"
      },
      "source": [
        "# U-NET final model w on2Dtranspose and batch normalization after activation\n",
        "# questions do I need to use shuffle =true in fit module ?\n",
        "import tensorflow as tf\n",
        "\n",
        "def unet_model ():\n",
        "    filter_size=16 \n",
        "    input_layer = tf.keras.Input((256,256,1))\n",
        "    \n",
        "    pre_conv = tf.keras.layers.Conv2D(filter_size * 1, (3, 3), padding=\"same\")(input_layer)\n",
        "    pre_conv = tf.keras.layers.LeakyReLU(alpha=.01)(pre_conv)\n",
        "\n",
        "\n",
        "# context module 1 pre-activation residual block\n",
        "    conv1 = tf.keras.layers.BatchNormalization()(pre_conv)\n",
        "    conv1 = tf.keras.layers.LeakyReLU(alpha=.01)(conv1)\n",
        "    conv1 = tf.keras.layers.Conv2D(filter_size * 1, (3, 3), padding=\"same\" )(conv1) \n",
        "    conv1 = tf.keras.layers.Dropout(.3) (conv1)\n",
        "    conv1 = tf.keras.layers.BatchNormalization()(conv1)\n",
        "    conv1 = tf.keras.layers.LeakyReLU(alpha=.01)(conv1)\n",
        "    conv1 = tf.keras.layers.Conv2D(filter_size * 1, (3, 3), padding=\"same\")(conv1)    \n",
        "    conv1 = tf.keras.layers.Add()([pre_conv,conv1])\n",
        "    \n",
        "# downsample and double number of feature maps   \n",
        "    pool1 = tf.keras.layers.Conv2D(filter_size * 2, (3,3), (2,2) , padding='same')(conv1)\n",
        "    pool1 = tf.keras.layers.LeakyReLU(alpha=.01)(pool1)\n",
        "    \n",
        "# context module 2\n",
        "    conv2 = tf.keras.layers.BatchNormalization()(pool1)\n",
        "    conv2 = tf.keras.layers.LeakyReLU(alpha=.01)(conv2)\n",
        "    conv2 = tf.keras.layers.Conv2D(filter_size * 2, (3, 3), padding=\"same\")(conv2)\n",
        "    conv2 = tf.keras.layers.Dropout(.3) (conv2)  \n",
        "    conv2 = tf.keras.layers.BatchNormalization()(conv2)\n",
        "    conv2 = tf.keras.layers.LeakyReLU(alpha=.01)(conv2)\n",
        "    conv2 = tf.keras.layers.Conv2D(filter_size * 2, (3, 3), padding=\"same\")(conv2)\n",
        "    conv2 = tf.keras.layers.Add()([pool1,conv2])\n",
        "\n",
        "# downsample and double number of feature maps\n",
        "    pool2 = tf.keras.layers.Conv2D(filter_size*4, (3,3),(2,2), padding='same')(conv2)\n",
        "    pool2 = tf.keras.layers.LeakyReLU(alpha=.01)(pool2)\n",
        "\n",
        "# context module 3\n",
        "    conv3 = tf.keras.layers.BatchNormalization()(pool2)\n",
        "    conv3 = tf.keras.layers.LeakyReLU(alpha=.01)(conv3)\n",
        "    conv3 = tf.keras.layers.Conv2D(filter_size * 4, (3, 3), padding=\"same\")(conv3)\n",
        "    conv3 = tf.keras.layers.Dropout(.3) (conv3)\n",
        "    conv3 = tf.keras.layers.BatchNormalization()(conv3)\n",
        "    conv3 = tf.keras.layers.LeakyReLU(alpha=.01)(conv3)\n",
        "    conv3 = tf.keras.layers.Conv2D(filter_size * 4, (3, 3), padding=\"same\")(conv3)\n",
        "    conv3 = tf.keras.layers.Add()([pool2,conv3])\n",
        "\n",
        "# downsample and double number of feature maps\n",
        "    pool3 = tf.keras.layers.Conv2D(filter_size*8, (3,3),(2,2),padding='same')(conv3)\n",
        "    pool3 = tf.keras.layers.LeakyReLU(alpha=.01)(pool3)\n",
        "\n",
        "# context module 4\n",
        "    conv4 = tf.keras.layers.BatchNormalization()(pool3)\n",
        "    conv4 = tf.keras.layers.LeakyReLU(alpha=.01)(conv4)\n",
        "    conv4 = tf.keras.layers.Conv2D(filter_size * 8, (3, 3), padding=\"same\")(conv4)\n",
        "    conv4 = tf.keras.layers.Dropout(.3) (conv4)\n",
        "    conv4 = tf.keras.layers.BatchNormalization()(conv4)\n",
        "    conv4 = tf.keras.layers.LeakyReLU(alpha=.01)(conv4)\n",
        "    conv4 = tf.keras.layers.Conv2D(filter_size * 8, (3, 3), padding=\"same\")(conv4)\n",
        "    conv4 = tf.keras.layers.Add()([pool3,conv4])\n",
        "    print (\"conv4\",conv4.shape)\n",
        "\n",
        "# downsample and double number of feature maps\n",
        "    pool4 = tf.keras.layers.Conv2D(filter_size*16, (3,3),(2,2),padding='same')(conv4)\n",
        "    pool4 = tf.keras.layers.LeakyReLU(alpha=.01)(pool4) \n",
        "\n",
        "# context module 5\n",
        "    # Middle\n",
        "    convm = tf.keras.layers.BatchNormalization()(pool4)\n",
        "    convm = tf.keras.layers.LeakyReLU(alpha=.01)(convm)\n",
        "    convm = tf.keras.layers.Conv2D(filter_size * 16, (3, 3), padding=\"same\")(convm)\n",
        "    convm = tf.keras.layers.Dropout(.3) (convm)\n",
        "    convm = tf.keras.layers.BatchNormalization()(convm)\n",
        "    convm = tf.keras.layers.LeakyReLU(alpha=.01)(convm)\n",
        "    convm = tf.keras.layers.Conv2D(filter_size * 16, (3, 3), padding=\"same\")(convm)\n",
        "    convm = tf.keras.layers.Add()([pool4,convm])\n",
        "\n",
        "\n",
        "#upsampling module 1\n",
        "    deconv4 = tf.keras.layers.UpSampling2D(size=(2,2) , interpolation='bilinear')(convm)\n",
        "    deconv4 = tf.keras.layers.Conv2D (filter_size *8, (3, 3) , padding=\"same\")(deconv4)\n",
        "    deconv4 = tf.keras.layers.LeakyReLU(alpha=.01)(deconv4) \n",
        "    print (\"upsample 1\",deconv4.shape)\n",
        "\n",
        "#concatatinate layers \n",
        "    uconv4 = tf.keras.layers.concatenate([deconv4, conv4], axis=3)\n",
        "\n",
        "\n",
        "#localization module 1\n",
        "    uconv4 = tf.keras.layers.Conv2D(filter_size * 16, (3, 3) , padding=\"same\")(uconv4)\n",
        "    uconv4 = tf.keras.layers.BatchNormalization()(uconv4)\n",
        "    uconv4 = tf.keras.layers.LeakyReLU(alpha=.01)(uconv4)\n",
        "    uconv4 = tf.keras.layers.Conv2D(filter_size * 8, (1, 1), padding=\"same\")(uconv4)\n",
        "    uconv4 = tf.keras.layers.BatchNormalization()(uconv4)\n",
        "    uconv4 = tf.keras.layers.LeakyReLU(alpha=.01)(uconv4)\n",
        "\n",
        "#upsampling module 2\n",
        "    deconv3 = tf.keras.layers.UpSampling2D(size=(2,2) , interpolation='bilinear')(uconv4)\n",
        "    deconv3 = tf.keras.layers.Conv2D (filter_size *4, (3, 3) , padding=\"same\")(deconv3)\n",
        "    deconv3 = tf.keras.layers.LeakyReLU(alpha=.01)(deconv3) \n",
        "\n",
        "  \n",
        "\n",
        "# concatatinate layers  \n",
        "    uconv3 = tf.keras.layers.concatenate([deconv3, conv3], axis=3)\n",
        "\n",
        "\n",
        "# localization module 2\n",
        "    uconv3 = tf.keras.layers.Conv2D(filter_size * 8, (3, 3), padding=\"same\")(uconv3)\n",
        "    uconv3 = tf.keras.layers.BatchNormalization()(uconv3)\n",
        "    uconv3 = tf.keras.layers.LeakyReLU(alpha=.01)(uconv3)\n",
        "    uconv3 = tf.keras.layers.Conv2D(filter_size * 4, (1, 1), padding=\"same\")(uconv3)\n",
        "    uconv3 = tf.keras.layers.BatchNormalization()(uconv3)\n",
        "    uconv3 = tf.keras.layers.LeakyReLU(alpha=.01)(uconv3)\n",
        "\n",
        "# segmentation layer 1\n",
        "    seg3 = tf.keras.layers.Conv2D(4, (3,3),  activation=\"softmax\", padding='same' )(uconv3)\n",
        "# upscale segmented layer 1\n",
        "    seg3 = tf.keras.layers.UpSampling2D(size=(2,2) , interpolation='bilinear')(seg3)\n",
        "\n",
        "\n",
        "# Upsample module 3\n",
        "    deconv2 = tf.keras.layers.UpSampling2D(size=(2,2) , interpolation='bilinear')(uconv3)\n",
        "    deconv2 = tf.keras.layers.Conv2D (filter_size *2, (3, 3) , padding=\"same\")(deconv2)\n",
        "    deconv2 = tf.keras.layers.LeakyReLU(alpha=.01)(deconv2)\n",
        "\n",
        "\n",
        "# concatination layer \n",
        "    uconv2 = tf.keras.layers.concatenate([deconv2, conv2], axis=3)\n",
        "\n",
        "\n",
        "# localization module 3\n",
        "    uconv2 = tf.keras.layers.Conv2D(filter_size * 4, (3, 3), padding=\"same\")(uconv2)\n",
        "    uconv2 = tf.keras.layers.BatchNormalization()(uconv2)\n",
        "    uconv2 = tf.keras.layers.LeakyReLU(alpha=.01)(uconv2)\n",
        "    uconv2 = tf.keras.layers.Conv2D(filter_size * 2, (1, 1), padding=\"same\")(uconv2)\n",
        "    uconv2 = tf.keras.layers.BatchNormalization()(uconv2)\n",
        "    uconv2 = tf.keras.layers.LeakyReLU(alpha=.01)(uconv2)\n",
        "\n",
        "# segmentation layer 2\n",
        "    seg2 = tf.keras.layers.Conv2D(4, (3,3),  activation=\"softmax\", padding='same')(uconv2)\n",
        "\n",
        "# add segmentation layer 1 and 2\n",
        "    seg_32 = tf.keras.layers.Add()([seg3,seg2])\n",
        "# upscale sum segmentation layer 1 and 2\n",
        "    seg_32 = tf.keras.layers.UpSampling2D(size=(2,2) , interpolation='bilinear')(seg_32)\n",
        "\n",
        "\n",
        "# Upsample module 4\n",
        "    deconv1 = tf.keras.layers.UpSampling2D(size=(2,2) , interpolation='bilinear')(uconv2)\n",
        "    deconv1 = tf.keras.layers.Conv2D (filter_size *1, (3, 3) , padding=\"same\")(deconv1)\n",
        "    deconv1 = tf.keras.layers.LeakyReLU(alpha=.01)(deconv1)\n",
        "\n",
        "\n",
        "# concatination layer\n",
        "    uconv1 = tf.keras.layers.concatenate([deconv1, conv1], axis=3 )\n",
        "\n",
        "#final convolution layer\n",
        "    uconv1 = tf.keras.layers.Conv2D(filter_size * 2, (3, 3), padding=\"same\")(uconv1)\n",
        "    uconv1 = tf.keras.layers.BatchNormalization()(uconv1)\n",
        "    uconv1 = tf.keras.layers.LeakyReLU(alpha=.01)(uconv1)\n",
        "    \n",
        "# final segmentation layer   \n",
        "    seg1 = tf.keras.layers.Conv2D(4, (3,3),  activation=\"softmax\", padding='same' )(uconv1)\n",
        "\n",
        "# sum all segmentation layers \n",
        "    seg_sum = tf.keras.layers.Add()([seg1,seg_32])\n",
        "\n",
        "\n",
        "    output_layer = tf.keras.layers.Conv2D(4, (3,3), padding='same' ,activation=\"softmax\")(seg_sum)\n",
        "    model = tf.keras.Model( input_layer , outputs=output_layer)\n",
        "\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aD1tLqv8Ax2o"
      },
      "source": [
        " \n",
        "def dice_coefficient (y_true, y_pred):\n",
        "  from keras import backend as k\n",
        "  \n",
        "  c1_true = k.flatten(y_true[:,:,:,3])\n",
        "  c1_pred = k.flatten(y_pred[:,:,:,3]) \n",
        "  c2_true = k.flatten(y_true[:,:,:,2])\n",
        "  c2_pred = k.flatten(y_pred[:,:,:,2]) \n",
        "  c3_true = k.flatten(y_true[:,:,:,1])\n",
        "  c3_pred = k.flatten(y_pred[:,:,:,1]) \n",
        "  c4_true = k.flatten(y_true[:,:,:,0])\n",
        "  c4_pred = k.flatten(y_pred[:,:,:,0]) \n",
        "\n",
        "  \n",
        "  intersection1 = k.sum(c1_true*c1_pred)\n",
        "  coeff1 = (2.0*intersection1)/(k.sum(c1_true) + k.sum(c1_pred) )\n",
        "\n",
        "  intersection2 = k.sum(c2_true*c2_pred)\n",
        "  coeff2 = (2.0*intersection2)/(k.sum(c2_true) + k.sum(c2_pred) )\n",
        "\n",
        "  \n",
        "  intersection3 = k.sum(c3_true*c3_pred)\n",
        "  coeff3 = (2.0*intersection3)/(k.sum(c3_true) + k.sum(c3_pred) )\n",
        "\n",
        "  intersection4 = k.sum(c4_true*c4_pred)\n",
        "  coeff4 = (2.0*intersection4)/(k.sum(c4_true) + k.sum(c4_pred) )\n",
        "  \n",
        "  coeff= (coeff1+coeff2+coeff3+coeff4)/4.0\n",
        "  return coeff\n",
        "  \n"
      ],
      "execution_count": 6,
      "outputs": []
    }
  ]
}