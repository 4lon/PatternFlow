{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FjZ_XxgC57LQ"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-2865c7d3389f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "import pathlib\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "from matplotlib import image\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yPJNfJkC2R08"
   },
   "outputs": [],
   "source": [
    "def download_oasis ():\n",
    "    \n",
    "    #download oasis brain MRI data\n",
    "    dataset_url = \"https://cloudstor.aarnet.edu.au/plus/s/n5aZ4XX1WBKp6HZ/download\"\n",
    "    data_dir = tf.keras.utils.get_file(origin=dataset_url,fname='oa-sis' ,untar=True)\n",
    "    data_dir = pathlib.Path(data_dir)\n",
    "    \n",
    "    # unzip data to current directory \n",
    "    print (data_dir)\n",
    "    ! unzip /root/.keras/datasets/oa-sis.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CsEL-j0w5Gd-"
   },
   "outputs": [],
   "source": [
    "def load_training (path):\n",
    "    # load training images (non segmented) in the path and store in numpy array\n",
    "    image_list = []\n",
    "    for filename in glob.glob(path+'/*.png'): \n",
    "        im=image.imread (filename)\n",
    "        image_list.append(im)\n",
    "\n",
    "    print('train_X shape:',np.array(image_list).shape)\n",
    "    train_set = np.array(image_list, dtype=np.float32)\n",
    "    return train_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hnIbIVop69Dn"
   },
   "outputs": [],
   "source": [
    "def process_training (data_set):\n",
    "    # the method normalizes training images and adds 4th dimention \n",
    "\n",
    "    train_set = data_set\n",
    "    train_set = (train_set - np.mean(train_set))/ np.std(train_set)\n",
    "    train_set= (train_set- np.amin(train_set))/ np.amax(train_set- np.amin(train_set))\n",
    "    train_set = train_set [:,:,:,np.newaxis]\n",
    "    \n",
    "    return train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OQRjoY8HFUoi"
   },
   "outputs": [],
   "source": [
    "def load_labels (path):\n",
    "    # loads labels images and map pixel values to class indices and convert image data type to unit8 \n",
    "\n",
    "    n_classes = 4\n",
    "    image_list =[]\n",
    "    for filename in glob.glob(path+'/*.png'): \n",
    "        im=image.imread (filename)\n",
    "        one_hot = np.zeros((im.shape[0], im.shape[1]))\n",
    "        for i, unique_value in enumerate(np.unique(im)):\n",
    "          one_hot[:, :][im == unique_value] = i\n",
    "        image_list.append(one_hot)\n",
    "\n",
    "    print('train_y shape:',np.array(image_list).shape)\n",
    "    labels = np.array(image_list, dtype=np.uint8)\n",
    "    \n",
    "    pyplot.imshow(labels[2])\n",
    "    pyplot.show()\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xsmg4eHmLJY5"
   },
   "outputs": [],
   "source": [
    "def process_labels(seg_data):\n",
    "    # one hot encode label data and convert to numpy array\n",
    "    onehot_seg_data = []\n",
    "    for n in range(seg_data.shape[0]): \n",
    "      im = seg_data[n]\n",
    "      n_classes = 4\n",
    "      one_hot = np.zeros((im.shape[0], im.shape[1], n_classes),dtype=np.uint8)\n",
    "      for i, unique_value in enumerate(np.unique(im)):\n",
    "          one_hot[:, :, i][im == unique_value] = 1\n",
    "    onehot_seg_data.append(one_hot)\n",
    "    \n",
    "    onehot_seg_data =np.array(onehot_seg_data)\n",
    "    print (onehot_seg_data.dtype)\n",
    "    #print (np.unique(onehot_validate_Y))\n",
    "    print (onehot_seg_data.shape)\n",
    "\n",
    "    return onehot_seg_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jAFXCuq8hFaV"
   },
   "outputs": [],
   "source": [
    "# U-NET final model w on2Dtranspose and batch normalization after activation\n",
    "# questions do I need to use shuffle =true in fit module ?\n",
    "import tensorflow as tf\n",
    "\n",
    "def unet_model ():\n",
    "    filter_size=16 \n",
    "    input_layer = tf.keras.Input((256,256,1))\n",
    "    \n",
    "    pre_conv = tf.keras.layers.Conv2D(filter_size * 1, (3, 3), padding=\"same\")(input_layer)\n",
    "    pre_conv = tf.keras.layers.LeakyReLU(alpha=.01)(pre_conv)\n",
    "\n",
    "\n",
    "# context module 1 pre-activation residual block\n",
    "    conv1 = tf.keras.layers.BatchNormalization()(pre_conv)\n",
    "    conv1 = tf.keras.layers.LeakyReLU(alpha=.01)(conv1)\n",
    "    conv1 = tf.keras.layers.Conv2D(filter_size * 1, (3, 3), padding=\"same\" )(conv1) \n",
    "    conv1 = tf.keras.layers.Dropout(.3) (conv1)\n",
    "    conv1 = tf.keras.layers.BatchNormalization()(conv1)\n",
    "    conv1 = tf.keras.layers.LeakyReLU(alpha=.01)(conv1)\n",
    "    conv1 = tf.keras.layers.Conv2D(filter_size * 1, (3, 3), padding=\"same\")(conv1)    \n",
    "    conv1 = tf.keras.layers.Add()([pre_conv,conv1])\n",
    "    \n",
    "# downsample and double number of feature maps   \n",
    "    pool1 = tf.keras.layers.Conv2D(filter_size * 2, (3,3), (2,2) , padding='same')(conv1)\n",
    "    pool1 = tf.keras.layers.LeakyReLU(alpha=.01)(pool1)\n",
    "    \n",
    "# context module 2\n",
    "    conv2 = tf.keras.layers.BatchNormalization()(pool1)\n",
    "    conv2 = tf.keras.layers.LeakyReLU(alpha=.01)(conv2)\n",
    "    conv2 = tf.keras.layers.Conv2D(filter_size * 2, (3, 3), padding=\"same\")(conv2)\n",
    "    conv2 = tf.keras.layers.Dropout(.3) (conv2)  \n",
    "    conv2 = tf.keras.layers.BatchNormalization()(conv2)\n",
    "    conv2 = tf.keras.layers.LeakyReLU(alpha=.01)(conv2)\n",
    "    conv2 = tf.keras.layers.Conv2D(filter_size * 2, (3, 3), padding=\"same\")(conv2)\n",
    "    conv2 = tf.keras.layers.Add()([pool1,conv2])\n",
    "\n",
    "# downsample and double number of feature maps\n",
    "    pool2 = tf.keras.layers.Conv2D(filter_size*4, (3,3),(2,2), padding='same')(conv2)\n",
    "    pool2 = tf.keras.layers.LeakyReLU(alpha=.01)(pool2)\n",
    "\n",
    "# context module 3\n",
    "    conv3 = tf.keras.layers.BatchNormalization()(pool2)\n",
    "    conv3 = tf.keras.layers.LeakyReLU(alpha=.01)(conv3)\n",
    "    conv3 = tf.keras.layers.Conv2D(filter_size * 4, (3, 3), padding=\"same\")(conv3)\n",
    "    conv3 = tf.keras.layers.Dropout(.3) (conv3)\n",
    "    conv3 = tf.keras.layers.BatchNormalization()(conv3)\n",
    "    conv3 = tf.keras.layers.LeakyReLU(alpha=.01)(conv3)\n",
    "    conv3 = tf.keras.layers.Conv2D(filter_size * 4, (3, 3), padding=\"same\")(conv3)\n",
    "    conv3 = tf.keras.layers.Add()([pool2,conv3])\n",
    "\n",
    "# downsample and double number of feature maps\n",
    "    pool3 = tf.keras.layers.Conv2D(filter_size*8, (3,3),(2,2),padding='same')(conv3)\n",
    "    pool3 = tf.keras.layers.LeakyReLU(alpha=.01)(pool3)\n",
    "\n",
    "# context module 4\n",
    "    conv4 = tf.keras.layers.BatchNormalization()(pool3)\n",
    "    conv4 = tf.keras.layers.LeakyReLU(alpha=.01)(conv4)\n",
    "    conv4 = tf.keras.layers.Conv2D(filter_size * 8, (3, 3), padding=\"same\")(conv4)\n",
    "    conv4 = tf.keras.layers.Dropout(.3) (conv4)\n",
    "    conv4 = tf.keras.layers.BatchNormalization()(conv4)\n",
    "    conv4 = tf.keras.layers.LeakyReLU(alpha=.01)(conv4)\n",
    "    conv4 = tf.keras.layers.Conv2D(filter_size * 8, (3, 3), padding=\"same\")(conv4)\n",
    "    conv4 = tf.keras.layers.Add()([pool3,conv4])\n",
    "    print (\"conv4\",conv4.shape)\n",
    "\n",
    "# downsample and double number of feature maps\n",
    "    pool4 = tf.keras.layers.Conv2D(filter_size*16, (3,3),(2,2),padding='same')(conv4)\n",
    "    pool4 = tf.keras.layers.LeakyReLU(alpha=.01)(pool4) \n",
    "\n",
    "# context module 5\n",
    "    # Middle\n",
    "    convm = tf.keras.layers.BatchNormalization()(pool4)\n",
    "    convm = tf.keras.layers.LeakyReLU(alpha=.01)(convm)\n",
    "    convm = tf.keras.layers.Conv2D(filter_size * 16, (3, 3), padding=\"same\")(convm)\n",
    "    convm = tf.keras.layers.Dropout(.3) (convm)\n",
    "    convm = tf.keras.layers.BatchNormalization()(convm)\n",
    "    convm = tf.keras.layers.LeakyReLU(alpha=.01)(convm)\n",
    "    convm = tf.keras.layers.Conv2D(filter_size * 16, (3, 3), padding=\"same\")(convm)\n",
    "    convm = tf.keras.layers.Add()([pool4,convm])\n",
    "    print (\"convm\",convm.shape)\n",
    "\n",
    "#upsampling module 1\n",
    "    deconv4 = tf.keras.layers.UpSampling2D(size=(2,2) , interpolation='nearest')(convm)\n",
    "    deconv4 = tf.keras.layers.Conv2D (filter_size *8, (3, 3) , padding=\"same\")(deconv4)\n",
    "    deconv4 = tf.keras.layers.LeakyReLU(alpha=.01)(deconv4) \n",
    "    print (\"upsample 1\",deconv4.shape)\n",
    "\n",
    "#concatatinate layers \n",
    "    uconv4 = tf.keras.layers.concatenate([deconv4, conv4], axis=3)\n",
    "    print (\"concat 1\",uconv4.shape)\n",
    "\n",
    "#localization module 1\n",
    "    uconv4 = tf.keras.layers.Conv2D(filter_size * 16, (3, 3) , padding=\"same\")(uconv4)\n",
    "    uconv4 = tf.keras.layers.BatchNormalization()(uconv4)\n",
    "    uconv4 = tf.keras.layers.LeakyReLU(alpha=.01)(uconv4)\n",
    "    uconv4 = tf.keras.layers.Conv2D(filter_size * 8, (1, 1), padding=\"same\")(uconv4)\n",
    "    uconv4 = tf.keras.layers.BatchNormalization()(uconv4)\n",
    "    uconv4 = tf.keras.layers.LeakyReLU(alpha=.01)(uconv4)\n",
    "    print (\"localization 1\",uconv4.shape)\n",
    "\n",
    "#upsampling module 2\n",
    "    deconv3 = tf.keras.layers.UpSampling2D(size=(2,2) , interpolation='nearest')(uconv4)\n",
    "    deconv3 = tf.keras.layers.Conv2D (filter_size *4, (3, 3) , padding=\"same\")(deconv3)\n",
    "    deconv3 = tf.keras.layers.LeakyReLU(alpha=.01)(deconv3) \n",
    "    #print (\"upsample 2\",deconv3.shape)\n",
    "  \n",
    "\n",
    "# concatatinate layers  \n",
    "    uconv3 = tf.keras.layers.concatenate([deconv3, conv3], axis=3)\n",
    "    #print (\"concat 3\",uconv3.shape)\n",
    "\n",
    "# localization module 2\n",
    "    uconv3 = tf.keras.layers.Conv2D(filter_size * 8, (3, 3), padding=\"same\")(uconv3)\n",
    "    uconv3 = tf.keras.layers.BatchNormalization()(uconv3)\n",
    "    uconv3 = tf.keras.layers.LeakyReLU(alpha=.01)(uconv3)\n",
    "    uconv3 = tf.keras.layers.Conv2D(filter_size * 4, (1, 1), padding=\"same\")(uconv3)\n",
    "    uconv3 = tf.keras.layers.BatchNormalization()(uconv3)\n",
    "    uconv3 = tf.keras.layers.LeakyReLU(alpha=.01)(uconv3)\n",
    "    #print (\"localization 2\",uconv3.shape)\n",
    "\n",
    "    seg3 = tf.keras.layers.Conv2D(4, (3,3),  activation=\"softmax\", padding='same' )(uconv3)\n",
    "    seg3 = tf.keras.layers.UpSampling2D(size=(2,2) , interpolation='nearest')(seg3)\n",
    "    #print (\"seg3\",seg3.shape)\n",
    "\n",
    "# Upsample module 3\n",
    "    deconv2 = tf.keras.layers.UpSampling2D(size=(2,2) , interpolation='nearest')(uconv3)\n",
    "    deconv2 = tf.keras.layers.Conv2D (filter_size *2, (3, 3) , padding=\"same\")(deconv2)\n",
    "    deconv2 = tf.keras.layers.LeakyReLU(alpha=.01)(deconv2)\n",
    "    #print (\"upsample 3\",deconv2.shape)\n",
    "\n",
    "# concatination layer \n",
    "    uconv2 = tf.keras.layers.concatenate([deconv2, conv2], axis=3)\n",
    "    #print (\"concat 3\",uconv2.shape)\n",
    "\n",
    "# localization module 3\n",
    "    uconv2 = tf.keras.layers.Conv2D(filter_size * 4, (3, 3), padding=\"same\")(uconv2)\n",
    "    uconv2 = tf.keras.layers.BatchNormalization()(uconv2)\n",
    "    uconv2 = tf.keras.layers.LeakyReLU(alpha=.01)(uconv2)\n",
    "    uconv2 = tf.keras.layers.Conv2D(filter_size * 2, (1, 1), padding=\"same\")(uconv2)\n",
    "    uconv2 = tf.keras.layers.BatchNormalization()(uconv2)\n",
    "    uconv2 = tf.keras.layers.LeakyReLU(alpha=.01)(uconv2)\n",
    "    #print (\"localization 3\",uconv2.shape)\n",
    "\n",
    "    seg2 = tf.keras.layers.Conv2D(4, (3,3),  activation=\"softmax\", padding='same')(uconv2)\n",
    "    #print (\"seg2\",seg2.shape)\n",
    "    seg_32 = tf.keras.layers.Add()([seg3,seg2])\n",
    "\n",
    "    seg_32 = tf.keras.layers.UpSampling2D(size=(2,2) , interpolation='nearest')(seg_32)\n",
    "    #print (\"seg_32\",seg_32.shape)\n",
    "\n",
    "# Upsample module 4\n",
    "    deconv1 = tf.keras.layers.UpSampling2D(size=(2,2) , interpolation='nearest')(uconv2)\n",
    "    deconv1 = tf.keras.layers.Conv2D (filter_size *1, (3, 3) , padding=\"same\")(deconv1)\n",
    "    deconv1 = tf.keras.layers.LeakyReLU(alpha=.01)(deconv1)\n",
    "    #print (\"upsample 4\",deconv1.shape)\n",
    "\n",
    "# concatination layer\n",
    "    uconv1 = tf.keras.layers.concatenate([deconv1, conv1], axis=3)\n",
    "    #print (\"concat 4\",uconv1.shape)\n",
    "\n",
    "    uconv1 = tf.keras.layers.Conv2D(filter_size * 2, (3, 3), padding=\"same\")(uconv1)\n",
    "    #uconv1 = tf.keras.layers.BatchNormalization()(uconv1)\n",
    "    uconv1 = tf.keras.layers.LeakyReLU(alpha=.01)(uconv1)\n",
    "    \n",
    "    #print (\"conv before last activation\",uconv1.shape)\n",
    "    \n",
    "    seg1 = tf.keras.layers.Conv2D(4, (3,3),  activation=\"softmax\", padding='same' )(uconv1)\n",
    "    #print (\"seg 1\",seg1.shape)\n",
    "\n",
    "    seg_sum = tf.keras.layers.Add()([seg1,seg_32])\n",
    "\n",
    "\n",
    "    output_layer = tf.keras.layers.Conv2D(4, (3,3), padding='same' ,activation=\"softmax\")(seg_sum)\n",
    "    \n",
    "    model = tf.keras.Model( input_layer , outputs=output_layer)\n",
    "\n",
    "\n",
    "    \n",
    "    return model\n",
    "\n",
    "#'CategoricalCrossentropy'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kqPbya4v4I1D",
    "outputId": "971aaa0c-4bdc-44aa-eeda-55393c7e5dae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv4 (None, 32, 32, 128)\n",
      "convm (None, 16, 16, 256)\n",
      "upsample 1 (None, 32, 32, 128)\n",
      "concat 1 (None, 32, 32, 256)\n",
      "localization 1 (None, 32, 32, 128)\n"
     ]
    }
   ],
   "source": [
    "# test script load and preprocess training images\n",
    "\n",
    "# download oasis data and unzip files \n",
    "download_oasis()\n",
    "\n",
    "# load training data set  \n",
    "train_X = load_training ('/content/keras_png_slices_data/keras_png_slices_train')\n",
    "\n",
    "# check loaded image\n",
    "pyplot.imshow(train_X[2])\n",
    "pyplot.show()\n",
    "\n",
    "# pre- process training dataset \n",
    "train_X = process_training(train_X)\n",
    "\n",
    "# load validaton data set and process it \n",
    "validate_X = load_training ('/content/keras_png_slices_data/keras_png_slices_validate')\n",
    "\n",
    "# check loaded images\n",
    "pyplot.imshow(validate_X[2])\n",
    "pyplot.show()\n",
    "\n",
    "# pre process validation data set\n",
    "validate_X = process_training(validate_X)\n",
    "\n",
    "\n",
    "# load test data set and process it \n",
    "test_X = load_training ('/content/keras_png_slices_data/keras_png_slices_test')\n",
    "\n",
    "# check loaded images\n",
    "pyplot.imshow(test_X[2])\n",
    "pyplot.show()\n",
    "\n",
    "# pre process test data set\n",
    "test_X = process_training(test_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load and pre process segmented training set \n",
    "train_Y = load_labels('/content/keras_png_slices_data/keras_png_slices_seg_train')\n",
    "train_Y = process_labels(train_Y)\n",
    "\n",
    "# check loaded images\n",
    "pyplot.imshow(train_Y[2,:,:,3])\n",
    "pyplot.show()\n",
    "\n",
    "\n",
    "# load and pre process segmented validation set \n",
    "validate_Y = load_labels('/content/keras_png_slices_data/keras_png_slices_seg_validate')\n",
    "validate_Y = process_labels(validate_Y)\n",
    " \n",
    "# check loaded images\n",
    "pyplot.imshow(validate_Y[2,:,:,3])\n",
    "pyplot.show()\n",
    "\n",
    "# load and pre process segmented test set \n",
    "test_Y = load_labels('/content/keras_png_slices_data/keras_png_slices_seg_test')\n",
    "test_Y = process_labels(test_Y)\n",
    " \n",
    "# check loaded images\n",
    "pyplot.imshow(test_Y[2,:,:,3])\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model and set training paramters \n",
    "\n",
    "# create a model instance and set training paramters \n",
    "\n",
    "model = unet_model()\n",
    "opt= tf.keras.optimizers.Adam (learning_rate=.0005)\n",
    "model.compile (optimizer=opt, loss= 'CategoricalCrossentropy' , metrics=['accuracy'])\n",
    "\n",
    "# set early stop criteria \n",
    "ES = tf.keras.callbacks.EarlyStopping( monitor='val_accuracy',min_delta=.0001, patience=40, verbose=0, mode='max', restore_best_weights=True)\n",
    "\n",
    "# record history of training to display loss over ephocs \n",
    "history = model.fit(train_X, train_Y,  validation_data= (validate_X, validate_Y) ,batch_size=32,shuffle='True',epochs=2, callbacks=[ES])\n",
    "\n",
    "# evaluate against testing data \n",
    "model.evaluate(test_X,test_Y)\n",
    "\n",
    "# save trained model weights \n",
    "model.save_weights('/content/drive/My Drive/modelweights/unet8')\n",
    "\n",
    "\n",
    "# plot training and validation loss \n",
    "pyplot.title('Categorical Cross Entropy Loss')\n",
    "pyplot.plot(history.history['loss'], color='blue', label='train')\n",
    "pyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
    "pyplot.legend(('training','validation'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = unet_model()\n",
    "opt= tf.keras.optimizers.Adam (learning_rate=.0005)\n",
    "loaded_model.compile (optimizer=opt, loss= 'CategoricalCrossentropy' , metrics=['accuracy'])\n",
    "loaded_model.load_weights('/content/drive/My Drive/modelweights/unet8')\n",
    "loaded_model.evaluate (test_X,test_Y)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Methods.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
