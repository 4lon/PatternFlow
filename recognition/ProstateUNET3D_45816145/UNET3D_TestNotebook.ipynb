{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a99951d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import nibabel as nib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1518553f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Labelled Scans: 211\n"
     ]
    }
   ],
   "source": [
    "# Formulate train, val, test file paths for \"scans\" and \"labels\"\n",
    "# Stored as filepaths, as the generator will do the file reading\n",
    "def get_nifti_files_in(directory):\n",
    "    paths = []\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".nii.gz\"):\n",
    "            paths.append(os.path.join(directory, file))\n",
    "    \n",
    "    # Sort so ordering is not file system order dependent\n",
    "    return sorted(paths)\n",
    "\n",
    "base_path = \"data\"\n",
    "#data_dimensions = (256, 256, 128)\n",
    "data_dimensions = (32, 32, 32)\n",
    "\n",
    "# Background, Body, Bone, Bladder, Rectum, Prostate\n",
    "class_count = 6\n",
    "\n",
    "scans = get_nifti_files_in(os.path.join(base_path, \"semantic_MRs_anon\"))\n",
    "labels = get_nifti_files_in(os.path.join(base_path, \"semantic_labels_anon\"))\n",
    "\n",
    "# Make sure we have an equal number of scans and labels\n",
    "assert len(scans) == len(labels)\n",
    "\n",
    "# Zips the two \"scans\" and \"labels\" arrays together to produce [[scan_filename, label_filename], ...]\n",
    "data_paths = np.dstack((scans, labels))[0]\n",
    "\n",
    "print(\"Raw Labelled Scans: \" + str(len(data_paths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "025588ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient Count: 38\n"
     ]
    }
   ],
   "source": [
    "# Categorise input data into a form which represents the patient heirarchy, to not train and test on the same patient!\n",
    "def generate_patient_data_heirarchy(data_paths):\n",
    "    # Just incase patient IDs aren't continuous, i will use a dictionary mapping from id -> [[scan_path, label_path]]\n",
    "    patient_data = {}\n",
    "    \n",
    "    for scan_path, label_path in data_paths:\n",
    "        # Use scan filename to parse out patient ID\n",
    "        scan_file = os.path.basename(scan_path)\n",
    "        # Using scan formatting of \"Case_[patient_id]_...\"\n",
    "        patient_id = int(scan_file.split(\"_\")[1])\n",
    "        \n",
    "        # Simple dict, array population with first time checking\n",
    "        if patient_id in patient_data:\n",
    "            patient_data[patient_id].append([scan_path, label_path])\n",
    "        else:\n",
    "            patient_data[patient_id] = [[scan_path, label_path]]\n",
    "        \n",
    "    return patient_data\n",
    "\n",
    "patient_data = generate_patient_data_heirarchy(data_paths)\n",
    "\n",
    "print(\"Patient Count: \" + str(len(patient_data.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bf13e3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting dataset using Train: 0.85 Val: 0.1 Test: 0.05\n",
      "Train Count: 177\n",
      "Val Count: 24\n",
      "Test Count: 10\n"
     ]
    }
   ],
   "source": [
    "# Create Train, Val, Test split (move into utility function)\n",
    "def train_val_test_split(train_split, val_split, test_split, patient_data):\n",
    "    print(\"Splitting dataset using Train: \" + str(train_split) + \n",
    "          \" Val: \" + str(val_split) + \" Test: \" + str(test_split))\n",
    "\n",
    "    assert train_split + val_split + test_split == 1.0\n",
    "\n",
    "    patient_ids = patient_data.keys()\n",
    "    # Randomise the order in which we loop over the data - gives a more random distribution of data in each bucket\n",
    "    np.random.shuffle(data_paths)\n",
    "    \n",
    "    train_data = []\n",
    "    val_data = []\n",
    "    test_data = []\n",
    "    \n",
    "    # The method works by calculating the percent allocated after each allocation,\n",
    "    # Then assigning more data to the bucket which is FURTHEST BELOW the desired\n",
    "    # Allocation. This iterative method will give us a good allocation, which is close enough to the desired one.\n",
    "    for patient_id in patient_ids:\n",
    "        # Clamp to minimum value of 1 so we don't have to handle div by 0 on first iteration\n",
    "        assigned_count = max(len(train_data) + len(val_data) + len(test_data), 1)\n",
    "        \n",
    "        # Calculated by (desired allocation percent - (no. allocated to that bucket / total allocated))\n",
    "        train_displacement = train_split - (len(train_data) / assigned_count)\n",
    "        val_displacement = val_split - (len(val_data) / assigned_count)\n",
    "        test_displacement = test_split - (len(test_data) / assigned_count)\n",
    "        \n",
    "        # Allocate this patient to the bucket furthest from allocation (i.e. max displacement)\n",
    "        if train_displacement > val_displacement and train_displacement > test_displacement:\n",
    "            # Use extend so train_data is flat\n",
    "            train_data.extend(patient_data[patient_id])\n",
    "        elif val_displacement > test_displacement:\n",
    "            val_data.extend(patient_data[patient_id])\n",
    "        else:\n",
    "            test_data.extend(patient_data[patient_id])\n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "train_paths, val_paths, test_paths = train_val_test_split(0.85, 0.1, 0.05, patient_data)\n",
    "\n",
    "# Sanity checks\n",
    "source_data_length = len(data_paths)\n",
    "assert len(train_paths) + len(val_paths) + len(test_paths) == source_data_length\n",
    "\n",
    "print(\"Train Count: \" + str(len(train_paths)))\n",
    "print(\"Val Count: \" + str(len(val_paths)))\n",
    "print(\"Test Count: \" + str(len(test_paths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8bccd68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator:\n",
    "# Maximum scan voxel value in dataset was above 512 but below 1023 so i use closest power of two -> 1023, for normalisation\n",
    "class Prostate3DGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, data_paths, batch_size, data_dimensions, class_count):\n",
    "        self.data_paths = data_paths\n",
    "        self.batch_size = batch_size\n",
    "        self.data_dimensions = data_dimensions\n",
    "        self.class_count = class_count\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.data_paths) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        start_index = index * self.batch_size\n",
    "        batch_data_paths = self.data_paths[start_index : start_index + self.batch_size]\n",
    "        \n",
    "        scans = np.empty((self.batch_size, *self.data_dimensions))\n",
    "        labels = np.empty((self.batch_size, *self.data_dimensions, self.class_count), dtype=int)\n",
    "        \n",
    "        for dataIndex in range(len(batch_data_paths)):\n",
    "            # Populate \"scans\"\n",
    "            #scan_voxels = nib.load(batch_data_paths[dataIndex][0])\n",
    "            #scans[dataIndex] = tf.cast(np.array(scan_voxels.dataobj) / 1023.0, tf.float32)\n",
    "            \n",
    "            scan_voxels = nib.load(batch_data_paths[dataIndex][0])\n",
    "            np_scan_voxels = np.array(scan_voxels.dataobj)[112:144, 112:144, 48:80]\n",
    "            scans[dataIndex] = tf.cast(np_scan_voxels / 1023.0, tf.float32)\n",
    "            \n",
    "            # Populate \"labels\"\n",
    "            #nibabel_voxels = nib.load(batch_data_paths[dataIndex][1])\n",
    "            #prepared_voxels = tf.cast(np.array(nibabel_voxels.dataobj), tf.float32)\n",
    "            #labels[dataIndex] = keras.utils.to_categorical(prepared_voxels, num_classes=self.class_count)\n",
    "            \n",
    "            nibabel_voxels = nib.load(batch_data_paths[dataIndex][1])\n",
    "            np_nibabel_voxels = np.array(nibabel_voxels.dataobj)[112:144, 112:144, 48:80]\n",
    "            prepared_voxels = tf.cast(np_nibabel_voxels, tf.float32)\n",
    "            labels[dataIndex] = keras.utils.to_categorical(prepared_voxels, num_classes=self.class_count)\n",
    "            \n",
    "        return scans, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c9e76c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize generators\n",
    "batch_size = 1\n",
    "\n",
    "train_generator = Prostate3DGenerator(train_paths, batch_size, data_dimensions, class_count)\n",
    "val_generator = Prostate3DGenerator(val_paths, batch_size, data_dimensions, class_count)\n",
    "test_generator = Prostate3DGenerator(test_paths, batch_size, data_dimensions, class_count)\n",
    "\n",
    "#item = train_generator.__getitem__(0)\n",
    "#plt.imshow(item[0][0][15], interpolation='nearest')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ddd7687f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 32, 32, 32,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_44 (Conv3D)              (None, 32, 32, 32, 6 1792        input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 32, 32, 32, 6 256         conv3d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_45 (Conv3D)              (None, 32, 32, 32, 6 110656      batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_8 (MaxPooling3D)  (None, 16, 16, 16, 6 0           conv3d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 16, 16, 16, 6 0           max_pooling3d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_46 (Conv3D)              (None, 16, 16, 16, 1 221312      dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 16, 16, 16, 1 512         conv3d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_47 (Conv3D)              (None, 16, 16, 16, 1 442496      batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_9 (MaxPooling3D)  (None, 8, 8, 8, 128) 0           conv3d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 8, 8, 8, 128) 0           max_pooling3d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_48 (Conv3D)              (None, 8, 8, 8, 256) 884992      dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 8, 8, 8, 256) 1024        conv3d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_49 (Conv3D)              (None, 8, 8, 8, 256) 1769728     batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_transpose_8 (Conv3DTrans (None, 16, 16, 16, 1 262272      conv3d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 16, 16, 16, 1 0           conv3d_transpose_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 16, 16, 16, 2 0           dropout_18[0][0]                 \n",
      "                                                                 conv3d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_50 (Conv3D)              (None, 16, 16, 16, 1 884864      concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 16, 16, 16, 1 512         conv3d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_51 (Conv3D)              (None, 16, 16, 16, 1 442496      batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_transpose_9 (Conv3DTrans (None, 32, 32, 32, 6 65600       conv3d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 32, 32, 32, 6 0           conv3d_transpose_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 32, 32, 32, 1 0           dropout_19[0][0]                 \n",
      "                                                                 conv3d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_52 (Conv3D)              (None, 32, 32, 32, 6 221248      concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 32, 32, 32, 6 256         conv3d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_53 (Conv3D)              (None, 32, 32, 32, 6 110656      batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_54 (Conv3D)              (None, 32, 32, 32, 6 390         conv3d_53[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 5,421,062\n",
      "Trainable params: 5,419,782\n",
      "Non-trainable params: 1,280\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Construct keras model\n",
    "first_layer = layers.Input((*data_dimensions, 1))\n",
    "\n",
    "# We will pass through \"previous\" to represent last layer\n",
    "previous = first_layer\n",
    "\n",
    "filters = [64, 128, 256]\n",
    "\n",
    "# LEFT SIDE\n",
    "downscale_layers = len(filters)\n",
    "downscale_filters = filters\n",
    "downscale_tails = []\n",
    "for i in range(downscale_layers):\n",
    "    previous = layers.Conv3D(downscale_filters[i], (3, 3, 3), padding='same', activation='relu')(previous)\n",
    "    previous = layers.BatchNormalization()(previous)\n",
    "    previous = layers.Conv3D(downscale_filters[i], (3, 3, 3), padding='same', activation='relu')(previous)\n",
    "    \n",
    "    if i != downscale_layers - 1:\n",
    "        downscale_tails.append(previous)\n",
    "        previous = layers.MaxPool3D((2, 2, 2), strides=(2, 2, 2))(previous)\n",
    "        previous = layers.Dropout(0.2)(previous)\n",
    "        \n",
    "# RIGHT SIDE\n",
    "# reverse references, since upscale is looped other way\n",
    "downscale_tails = list(reversed(downscale_tails))\n",
    "\n",
    "upscale_layers = len(filters) - 1\n",
    "upscale_filters = list(reversed(filters))[1:]\n",
    "for i in range(upscale_layers):\n",
    "    # Up Convolution\n",
    "    previous = layers.Conv3DTranspose(upscale_filters[i], (2, 2, 2), strides=(2, 2, 2))(previous)\n",
    "    previous = layers.Dropout(0.2)(previous)\n",
    "    \n",
    "    # Pull across\n",
    "    tail = downscale_tails[i]\n",
    "    previous = layers.concatenate([previous, tail])\n",
    "    \n",
    "    # Convolutions\n",
    "    previous = layers.Conv3D(upscale_filters[i], (3, 3, 3), padding='same', activation='relu')(previous)\n",
    "    previous = layers.BatchNormalization()(previous)\n",
    "    previous = layers.Conv3D(upscale_filters[i], (3, 3, 3), padding='same', activation='relu')(previous)\n",
    "        \n",
    "last_layer = layers.Conv3D(class_count, (1, 1, 1), activation='softmax')(previous)\n",
    "\n",
    "model = keras.Model(first_layer, last_layer)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e5826e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function and coefficients to be used during training:\n",
    "def dice_coefficient(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    #y_pred = tf.cast(y_pred, tf.float32)\n",
    "    \n",
    "    smoothing_factor = 1\n",
    "    flat_y_true = K.flatten(y_true)\n",
    "    flat_y_pred = K.flatten(y_pred)\n",
    "    #print(flat_y_pred.shape)\n",
    "    return (2. * K.sum(flat_y_true * flat_y_pred) + smoothing_factor) / (K.sum(flat_y_true) + K.sum(flat_y_pred) + smoothing_factor)\n",
    "\n",
    "def dice_coefficient_loss(y_true, y_pred):\n",
    "    return 1 - dice_coefficient(y_true, y_pred)\n",
    "\n",
    "#item = train_generator.__getitem__(0)\n",
    "#dice_coefficient(item[1][0], item[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "482884a5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      " 33/177 [====>.........................] - ETA: 27s - loss: 0.4818 - accuracy: 0.5308"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2348/1477068066.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m#model.load_weights(\"oasis.h5\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\Users\\Jake\\miniconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                 _r=1):\n\u001b[0;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Users\\Jake\\miniconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Users\\Jake\\miniconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Users\\Jake\\miniconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3021\u001b[0m       (graph_function,\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3023\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Users\\Jake\\miniconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1960\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mE:\\Users\\Jake\\miniconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Users\\Jake\\miniconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=\"adam\", loss=dice_coefficient_loss, metrics=[\"accuracy\"])\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"oasis.h5\", save_best_only=True)\n",
    "]\n",
    "\n",
    "# Train the model, doing validation at the end of each epoch.\n",
    "epochs = 15\n",
    "\n",
    "#model.load_weights(\"oasis.h5\")\n",
    "train_data = model.fit(train_generator, epochs=epochs, validation_data=val_generator, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3d84844d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 2s 243ms/step - loss: 0.1908 - accuracy: 0.8094\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1907830834388733, 0.8094143271446228]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_generator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
