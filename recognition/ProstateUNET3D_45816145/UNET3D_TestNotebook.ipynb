{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a99951d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import nibabel as nib\n",
    "\n",
    "from model import unet3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1518553f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formulate train, val, test file paths for \"scans\" and \"labels\"\n",
    "# Stored as filepaths, as the generator will do the file reading\n",
    "def get_nifti_files_in(directory):\n",
    "    paths = []\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".nii.gz\"):\n",
    "            paths.append(os.path.join(directory, file))\n",
    "    \n",
    "    # Sort so ordering is not file system order dependent\n",
    "    return sorted(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efd15d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorise input data into a form which represents the patient heirarchy, to not train and test on the same patient!\n",
    "def generate_patient_data_heirarchy(data_paths):\n",
    "    # Just incase patient IDs aren't continuous, i will use a dictionary mapping from id -> [[scan_path, label_path]]\n",
    "    patient_data = {}\n",
    "    \n",
    "    for scan_path, label_path in data_paths:\n",
    "        # Use scan filename to parse out patient ID\n",
    "        scan_file = os.path.basename(scan_path)\n",
    "        # Using scan formatting of \"Case_[patient_id]_...\"\n",
    "        patient_id = int(scan_file.split(\"_\")[1])\n",
    "        \n",
    "        # Simple dict, array population with first time checking\n",
    "        if patient_id in patient_data:\n",
    "            patient_data[patient_id].append([scan_path, label_path])\n",
    "        else:\n",
    "            patient_data[patient_id] = [[scan_path, label_path]]\n",
    "        \n",
    "    return patient_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf13e3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Train, Val, Test split (move into utility function)\n",
    "def train_val_test_split(train_split, val_split, test_split, patient_data):\n",
    "    print(\"Splitting dataset using Train: \" + str(train_split) + \n",
    "          \" Val: \" + str(val_split) + \" Test: \" + str(test_split))\n",
    "\n",
    "    assert train_split + val_split + test_split == 1.0\n",
    "\n",
    "    patient_ids = list(patient_data.keys())\n",
    "    # Randomise the order in which we loop over the data - gives a more random distribution of data in each bucket\n",
    "    np.random.shuffle(patient_ids)\n",
    "    \n",
    "    train_data = []\n",
    "    val_data = []\n",
    "    test_data = []\n",
    "    \n",
    "    # The method works by calculating the percent allocated after each allocation,\n",
    "    # Then assigning more data to the bucket which is FURTHEST BELOW the desired\n",
    "    # Allocation. This iterative method will give us a good allocation, which is close enough to the desired one.\n",
    "    for patient_id in patient_ids:\n",
    "        # Clamp to minimum value of 1 so we don't have to handle div by 0 on first iteration\n",
    "        assigned_count = max(len(train_data) + len(val_data) + len(test_data), 1)\n",
    "        \n",
    "        # Calculated by (desired allocation percent - (no. allocated to that bucket / total allocated))\n",
    "        train_displacement = train_split - (len(train_data) / assigned_count)\n",
    "        val_displacement = val_split - (len(val_data) / assigned_count)\n",
    "        test_displacement = test_split - (len(test_data) / assigned_count)\n",
    "        \n",
    "        # Allocate this patient to the bucket furthest from allocation (i.e. max displacement)\n",
    "        if train_displacement > val_displacement and train_displacement > test_displacement:\n",
    "            # Use extend so train_data is flat\n",
    "            train_data.extend(patient_data[patient_id])\n",
    "        elif val_displacement > test_displacement:\n",
    "            val_data.extend(patient_data[patient_id])\n",
    "        else:\n",
    "            test_data.extend(patient_data[patient_id])\n",
    "    \n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bccd68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator:\n",
    "# Maximum scan voxel value in dataset was above 512 but below 1023 so i use closest power of two -> 1023, for normalisation\n",
    "class Prostate3DGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, data_paths, batch_size, data_dimensions, class_count):\n",
    "        self.data_paths = data_paths\n",
    "        self.batch_size = batch_size\n",
    "        self.data_dimensions = data_dimensions\n",
    "        self.class_count = class_count\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.data_paths) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        start_index = index * self.batch_size\n",
    "        batch_data_paths = self.data_paths[start_index : start_index + self.batch_size]\n",
    "        \n",
    "        scans = np.empty((self.batch_size, *self.data_dimensions))\n",
    "        labels = np.empty((self.batch_size, *self.data_dimensions, self.class_count), dtype=int)\n",
    "        \n",
    "        for dataIndex in range(len(batch_data_paths)):\n",
    "            # Populate \"scans\"\n",
    "            scan_voxels = nib.load(batch_data_paths[dataIndex][0])\n",
    "            scans[dataIndex] = tf.cast(np.array(scan_voxels.dataobj) / 1023.0, tf.float32)\n",
    "            \n",
    "            #scan_voxels = nib.load(batch_data_paths[dataIndex][0])\n",
    "            #np_scan_voxels = np.array(scan_voxels.dataobj)[96:160, 96:160, 32:96]\n",
    "            #scans[dataIndex] = tf.cast(np_scan_voxels / 1023.0, tf.float32)\n",
    "            \n",
    "            # Populate \"labels\"\n",
    "            nibabel_voxels = nib.load(batch_data_paths[dataIndex][1])\n",
    "            prepared_voxels = tf.cast(np.array(nibabel_voxels.dataobj), tf.float32)\n",
    "            labels[dataIndex] = keras.utils.to_categorical(prepared_voxels, num_classes=self.class_count)\n",
    "            \n",
    "            #nibabel_voxels = nib.load(batch_data_paths[dataIndex][1])\n",
    "            #np_nibabel_voxels = np.array(nibabel_voxels.dataobj)[96:160, 96:160, 32:96]\n",
    "            #prepared_voxels = tf.cast(np_nibabel_voxels, tf.float32)\n",
    "            #labels[dataIndex] = keras.utils.to_categorical(prepared_voxels, num_classes=self.class_count)\n",
    "            \n",
    "        return scans, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9e76c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generators(scans_directory, labels_directory):\n",
    "    scans = get_nifti_files_in(scans_directory)\n",
    "    labels = get_nifti_files_in(labels_directory)\n",
    "\n",
    "    # Make sure we have an equal number of scans and labels\n",
    "    assert len(scans) == len(labels)\n",
    "\n",
    "    # Zips the two \"scans\" and \"labels\" arrays together to produce [[scan_filename, label_filename], ...]\n",
    "    data_paths = np.dstack((scans, labels))[0]\n",
    "\n",
    "    print(\"Raw Labelled Scans: \" + str(len(data_paths)))\n",
    "\n",
    "\n",
    "\n",
    "    # Categorise data paths into patient based buckets\n",
    "    patient_data = generate_patient_data_heirarchy(data_paths)\n",
    "\n",
    "    print(\"Patient Count: \" + str(len(patient_data.keys())))\n",
    "\n",
    "\n",
    "\n",
    "    # Formulate training splits using patient categorised data\n",
    "    train_paths, val_paths, test_paths = train_val_test_split(0.85, 0.1, 0.05, patient_data)\n",
    "\n",
    "    # Sanity checks\n",
    "    source_data_length = len(data_paths)\n",
    "    assert len(train_paths) + len(val_paths) + len(test_paths) == source_data_length\n",
    "\n",
    "    print(\"Train Count: \" + str(len(train_paths)))\n",
    "    print(\"Val Count: \" + str(len(val_paths)))\n",
    "    print(\"Test Count: \" + str(len(test_paths)))\n",
    "\n",
    "\n",
    "\n",
    "    # Initialize generators\n",
    "    batch_size = 1\n",
    "\n",
    "    train_generator = Prostate3DGenerator(train_paths, batch_size, data_dimensions, class_count)\n",
    "    val_generator = Prostate3DGenerator(val_paths, batch_size, data_dimensions, class_count)\n",
    "    test_generator = Prostate3DGenerator(test_paths, batch_size, data_dimensions, class_count)\n",
    "    \n",
    "    return train_generator, val_generator, test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "796babba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function and coefficients to be used during training:\n",
    "def dice_coefficient(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    #y_pred = tf.cast(y_pred, tf.float32)\n",
    "    \n",
    "    smoothing_factor = 1\n",
    "    flat_y_true = K.flatten(y_true)\n",
    "    flat_y_pred = K.flatten(y_pred)\n",
    "    #print(flat_y_pred.shape)\n",
    "    return (2. * K.sum(flat_y_true * flat_y_pred) + smoothing_factor) / (K.sum(flat_y_true) + K.sum(flat_y_pred) + smoothing_factor)\n",
    "\n",
    "def dice_coefficient_loss(y_true, y_pred):\n",
    "    return 1 - dice_coefficient(y_true, y_pred)\n",
    "\n",
    "#item = train_generator.__getitem__(0)\n",
    "#dice_coefficient(item[1][0], item[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f6a5deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Labelled Scans: 211\n",
      "Patient Count: 38\n",
      "Splitting dataset using Train: 0.85 Val: 0.1 Test: 0.05\n",
      "Train Count: 178\n",
      "Val Count: 24\n",
      "Test Count: 9\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 256, 256, 12 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv3d (Conv3D)                 (None, 256, 256, 128 448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 256, 256, 128 64          conv3d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_1 (Conv3D)               (None, 256, 256, 128 6928        batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d (MaxPooling3D)    (None, 128, 128, 64, 0           conv3d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 128, 128, 64, 0           max_pooling3d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_2 (Conv3D)               (None, 128, 128, 64, 13856       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 64, 128         conv3d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_3 (Conv3D)               (None, 128, 128, 64, 27680       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3D)  (None, 64, 64, 32, 3 0           conv3d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 64, 64, 32, 3 0           max_pooling3d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_4 (Conv3D)               (None, 64, 64, 32, 6 55360       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64, 64, 32, 6 256         conv3d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_5 (Conv3D)               (None, 64, 64, 32, 6 110656      batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_transpose (Conv3DTranspo (None, 128, 128, 64, 16416       conv3d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128, 128, 64, 0           conv3d_transpose[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 128, 128, 64, 0           dropout_2[0][0]                  \n",
      "                                                                 conv3d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_6 (Conv3D)               (None, 128, 128, 64, 55328       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 128, 128, 64, 128         conv3d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_7 (Conv3D)               (None, 128, 128, 64, 27680       batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_transpose_1 (Conv3DTrans (None, 256, 256, 128 4112        conv3d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 256, 256, 128 0           conv3d_transpose_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 256, 256, 128 0           dropout_3[0][0]                  \n",
      "                                                                 conv3d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_8 (Conv3D)               (None, 256, 256, 128 13840       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 256, 256, 128 64          conv3d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_9 (Conv3D)               (None, 256, 256, 128 6928        batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_10 (Conv3D)              (None, 256, 256, 128 102         conv3d_9[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 339,974\n",
      "Trainable params: 339,654\n",
      "Non-trainable params: 320\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "base_path = \"data\"\n",
    "data_dimensions = (256, 256, 128)\n",
    "#data_dimensions = (64, 64, 64)\n",
    "\n",
    "# Background, Body, Bone, Bladder, Rectum, Prostate\n",
    "class_count = 6\n",
    "\n",
    "scans_directory = os.path.join(base_path, \"semantic_MRs_anon\")\n",
    "labels_directory = os.path.join(base_path, \"semantic_labels_anon\")\n",
    "train_generator, val_generator, test_generator = build_generators(scans_directory, labels_directory)\n",
    "\n",
    "model = unet3d([16, 32, 64], data_dimensions, class_count)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "482884a5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14248/2853597766.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m#model.load_weights(\"oasis.h5\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\Users\\Jake\\miniconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                 _r=1):\n\u001b[0;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Users\\Jake\\miniconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Users\\Jake\\miniconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    948\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    951\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Users\\Jake\\miniconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3021\u001b[0m       (graph_function,\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3023\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Users\\Jake\\miniconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1960\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mE:\\Users\\Jake\\miniconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Users\\Jake\\miniconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=\"adam\", loss=dice_coefficient_loss, metrics=[\"accuracy\"])\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"model.h5\", save_best_only=True)\n",
    "]\n",
    "\n",
    "# Train the model, doing validation at the end of each epoch.\n",
    "epochs = 15\n",
    "\n",
    "#model.load_weights(\"oasis.h5\")\n",
    "train_data = model.fit(train_generator, epochs=epochs, validation_data=val_generator, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e2ca33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_generator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
