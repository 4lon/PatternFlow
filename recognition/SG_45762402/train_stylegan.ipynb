{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55f048ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import random\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable, grad\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da8284d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting import_ipynb\n",
      "  Downloading import-ipynb-0.1.3.tar.gz (4.0 kB)\n",
      "Building wheels for collected packages: import-ipynb\n",
      "  Building wheel for import-ipynb (setup.py): started\n",
      "  Building wheel for import-ipynb (setup.py): finished with status 'done'\n",
      "  Created wheel for import-ipynb: filename=import_ipynb-0.1.3-py3-none-any.whl size=2975 sha256=07e86dd1ace146484249e4ab1ab7e6a868b8502f01e1b455513a90a184079edc\n",
      "  Stored in directory: c:\\users\\shane\\appdata\\local\\pip\\cache\\wheels\\06\\7e\\ad\\1cb03e935234186825cefc7e2c8f3451b4f654b5bc72232a7b\n",
      "Successfully built import-ipynb\n",
      "Installing collected packages: import-ipynb\n",
      "Successfully installed import-ipynb-0.1.3\n"
     ]
    }
   ],
   "source": [
    "#!pip install import_ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1689adba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def requires_grad(model, flag=True):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = flag\n",
    "\n",
    "\n",
    "def accumulate(model1, model2, decay=0.999):\n",
    "    par1 = dict(model1.named_parameters())\n",
    "    par2 = dict(model2.named_parameters())\n",
    "\n",
    "    for k in par1.keys():\n",
    "        par1[k].data.mul_(decay).add_(1 - decay, par2[k].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dc40d325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data(dataset, batch_size, image_size=4):\n",
    "    '''\n",
    "    dataset.resolution = image_size\n",
    "    loader = DataLoader(dataset, shuffle=True, batch_size=batch_size, num_workers=1, drop_last=True)\n",
    "    \n",
    "    '''\n",
    "    transform = transforms.Compose([\n",
    "            transforms.Resize(image_size),          # Resize to the same size\n",
    "            transforms.CenterCrop(image_size),      # Crop to get square area\n",
    "            transforms.RandomHorizontalFlip(),      # Increase number of samples\n",
    "            transforms.ToTensor(),            \n",
    "            transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                                 (0.5, 0.5, 0.5))])\n",
    "\n",
    "    dataset.transform = transform\n",
    "    loader = DataLoader(dataset, shuffle=True, batch_size=batch_size, num_workers=1,drop_last=True)\n",
    "\n",
    "\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8de00424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_lr(optimizer, lr):\n",
    "    for group in optimizer.param_groups:\n",
    "        mult = group.get('mult', 1)\n",
    "        group['lr'] = lr * mult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8dc5ee6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef imshow(tensor, i):\\n    grid = tensor[0]\\n    grid.clamp_(-1, 1).add_(1).div_(2)\\n    # Add 0.5 after unnormalizing to [0, 255] to round to nearest integer\\n    ndarr = grid.mul_(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).to('cpu', torch.uint8).numpy()\\n    img = Image.fromarray(ndarr)\\n    img.save(f'{save_folder_path}sample-iter{i}.png')\\n    plt.imshow(img)\\n    plt.show()\\n    \\n\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def imshow(tensor, i):\n",
    "    grid = tensor[0]\n",
    "    grid.clamp_(-1, 1).add_(1).div_(2)\n",
    "    # Add 0.5 after unnormalizing to [0, 255] to round to nearest integer\n",
    "    ndarr = grid.mul_(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).to('cpu', torch.uint8).numpy()\n",
    "    img = Image.fromarray(ndarr)\n",
    "    img.save(f'{save_folder_path}sample-iter{i}.png')\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c3e20b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Settings:\n",
    "\n",
    "init size:default=8\n",
    "step:Train step 1,2 3,....\n",
    "resolution:resolution of generated pictures\n",
    "\n",
    "\"\"\"\n",
    "init_size=8\n",
    "step = int(math.log2(init_size)) - 2\n",
    "resolution = 4 * 2 ** step\n",
    "batch_default=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d735e21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Train function\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "def train(generator, discriminator, g_optim, d_optim, dataset, step, startpoint=0, used_sample=0,\n",
    "         d_losses = [], g_losses = [], alpha=0):\n",
    "    \n",
    "    loader=sample_data(dataset, batch_size.get(resolution, mini_batch_size), resolution)\n",
    "    resolution  = 4 * 2 ** step\n",
    "    \n",
    "    origin_loader = gain_sample(dataset, batch_size.get(resolution, mini_batch_size), resolution)\n",
    "    data_loader = iter(origin_loader)\n",
    "    \n",
    "    reset_lr(g_optim, learning_rate.get(resolution, 0.001))\n",
    "    reset_lr(d_optim, learning_rate.get(resolution, 0.001))\n",
    "    \n",
    "    progress_bar = tqdm(range(3000000)\n",
    "    \n",
    "    \n",
    "    requires_grad(generator, False)\n",
    "    requires_grad(discriminator, True)\n",
    "\n",
    "    disc_loss_val = 0\n",
    "    gen_loss_val = 0\n",
    "    grad_loss_val = 0\n",
    "                        \n",
    "\n",
    "    #alpha = 0\n",
    "    #used_sample = 0\n",
    "\n",
    "    max_step = int(math.log2(args.max_size)) - 2\n",
    "    final_progress = False\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Train\n",
    "                        \n",
    "    for i in progress_bar:\n",
    "            discriminator.zero_grad()\n",
    "\n",
    "            alpha = min(1, 1 / args.phase * (used_sample + 1))\n",
    "\n",
    "            if (resolution == args.init_size and args.ckpt is None) or final_progress:\n",
    "                alpha = 1\n",
    "\n",
    "            if used_sample > args.phase * 2:\n",
    "                used_sample = 0\n",
    "                step += 1\n",
    "\n",
    "                if step > max_step:\n",
    "                    step = max_step\n",
    "                    final_progress = True\n",
    "                    ckpt_step = step + 1\n",
    "\n",
    "                else:\n",
    "                    alpha = 0\n",
    "                    ckpt_step = step\n",
    "\n",
    "                resolution = 4 * 2 ** step\n",
    "\n",
    "                loader = sample_data(\n",
    "                    dataset, args.batch.get(resolution, args.batch_default), resolution\n",
    "                )\n",
    "                data_loader = iter(loader)\n",
    "\n",
    "                torch.save(\n",
    "                    {\n",
    "                        'generator': generator.module.state_dict(),\n",
    "                        'discriminator': discriminator.module.state_dict(),\n",
    "                        'g_optimizer': g_optimizer.state_dict(),\n",
    "                        'd_optimizer': d_optimizer.state_dict(),\n",
    "                        'g_running': g_running.state_dict(),\n",
    "                    },\n",
    "                    f'checkpoint/train_step-{ckpt_step}.model',\n",
    "                )\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7fe29e35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f036fad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
