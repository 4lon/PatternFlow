{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "from dotenv import load_dotenv\n",
    "from load_dataset import OASISDataset\n",
    "from vqvae import ResponsiveVQVAE2\n",
    "\n",
    "load_dotenv()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "DATASET_DATA = {\n",
    "    'oasis': {\n",
    "        'channels': 1,\n",
    "        'dimension': 64\n",
    "    }\n",
    "}\n",
    "dataset = \"oasis\"\n",
    "LATENT_DIMENSIONS = (32, 16)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_root = os.getenv(\"SLICES_PATH\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize(DATASET_DATA[dataset]['dimension']),\n",
    "        transforms.CenterCrop(DATASET_DATA[dataset]['dimension']),\n",
    "        transforms.ToTensor(),\n",
    "        # Don't need to normalize because it is done in __getitem__.\n",
    "])\n",
    "\n",
    "data = OASISDataset(data_root, transform=transform)\n",
    "test_size = len(data) // 20\n",
    "_, test_data = random_split(data, [len(data) - test_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False, num_workers=8, pin_memory=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "########################\n",
    "# Hyperparameters\n",
    "########################\n",
    "epochs = 100\n",
    "\n",
    "num_hiddens = 128\n",
    "num_residual_hiddens = 32\n",
    "num_residual_layers = 2\n",
    "\n",
    "embedding_dim = 64\n",
    "num_embeddings = 512\n",
    "commitment_cost = 0.25\n",
    "decay = 0.99\n",
    "\n",
    "learning_rate = 1e-3\n",
    "########################\n",
    "model = ResponsiveVQVAE2(DATASET_DATA[dataset]['dimension'], LATENT_DIMENSIONS, DATASET_DATA[dataset]['channels'], num_hiddens,\n",
    "                         num_residual_layers, num_residual_hiddens, num_embeddings, embedding_dim, decay)\n",
    "state_dict = torch.load(\"./results/oasis/vqvae.pth\")\n",
    "model.load_state_dict(state_dict)\n",
    "model = model.to(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.eval()\n",
    "\n",
    "originals = next(iter(test_loader))\n",
    "originals = originals.to(device)\n",
    "\n",
    "reconstructions, _ = model(originals)\n",
    "\n",
    "print(originals.shape)\n",
    "print(reconstructions.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plot the real images\n",
    "plt.figure(figsize=(30,15))\n",
    "plt.subplot(1,2,1)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Original Images\")\n",
    "plt.imshow(np.transpose(make_grid(originals, padding=5, normalize=True).cpu(),(1,2,0)))\n",
    "\n",
    "# Plot the fake images from the last epoch\n",
    "plt.subplot(1,2,2)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Reconstructed Images\")\n",
    "plt.imshow(np.transpose(make_grid(reconstructions, padding=5, normalize=True).cpu(),(1,2,0)))\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from ssim import ssim\n",
    "\n",
    "batch_ssim = []\n",
    "for _, batch in enumerate(test_loader):\n",
    "    originals = next(iter(test_loader))\n",
    "    originals = originals.to(device)\n",
    "    reconstructions, _ = model(originals)\n",
    "\n",
    "    batch_ssim.append(ssim(originals, reconstructions, val_range=1).item())\n",
    "\n",
    "print(statistics.mean(batch_ssim))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PixelCNN"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "from pixel_cnn.prior_models import TopPrior, BottomPrior\n",
    "\n",
    "BATCH_SIZE = 16"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "top_prior = TopPrior()\n",
    "bottom_prior = BottomPrior()\n",
    "\n",
    "top_prior.load_state_dict(torch.load(\"./results/oasis/pixel_cnn_top.pth\"))\n",
    "bottom_prior.load_state_dict(torch.load(\"./results/oasis/pixel_cnn_bottom.pth\"))\n",
    "\n",
    "top_prior.to(device)\n",
    "_ = bottom_prior.to(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "top_sample = torch.zeros([BATCH_SIZE, 16, 16], dtype=int).to(device)\n",
    "for row in tqdm(range(top_sample.shape[1])):\n",
    "    for col in range(top_sample.shape[2]):\n",
    "        with torch.no_grad():\n",
    "            logits = top_prior(top_sample)\n",
    "            probs = torch.softmax(logits[:, :, row, col], -1)\n",
    "            top_sample.data[:, row, col].copy_(\n",
    "                probs.multinomial(1).squeeze().data\n",
    "            )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "bottom_sample = torch.zeros([BATCH_SIZE, 32, 32], dtype=int).to(device)\n",
    "for row in tqdm(range(bottom_sample.shape[1])):\n",
    "    for col in range(bottom_sample.shape[2]):\n",
    "        with torch.no_grad():\n",
    "            logits = bottom_prior(bottom_sample, top_sample)\n",
    "            probs = torch.softmax(logits[:, :, row, col], -1)\n",
    "            bottom_sample.data[:, row, col].copy_(\n",
    "                probs.multinomial(1).squeeze().data\n",
    "            )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "images = model.decode_codebook(bottom_sample, top_sample)\n",
    "\n",
    "# Plot the real images\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Generated Images\")\n",
    "plt.imshow(np.transpose(make_grid(images, padding=5, normalize=True).cpu(),(1,2,0)))\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('comp3710': conda)"
  },
  "interpreter": {
   "hash": "bedd2988a476255cb3594ee995fd2430e7547575bac76b0de8973cda89cceddc"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}