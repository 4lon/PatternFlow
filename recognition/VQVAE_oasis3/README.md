## VQVAE Implementation for OASIS Dataset

#### VQVAE Algorithm
This project sought to implement a VQVAE-based image generator for the OASIS3 dataset. To achieve this, the algorithm used by Razavi et al in *'Generating Diverse High-Fidelity Images with VQ-VAE-2'* was adapted into a Tensorflow Keras implementation that worked on the monochrome image data. The final form of this algorithm as implemented here is shown below.

![vqvae_diagram](vqvae_diagram.png)

The filter values given in the figure above resulted in a Structured Similarity (SSIM) score of 0.867. This was achieved by training the algorithm for 400 epochs, and the step-wise results of this training are shown in the graph below. As can be seen, the SSIM and reconstruction loss actually stalled at 40 epochs, however, this hurdle was overcome and the rest of the training progressed steadily.

![vqvae_loss](vqvae_losses.png)

The results of this algorithm can speak for themselves, as they recreate the image very faithfully. Below is a sample of reconstructed test images from the VQVAE, followed by a graphical representation of its encodings.

![vqvae_recon](vqvae_reconstructions.png)

![vqvae_encoded](vqvae_encodings.png)

These encodings are generated from a set of internal codebook indices within each VectorQuantizer layer, and are therefore what is sought to be generated to create a facsimile image of a brain.


#### PixelCNN Generator
In order to generate the finished brain, the system required encoded inputs to be generated by another algorithm - the PixelCNN. This is used to generate a set of encodings from the codebook indices from a VectorQuantizer, which can then in turn be decoded by the VQVAE to create the final image. As such, two separate instances are required to generate encodings for the top and bottom level encodings within the algorithm. This is mostly boilerplate code from *https://keras.io/examples/generative/pixelcnn/*, so please refer there for greater detail on this functionality.
