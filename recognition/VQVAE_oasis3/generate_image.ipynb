{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"generate_image.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1A5jZi1mr5GTq50c9fR8x3mpQ9Kd712jy","authorship_tag":"ABX9TyO23FVG3uSouD//57BI0PkH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"f2c04564ee2947fb80d35b4584c3ed23":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_88bea928d57741ea94930d10540aae5e","IPY_MODEL_924c94eb6bea4a6ea5b03a140842bfa8","IPY_MODEL_af1b669012564ca3b9bf62c333e17882"],"layout":"IPY_MODEL_6e7f000f24304082a5b7feb144d69750"}},"88bea928d57741ea94930d10540aae5e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0b9e33d146ad40c5bea11e6282495864","placeholder":"​","style":"IPY_MODEL_2368afbaa83e43b58b56fb18d8a2cd8f","value":"Extracting Codebook Indices::   0%"}},"924c94eb6bea4a6ea5b03a140842bfa8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_2765e1515f9f45b7a2937fdc07104e54","max":967,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f7ff70c3adb74415bf165b5393ffcb75","value":0}},"af1b669012564ca3b9bf62c333e17882":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_92aa2b6a93f14252bac107d20a3f140f","placeholder":"​","style":"IPY_MODEL_3e01bc2c936e428bac272dafb3c0c0ca","value":" 0/967 [00:00&lt;?, ?it/s]"}},"6e7f000f24304082a5b7feb144d69750":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0b9e33d146ad40c5bea11e6282495864":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2368afbaa83e43b58b56fb18d8a2cd8f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2765e1515f9f45b7a2937fdc07104e54":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f7ff70c3adb74415bf165b5393ffcb75":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"92aa2b6a93f14252bac107d20a3f140f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e01bc2c936e428bac272dafb3c0c0ca":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m2ar3LCAKZcG","executionInfo":{"status":"ok","timestamp":1635674506866,"user_tz":-600,"elapsed":2456,"user":{"displayName":"Robert Eldridge","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05348367581999052348"}},"outputId":"4aed4242-ff64-4680-a0a6-e720ca007797"},"source":["# Import General Utilities\n","import numpy as np\n","import os\n","import sys\n","import random\n","from tqdm.notebook import tqdm, trange\n","import time\n","import datetime\n","\n","#Import tensorflow and its requirements\n","import tensorflow as tf\n","import tensorflow_probability as tfp\n","import tensorflow.keras.datasets as tfds\n","import tensorflow.experimental.numpy as tnp\n","from tensorflow.data.experimental import cardinality as dlen\n","from tensorflow.keras import Model, Input\n","from tensorflow.keras import layers\n","AUTOTUNE = tf.data.AUTOTUNE\n","tnp.experimental_enable_numpy_behavior()\n","\n","# Import plotting tools and image converters\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","import PIL\n","import imageio\n","from IPython import display\n","\n","# Import file path variable handling\n","from pathlib import Path\n","from urllib.request import urlopen\n","from io import BytesIO\n","from zipfile import ZipFile\n","\n","# Constants\n","batch_size = 20\n","buffer_size = 64\n","epochs = 350\n","img_height = 256\n","img_width = 256\n","num_examples_to_generate = 8\n","\n","print(tf.__version__)"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["2.7.0-dev20210803\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-rb1YuyQKz9c","executionInfo":{"status":"ok","timestamp":1635674507444,"user_tz":-600,"elapsed":180,"user":{"displayName":"Robert Eldridge","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05348367581999052348"}},"outputId":"c4178066-9702-4c61-a238-361dc4ad8219"},"source":["# Download OASIS Dataset from the provided link\n","\n","def download_and_unzip(url, extract_to='.'): # Credit to Antoine Hebert\n","    http_response = urlopen(url)\n","    zipfile = ZipFile(BytesIO(http_response.read()))\n","    zipfile.extractall(path=extract_to)\n","    http_response.close()\n","\n","root_dir = '/root/.keras/datasets'\n","data_dir = Path(root_dir) / 'keras_png_slices_data'\n","\n","try:\n","    assert(len(os.listdir(data_dir)) == 6)\n","except:\n","    if not os.path.exists(root_dir):\n","        os.makedirs(root_dir)\n","    _URL = \"https://cloudstor.aarnet.edu.au/plus/s/tByzSZzvvVh0hZA/download\"\n","    download_and_unzip(_URL, extract_to=root_dir)\n","\n","print(f\"Data Directory is {data_dir}\")\n","print(f\"Contents: \\n\\r {os.listdir(data_dir)}\")\n","\n","# Folder Directory Paths\n","train_dir = data_dir / 'keras_png_slices_train'\n","train_ans_dir = data_dir / 'keras_png_slices_seg_train'\n","test_dir = data_dir / 'keras_png_slices_test'\n","test_ans_dir = data_dir / 'keras_png_slices_seg_test'\n","val_dir = data_dir / 'keras_png_slices_validate'\n","val_ans_dir = data_dir / 'keras_png_slices_seg_validate'\n","\n","train_ds_list = tf.data.Dataset.list_files(str(train_dir/'*.*'), shuffle=True)\n","test_ds_list = tf.data.Dataset.list_files(str(test_dir/'*.*'), shuffle=True)\n","val_ds_list = tf.data.Dataset.list_files(str(val_dir/'*.*'), shuffle=False)\n","\n","train_ds = train_ds_list.take(dlen(train_ds_list))\n","test_ds = test_ds_list.take(dlen(test_ds_list))\n","val_ds = val_ds_list.take(dlen(val_ds_list))\n","\n","print(f\"Train Set is {dlen(train_ds_list)} images, Test Set is {dlen(test_ds_list)} images and Validation Set is {dlen(val_ds_list)} images.\")"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Data Directory is \\root\\.keras\\datasets\\keras_png_slices_data\n","Contents: \n","\r ['keras_png_slices_seg_test', 'keras_png_slices_seg_train', 'keras_png_slices_seg_validate', 'keras_png_slices_test', 'keras_png_slices_train', 'keras_png_slices_validate']\n","Train Set is 9664 images, Test Set is 544 images and Validation Set is 1120 images.\n"]}]},{"cell_type":"code","metadata":{"id":"Dz9rFpZdK7hW","executionInfo":{"status":"ok","timestamp":1635674507537,"user_tz":-600,"elapsed":93,"user":{"displayName":"Robert Eldridge","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05348367581999052348"}}},"source":["def decode_img(img):\n","    # Convert the compressed string to a 3D uint8 tensor\n","    img = tf.io.decode_png(img, channels=1)\n","    tfimg = tf.image.resize(img, [img_height, img_width])\n","    tfimg = (tfimg / 255.0) - 0.5 # Normalise data\n","    # Resize the image to the desired size\n","    return tfimg\n","\n","def process_path(file_path):\n","    #label = get_label(file_path)\n","    # Load the raw data from the file as a string\n","    img = tf.io.read_file(file_path)\n","    img = decode_img(img)\n","    return img\n","\n","def configure_for_performance(ds, batches):\n","    ds = ds.cache()\n","    ds = ds.shuffle(buffer_size=buffer_size)\n","    ds = ds.batch(batches)\n","    ds = ds.prefetch(buffer_size=AUTOTUNE)\n","    return ds\n","\n","train_ds_raw = train_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n","test_ds_raw = test_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n","#val_ds_raw = val_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n","\n","train_ds = configure_for_performance(train_ds_raw, batch_size)\n","test_ds = configure_for_performance(test_ds_raw, batch_size)\n","#val_ds = configure_for_performance(val_ds_raw, batch_size)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"O_q8ryFvdaow","executionInfo":{"status":"ok","timestamp":1635674997562,"user_tz":-600,"elapsed":10,"user":{"displayName":"Robert Eldridge","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05348367581999052348"}}},"source":["class VectorQuantizer(layers.Layer):\n","    def __init__(self, num_embeddings, embedding_dim, label=\"vectorizer\", beta=0.25, **kwargs):\n","        super().__init__(**kwargs)\n","        self.embedding_dim = embedding_dim\n","        self.num_embeddings = num_embeddings\n","        self.label = label\n","        self.beta = beta  # This parameter is best kept between [0.25, 2] as per the paper.\n","\n","        # Initialize the embeddings which we will quantize.\n","        w_init = tf.random_uniform_initializer()\n","        self.embeddings = tf.Variable(\n","            initial_value=w_init(\n","                shape=(self.embedding_dim, self.num_embeddings), dtype=\"float32\"\n","            ),\n","            trainable=True,\n","            name=self.label,\n","        )\n","\n","    def call(self, x):\n","        input_shape = tf.shape(x)\n","\n","        # Quantization.\n","        encoding_indices = self.get_code_indices(x)\n","        encodings = tf.one_hot(encoding_indices, self.num_embeddings)\n","        quantized = tf.matmul(encodings, self.embeddings, transpose_b=True)\n","        quantized = tf.reshape(quantized, input_shape)\n","\n","        # Calculate vector quantization loss and add that to the layer. You can learn more\n","        # about adding losses to different layers here:\n","        # https://keras.io/guides/making_new_layers_and_models_via_subclassing/. Check\n","        # the original paper to get a handle on the formulation of the loss function.\n","        commitment_loss = self.beta * tf.reduce_mean(\n","            (tf.stop_gradient(quantized) - x) ** 2\n","        )\n","        codebook_loss = tf.reduce_mean((quantized - tf.stop_gradient(x)) ** 2)\n","        self.add_loss(commitment_loss + codebook_loss)\n","\n","        # Straight-through estimator.\n","        quantized = x + tf.stop_gradient(quantized - x)\n","        return quantized\n","\n","    def get_code_indices(self, x):\n","        # Calculate L2-normalized distance between the inputs and the codes.\n","\n","        # flatten the inputs keeping `embedding_dim` intact.\n","        input_shape = tf.shape(x)\n","        flattened_inputs = tf.reshape(x, [-1, self.embedding_dim])\n","        print(f\"Vector Quantizer {self.label} has input shape {input_shape} and flattened shape {flattened_inputs.shape}\")\n","        print(f\"Num Embeddings = {self.embedding_dim}, {self.embeddings.shape}\")\n","\n","        similarity = tf.matmul(flattened_inputs, self.embeddings)\n","        distances = (\n","            tf.reduce_sum(flattened_inputs ** 2, axis=1, keepdims=True)\n","            + tf.reduce_sum(self.embeddings ** 2, axis=0)\n","            - 2 * similarity\n","        )\n","\n","        # Derive the indices for minimum distances.\n","        encoding_indices = tf.argmin(distances, axis=1)\n","        return encoding_indices\n","\n","\n","class SkipConn(layers.Layer):\n","    def __init__(self, in_channels, channels, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        self.conv1 = layers.Conv2D(channels, 3, activation=\"relu\", padding=\"same\")\n","        self.conv2 = layers.Conv2D(in_channels, 1, activation=\"relu\", padding=\"same\")\n","\n","    def call(self, input):\n","        x = self.conv1(input)\n","        x = self.conv2(x)\n","        out = x + input\n","        return out\n","\n","\n","def get_encoder(input_shape, channels=128, strides=4, name='encoder'):\n","    encoder_inputs = Input(shape=input_shape, name=name+'input')\n","\n","    if strides == 4:\n","        x = layers.Conv2D(channels // 2, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n","        x = layers.Conv2D(channels, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n","\n","    elif strides == 2:\n","        x = layers.Conv2D(channels // 2, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n","        \n","    x = layers.Conv2D(channels, 3, activation=\"relu\", padding=\"same\")(x)\n","\n","    for i in range(2):\n","        x = SkipConn(channels, 32)(x)\n","        \n","    return Model(inputs=encoder_inputs, outputs=x, name=name)\n","\n","\n","def get_decoder(input_shape, out_channels=128, channels=128, strides=4, name='decoder'):\n","    latent_inputs = Input(shape=input_shape, name=name+'decoder_input')\n","\n","    x = layers.Conv2D(channels, 3, activation=\"relu\", padding=\"same\")(latent_inputs)\n","\n","    for i in range(2):\n","        x = SkipConn(channels, 32)(x)\n","\n","    if strides == 4:\n","        x = layers.Conv2DTranspose(channels // 2, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n","\n","    decoder_outputs = layers.Conv2DTranspose(out_channels, 3, strides=2, padding=\"same\")(x)\n","    return Model(inputs=latent_inputs, outputs=decoder_outputs, name=name)\n","\n","\n","class VQVAE(Model):\n","    def __init__(self, latent_dim=16, num_embeddings=128, embed_dim=192, n_embed=256):\n","        super(VQVAE, self).__init__()\n","\n","        #self.encoder = get_encoder(input_shape=(256, 256, 1), channels=num_embeddings, strides=4)\n","        self.enc_b = get_encoder(input_shape=(256, 256, 1), channels=latent_dim, strides=4, name=\"encoder_b\")\n","        self.enc_t = get_encoder(input_shape=(64, 64, latent_dim), channels=latent_dim, strides=2, name=\"encoder_t\")\n","        #self.quantize_conv_t = layers.Conv2D(embed_dim, 1, activation=\"relu\", padding=\"same\")\n","        self.quantize_t = VectorQuantizer(num_embeddings, latent_dim, label=\"t_vq\")\n","\n","        self.dec_t = get_decoder(input_shape=(32, 32, latent_dim), out_channels=latent_dim, channels=latent_dim, strides=2, name=\"decoder_t\")\n","        self.quantize_b = VectorQuantizer(embed_dim, n_embed, label=\"b_vq\")\n","        self.upsample_t = layers.Conv2DTranspose(embed_dim, 3, activation=\"relu\", strides=2, padding=\"same\")\n","\n","        self.dec = get_decoder(input_shape=(64, 64, 2 * latent_dim + embed_dim), out_channels=1, channels=latent_dim, strides=4, name=\"decoder_b\")\n","\n","    def encode(self, input):\n","        enc_b = self.enc_b(input)\n","        enc_t = self.enc_t(enc_b)\n","\n","        quant_t = self.quantize_t(enc_t)\n","\n","        dec_t = self.dec_t(quant_t)\n","        enc_b = tf.concat([dec_t, enc_b], 3)\n","\n","        quant_b = self.quantize_b(enc_b)\n","        #upsample_t = self.upsample_t(quant_t)\n","        #quant = tf.concat([upsample_t, quant_b], 3)\n","        \n","        return quant_t, quant_b\n","\n","    def decode(self, quant_t, quant_b):\n","        upsample_t = self.upsample_t(quant_t)\n","        quant = tf.concat([upsample_t, quant_b], 3)\n","        dec = self.dec(quant)\n","\n","        return dec\n","\n","    def encode_to_t(self, input):\n","        enc_b = self.enc_b(input)\n","        enc_t = self.enc_t(enc_b)\n","        return enc_t\n","\n","    def encode_to_b(self, input):\n","        enc_b = self.enc_b(input)\n","        enc_t = self.enc_t(enc_b)\n","\n","        quant_t = self.quantize_t(enc_t)\n","\n","        dec_t = self.dec_t(quant_t)\n","        enc_b = tf.concat([dec_t, enc_b], 3)\n","        return enc_b\n","\n","    def call(self, input):\n","        quant_t, quant_b = self.encode(input)\n","        dec = self.decode(quant_t, quant_b)\n","\n","        return dec\n"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A4xjcDYw6EyH","executionInfo":{"status":"ok","timestamp":1635674567683,"user_tz":-600,"elapsed":362,"user":{"displayName":"Robert Eldridge","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05348367581999052348"}},"outputId":"e000da2f-0b2c-4a54-85a1-63a92d784443"},"source":["def make_vqvae(latent_dim=16, num_embeddings=128, embed_dim=64, n_embed=256):\n","\n","    enc_b = get_encoder(input_shape=(256, 256, 1), channels=num_embeddings, strides=4, name=\"encoder_b\")\n","    enc_t = get_encoder(input_shape=(64, 64, num_embeddings), channels=num_embeddings, strides=2, name=\"encoder_t\")\n","    quantize_t = VectorQuantizer(num_embeddings, latent_dim, label=\"t_vq\")\n","\n","    dec_t = get_decoder(input_shape=(32, 32, num_embeddings), out_channels=num_embeddings, channels=num_embeddings, strides=2, name=\"decoder_t\")\n","    quantize_b = VectorQuantizer(n_embed, embed_dim, label=\"b_vq\")\n","    upsample_t = layers.Conv2DTranspose(embed_dim, 3, activation=\"relu\", strides=2, padding=\"same\")\n","\n","    dec = get_decoder(input_shape=(64, 64, 2 * num_embeddings + embed_dim), out_channels=1, channels=num_embeddings, strides=4, name=\"decoder_b\")\n","\n","    inputs = Input(shape=(256, 256, 1))\n","    encoded_b = enc_b(inputs)\n","    encoded_t = enc_t(encoded_b)\n","\n","    quant_t = quantize_t(encoded_t)\n","    decoded_t = dec_t(quant_t)\n","    encoded_b = tf.concat([decoded_t, encoded_b], 3)\n","\n","    quant_b = quantize_b(encoded_b)\n","\n","    upsampled_t = upsample_t(quant_t)\n","    quant = tf.concat([upsampled_t, quant_b], 3)\n","    decoded = dec(quant)\n","\n","    return Model(inputs, decoded, name=\"layer2_vqvae\")\n","    \n","make_vqvae(latent_dim=8, num_embeddings=96).summary()"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"layer2_vqvae\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_2 (InputLayer)            [(None, 256, 256, 1  0           []                               \n","                                )]                                                                \n","__________________________________________________________________________________________________\n","encoder_b (Functional)          (None, 64, 64, 96)   186784      ['input_2[0][0]']                \n","__________________________________________________________________________________________________\n","encoder_t (Functional)          (None, 32, 32, 96)   144784      ['encoder_b[0][0]']              \n","__________________________________________________________________________________________________\n","vector_quantizer_2 (VectorQuan  (None, 32, 32, 96)   768         ['encoder_t[0][0]']              \n","tizer)                                                                                            \n","__________________________________________________________________________________________________\n","decoder_t (Functional)          (None, 64, 64, 96)   227776      ['vector_quantizer_2[0][0]']     \n","__________________________________________________________________________________________________\n","tf.concat_2 (TFOpLambda)        (None, 64, 64, 192)  0           ['decoder_t[0][0]',              \n","                                                                  'encoder_b[0][0]']              \n","__________________________________________________________________________________________________\n","conv2d_transpose_5 (Conv2DTran  (None, 64, 64, 64)   55360       ['vector_quantizer_2[0][0]']     \n","spose)                                                                                            \n","__________________________________________________________________________________________________\n","vector_quantizer_3 (VectorQuan  (None, 64, 64, 192)  16384       ['tf.concat_2[0][0]']            \n","tizer)                                                                                            \n","__________________________________________________________________________________________________\n","tf.concat_3 (TFOpLambda)        (None, 64, 64, 256)  0           ['conv2d_transpose_5[0][0]',     \n","                                                                  'vector_quantizer_3[0][0]']     \n","__________________________________________________________________________________________________\n","decoder_b (Functional)          (None, 256, 256, 1)  324929      ['tf.concat_3[0][0]']            \n","==================================================================================================\n","Total params: 956,785\n","Trainable params: 956,785\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":469},"id":"n1W_7c5ALt0_","executionInfo":{"status":"error","timestamp":1635675002623,"user_tz":-600,"elapsed":197,"user":{"displayName":"Robert Eldridge","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05348367581999052348"}},"outputId":"480247ef-461e-4792-9a03-8274b883dd74"},"source":["trained_vqvae_model = VQVAE(latent_dim=128, num_embeddings=256, embed_dim=128)\n","trained_vqvae_model.compile(optimizer=tf.keras.optimizers.Adam(1e-4))\n","\n","#trained_vqvae_model.load_weights(f'/content/drive/MyDrive/COMP3710/PatternFlow/recognition/VQVAE_oasis3/trained_models/96filter_VQVAE')\n","trained_vqvae_model.load_weights(f'/root/trained_model/model020')\n","#trained_vqvae_model = tf.keras.models.load_model(f'/content/drive/MyDrive/COMP3710/PatternFlow/recognition/VQVAE_oasis3/trained_models/96filter_VQVAE')\n","#trained_vqvae_model.compile(optimizer=tf.keras.optimizers.Adam(1e-4))\n","trained_vqvae_model.build((None, 256, 256, 1))\n","trained_vqvae_model.summary()"],"execution_count":16,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20496/2044088995.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#trained_vqvae_model.load_weights(f'/content/drive/MyDrive/COMP3710/PatternFlow/recognition/VQVAE_oasis3/trained_models/96filter_VQVAE')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtrained_vqvae_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'/root/trained_model/model020'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;31m#trained_vqvae_model = tf.keras.models.load_model(f'/content/drive/MyDrive/COMP3710/PatternFlow/recognition/VQVAE_oasis3/trained_models/96filter_VQVAE')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#trained_vqvae_model.compile(optimizer=tf.keras.optimizers.Adam(1e-4))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\users\\rweld\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36massert_is_compatible_with\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m   1169\u001b[0m     \"\"\"\n\u001b[0;32m   1170\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1171\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Shapes %s and %s are incompatible\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1173\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mmost_specific_compatible_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mValueError\u001b[0m: Shapes (256, 128) and (128, 256) are incompatible"]}]},{"cell_type":"code","metadata":{"id":"FIh2M7dEKshJ","executionInfo":{"status":"aborted","timestamp":1635674508565,"user_tz":-600,"elapsed":4234,"user":{"displayName":"Robert Eldridge","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05348367581999052348"}}},"source":["def show_subplot(original, reconstructed):\n","\n","    original = original + 0.5\n","    reconstructed = reconstructed + 0.5\n","\n","    plt.figure(figsize=(10, 10))\n","    plt.subplot(1, 2, 1)\n","    plt.imshow(original[:, :, 0])\n","    plt.title(\"Original\")\n","    plt.axis(\"off\")\n","\n","    plt.subplot(1, 2, 2)\n","    plt.imshow(reconstructed[:, :, 0])\n","    plt.title(\"Reconstructed\")\n","    plt.axis(\"off\")\n","\n","    plt.show()\n","\n","def show_generated_image(samples_t, samples_b):\n","\n","    generated = trained_vqvae_model.decode(samples_t, samples_b)\n","\n","    for i in range(samples_t.shape[0]):\n","        plt.figure(figsize=(12, 4))\n","        plt.subplot(1, 3, 1)\n","        plt.imshow(samples_t[i][:, :, 0] + 0.5)\n","        plt.title(\"Low Detail Encoding\")\n","        plt.axis(\"off\")\n","\n","        plt.subplot(1, 3, 2)\n","        plt.imshow(samples_b[i][:, :, 0] + 0.5)\n","        plt.title(\"High Detail Encoding\")\n","        plt.axis(\"off\")\n","\n","        plt.subplot(1, 3, 3)\n","        plt.imshow(generated[i][:, :, 0] + 0.5)\n","        plt.title(\"Original\")\n","        plt.axis(\"off\")\n","\n","        plt.show()\n","\n","\n","test_images = next(iter(test_ds))\n","reconstructions_test = trained_vqvae_model.predict(test_images)\n","\n","for i in range(5):\n","    show_subplot(test_images[i], reconstructions_test[i])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qarp5Z_oNNso","executionInfo":{"status":"aborted","timestamp":1635674508567,"user_tz":-600,"elapsed":4229,"user":{"displayName":"Robert Eldridge","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05348367581999052348"}}},"source":["# Testing encoding visualisation\n","enc_out_t, enc_out_b = trained_vqvae_model.encode(test_images)\n","print(f\"Low filter encoder is {enc_out_t.shape}, High filter encoder is {enc_out_b.shape}\")\n","\n","for i in range(4):\n","    plt.figure(figsize=(12, 4))\n","    plt.subplot(1, 3, 1)\n","    plt.imshow(test_images[i][:, :, 0] + 0.5)\n","    plt.title(\"Original\")\n","    plt.axis(\"off\")\n","\n","    plt.subplot(1, 3, 2)\n","    plt.imshow(enc_out_t[i][:, :, 0] + 0.5)\n","    plt.title(\"Low Detail Encoding\")\n","    plt.axis(\"off\")\n","\n","    plt.subplot(1, 3, 3)\n","    plt.imshow(enc_out_b[i][:, :, 0] + 0.5)\n","    plt.title(\"High Detail Encoding\")\n","    plt.axis(\"off\")\n","    plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wkbmaFyikILz","executionInfo":{"status":"aborted","timestamp":1635674508569,"user_tz":-600,"elapsed":4225,"user":{"displayName":"Robert Eldridge","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05348367581999052348"}}},"source":["ssim_total = 0\n","count = 0\n","\n","for batch in tqdm(test_ds):\n","    reconstructed_comp = trained_vqvae_model.predict(batch)\n","\n","    for i in range(batch.shape[0]):\n","        original = tf.math.round((batch[i] + 0.5) * 255.0)\n","        reconstruction = tf.math.round((reconstructed_comp[i] + 0.5) * 255.0)\n","        ssim = tf.image.ssim(original, reconstruction, max_val=255.0)\n","        ssim_total += ssim\n","        count += 1\n","\n","print(f\"Average SSIM score is {ssim_total / count}.\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":450,"referenced_widgets":["f2c04564ee2947fb80d35b4584c3ed23","88bea928d57741ea94930d10540aae5e","924c94eb6bea4a6ea5b03a140842bfa8","af1b669012564ca3b9bf62c333e17882","6e7f000f24304082a5b7feb144d69750","0b9e33d146ad40c5bea11e6282495864","2368afbaa83e43b58b56fb18d8a2cd8f","2765e1515f9f45b7a2937fdc07104e54","f7ff70c3adb74415bf165b5393ffcb75","92aa2b6a93f14252bac107d20a3f140f","3e01bc2c936e428bac272dafb3c0c0ca"]},"id":"CFO6bXXrsOGp","executionInfo":{"status":"error","timestamp":1635674704088,"user_tz":-600,"elapsed":120,"user":{"displayName":"Robert Eldridge","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05348367581999052348"}},"outputId":"2a361f6a-3449-43bb-f145-a09c768956c4"},"source":["# Generate an encoded dataset\n","\n","def codebooks_t(ds):\n","    encoded_outputs = trained_vqvae_model.encode_to_t(ds)\n","    #flat_enc_outputs = tf.reshape(encoded_outputs, [-1, encoded_outputs.shape[-1]])\n","    codebook_indices = trained_vqvae_model.quantize_t.get_code_indices(encoded_outputs)#flat_enc_outputs)\n","    codebook_indices = codebook_indices.numpy().reshape(encoded_outputs.shape[:-1])\n","\n","    return codebook_indices\n","\n","def codebooks_b(ds):\n","    encoded_outputs = trained_vqvae_model.encode_to_b(ds)\n","    #flat_enc_outputs = tf.reshape(encoded_outputs, [-1, encoded_outputs.shape[-1]])\n","    codebook_indices = trained_vqvae_model.quantize_b.get_code_indices(encoded_outputs)#flat_enc_outputs)\n","    print(codebook_indices.shape)\n","    codebook_indices = codebook_indices.numpy().reshape(encoded_outputs.shape[:-1])\n","\n","    return codebook_indices\n","\n","pixel_ds = configure_for_performance(train_ds_raw, 10)\n","\n","codebook_tensor_t = 0\n","codebook_tensor_b = 0\n","first = True\n","\n","for batch in tqdm(pixel_ds, desc=f\"Extracting Codebook Indices:\"):\n","    if first:\n","        codebook_tensor_t = codebooks_t(batch)\n","        codebook_tensor_b = codebooks_b(batch)\n","        first = False\n","    else:\n","        codebook_tensor_t = tf.concat([codebook_tensor_t, codebooks_t(batch)], 0)\n","        codebook_tensor_b = tf.concat([codebook_tensor_b, codebooks_b(batch)], 0)\n","        \n","print(codebook_tensor_b.shape)"],"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":["Extracting Codebook Indices::   0%|          | 0/967 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2c04564ee2947fb80d35b4584c3ed23"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Vector Quantizer t_vq has input shape [ 10  32  32 128] and flattened shape (10240, 128)\n","Vector Quantizer t_vq has input shape [ 10  32  32 128] and flattened shape (10240, 128)\n","Vector Quantizer b_vq has input shape [ 10  64  64 256] and flattened shape (81920, 128)\n","(81920,)\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20496/985133346.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfirst\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mcodebook_tensor_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcodebooks_t\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mcodebook_tensor_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcodebooks_b\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[0mfirst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20496/985133346.py\u001b[0m in \u001b[0;36mcodebooks_b\u001b[1;34m(ds)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mcodebook_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrained_vqvae_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquantize_b\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_code_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded_outputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#flat_enc_outputs)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodebook_indices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mcodebook_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcodebook_indices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded_outputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcodebook_indices\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 81920 into shape (10,64,64)"]}]},{"cell_type":"code","metadata":{"id":"mCKC58DmxRy1","executionInfo":{"status":"aborted","timestamp":1635674508574,"user_tz":-600,"elapsed":4220,"user":{"displayName":"Robert Eldridge","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05348367581999052348"}}},"source":["# PixelCNN implementation\n","# The first layer is the PixelCNN layer. This layer simply\n","# builds on the 2D convolutional layer, but includes masking.\n","class PixelConvLayer(layers.Layer):\n","    def __init__(self, mask_type, **kwargs):\n","        super(PixelConvLayer, self).__init__()\n","        self.mask_type = mask_type\n","        self.conv = layers.Conv2D(**kwargs)\n","\n","    def build(self, input_shape):\n","        # Build the conv2d layer to initialize kernel variables\n","        self.conv.build(input_shape)\n","        # Use the initialized kernel to create the mask\n","        kernel_shape = self.conv.kernel.get_shape()\n","        self.mask = np.zeros(shape=kernel_shape)\n","        self.mask[: kernel_shape[0] // 2, ...] = 1.0\n","        self.mask[kernel_shape[0] // 2, : kernel_shape[1] // 2, ...] = 1.0\n","        if self.mask_type == \"B\":\n","            self.mask[kernel_shape[0] // 2, kernel_shape[1] // 2, ...] = 1.0\n","        self.mask = tf.cast(self.mask, tf.float32)\n","\n","    def call(self, inputs):\n","        self.conv.kernel.assign(self.conv.kernel * self.mask)\n","        return tf.cast(self.conv(inputs), tf.float32)\n","\n","\n","# Next, we build our residual block layer.\n","# This is just a normal residual block, but based on the PixelConvLayer.\n","class ResidualBlock(layers.Layer):\n","    def __init__(self, filters, **kwargs):\n","        super(ResidualBlock, self).__init__(**kwargs)\n","        self.conv1 = layers.Conv2D(\n","            filters=filters, kernel_size=1, activation=\"relu\"\n","        )\n","        self.pixel_conv = PixelConvLayer(\n","            mask_type=\"B\",\n","            filters=filters // 2,\n","            kernel_size=3,\n","            activation=\"relu\",\n","            padding=\"same\",\n","        )\n","        self.conv2 = layers.Conv2D(\n","            filters=filters, kernel_size=1, activation=\"relu\"\n","        )\n","\n","    def call(self, inputs):\n","        x = self.conv1(inputs)\n","        x = self.pixel_conv(x)\n","        x = self.conv2(x)\n","        return layers.add([inputs, x])\n","\n","\n","def pixel_cnn(shape, filters=128, kernels=64, embeddings=128, name=\"pixel_cnn\"):\n","    pixelcnn_inputs = Input(shape=shape, dtype=tf.int32)\n","    ohe = tf.one_hot(pixelcnn_inputs, embeddings)\n","    ohe = tf.cast(ohe, tf.float32)\n","    x = PixelConvLayer(\n","        mask_type=\"A\", filters=filters, kernel_size=kernels, activation=\"relu\", padding=\"same\"\n","    )(ohe)\n","\n","    for _ in range(2):\n","        x = ResidualBlock(filters=filters)(x)\n","\n","    for _ in range(2):\n","        x = PixelConvLayer(\n","            mask_type=\"B\",\n","            filters=filters,\n","            kernel_size=1,\n","            strides=1,\n","            activation=\"relu\",\n","            padding=\"valid\",\n","        )(x)\n","\n","    out = layers.Conv2D(\n","        filters=embeddings, kernel_size=1, strides=1, padding=\"valid\"\n","    )(x)\n","\n","    return Model(pixelcnn_inputs, out, name=name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LBYEWEY42ekW","executionInfo":{"status":"aborted","timestamp":1635674508576,"user_tz":-600,"elapsed":4217,"user":{"displayName":"Robert Eldridge","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05348367581999052348"}}},"source":["pixel_cnn_t = pixel_cnn((32, 32), filters=128, kernels=64, embeddings=128, name=\"pixel_cnn_top\")\n","pixel_cnn_t.load_weights(f\"/root/trained_pixel_model/model019_t\")\n","pixel_cnn_b = pixel_cnn((64, 64), filters=128, kernels=64, embeddings=256, name=\"pixel_cnn_bottom\")\n","pixel_cnn_b.load_weights(f\"/root/trained_pixel_model/model019_b\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aczZH2eM3T_H","executionInfo":{"status":"aborted","timestamp":1635674508579,"user_tz":-600,"elapsed":4214,"user":{"displayName":"Robert Eldridge","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05348367581999052348"}}},"source":["# Create a mini sampler model\n","def gen_priors_layer(model, num_samples=10):\n","    inputs = Input(shape=model.input_shape[1:])\n","    x = model(inputs, training=False)\n","    dist = tfp.distributions.Categorical(logits=x)\n","    sampled = dist.sample()\n","    sampler = Model(inputs, sampled)\n","\n","    # Create an empty array of priors.\n","    priors = np.zeros(shape=(num_samples,) + (model.input_shape)[1:])\n","    num_samples, rows, cols = priors.shape\n","\n","    # Iterate over the priors because generation has to be done sequentially pixel by pixel.\n","    for row in trange(rows, desc=\"Predicting priors\"):\n","        for col in range(cols):\n","            # Feed the whole array and retrieving the pixel value probabilities for the next\n","            # pixel.\n","            probs = sampler.predict(priors)\n","            # Use the probabilities to pick pixel values and append the values to the priors.\n","            priors[:, row, col] = probs[:, row, col]\n","    return priors\n","\n","priors_t = gen_priors_layer(pixel_cnn_t)\n","priors_b = gen_priors_layer(pixel_cnn_b)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LpkDlQx4WxO8","executionInfo":{"status":"aborted","timestamp":1635674508581,"user_tz":-600,"elapsed":4210,"user":{"displayName":"Robert Eldridge","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05348367581999052348"}}},"source":["def gen_encodings(priors, embeddings, pretrained_embeddings, shape):\n","    # Perform an embedding lookup.\n","    priors_ohe = tf.one_hot(priors.astype(\"int32\"), embeddings).numpy()\n","    print(priors_ohe.shape)\n","    quantized = tf.matmul(priors_ohe.astype(\"float32\"), pretrained_embeddings)\n","    print(quantized.shape)\n","    #quantized = tf.reshape(quantized, (-1, *shape))\n","    return priors_ohe#quantized\n","\n","pad_t = gen_encodings(priors_t, 96, trained_vqvae_model.quantize_t.embeddings, (32, 32, 96))\n","pad_b = gen_encodings(priors_b, 192, trained_vqvae_model.quantize_b.embeddings, (64, 64, 192))\n","\n","show_generated_image(pad_t, pad_b)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KSM4HvVEcIJG","executionInfo":{"status":"aborted","timestamp":1635674508582,"user_tz":-600,"elapsed":4204,"user":{"displayName":"Robert Eldridge","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05348367581999052348"}}},"source":["pixel_cnn_t = pixel_cnn((32, 32), filters=128, kernels=64, embeddings=128, name=\"pixel_cnn_top\")\n","\n","pixel_cnn_t.compile(\n","    optimizer=tf.keras.optimizers.Adam(3e-4),\n","    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","    metrics=[\"accuracy\"],\n",")\n","\n","pixel_cnn_t.fit(\n","    x=codebook_tensor_t,\n","    y=codebook_tensor_t,\n","    batch_size=128,\n","    epochs=30,\n","    validation_split=0.1,\n",")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fMBhlIicGLLY","executionInfo":{"status":"aborted","timestamp":1635674508584,"user_tz":-600,"elapsed":4200,"user":{"displayName":"Robert Eldridge","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05348367581999052348"}}},"source":["os.makedirs(f\"/root/trained_pixel_model/model019_t\")\n","pixel_cnn_t.save(f\"/root/trained_pixel_model/model019_t\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"odcjKuzMFDV7","executionInfo":{"status":"aborted","timestamp":1635674508586,"user_tz":-600,"elapsed":4197,"user":{"displayName":"Robert Eldridge","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05348367581999052348"}}},"source":["pixel_cnn_b = pixel_cnn((64, 64), filters=128, kernels=64, embeddings=256, name=\"pixel_cnn_bottom\")\n","\n","pixel_cnn_b.compile(\n","    optimizer=tf.keras.optimizers.Adam(3e-4),\n","    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","    metrics=[\"accuracy\"],\n",")\n","\n","pixel_cnn_b.fit(\n","    x=codebook_tensor_b,\n","    y=codebook_tensor_b,\n","    batch_size=20,\n","    epochs=2,\n","    validation_split=0.1,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r58hYVuzGaBn","executionInfo":{"status":"aborted","timestamp":1635674508588,"user_tz":-600,"elapsed":4193,"user":{"displayName":"Robert Eldridge","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05348367581999052348"}}},"source":["#os.makedirs(f\"/root/trained_pixel_model/model019_b\")\n","pixel_cnn_b.save(f\"/root/trained_pixel_model/model019_b\")"],"execution_count":null,"outputs":[]}]}