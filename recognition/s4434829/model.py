"""
Model
Should i make a class or functional
How should it train? using the train function it has or a train loop like i 
wrote for demos


To do:
VAE first (from lecture)
encode and decoder separately as per lecture

modify to vq part

actually seems theyre not related
but there is an encode and decode so do one first then other?

SOURCES/Notes:
VQVAE:
vaes
https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73

from the paper:
grad copy form enc to decoder
encoder: 
        2 conv layrs
            stride 2
            kernel 4x4
        2 residual blocks
            relu
            conv 3x3
            reulu
            conv 1x1
        (all 256 hidden)
decoder:
        2 residual
        2 transposed conv
adam optimiser
lr 2e^-4
batch size 128


Where in the code I have marked that I wasn't sure whether to activate or not
I have referenced this code:
https://github.com/deepmind/sonnet/blob/v1/sonnet/examples/vqvae_example.ipynb
The model seems to be a bit different, it has more than 2 conv layers ect.
Also it has implemented it by using classes and build and then a functional way,
which i didn't do.

Video explaining the paper on VQVAE:
https://www.youtube.com/watch?v=VZFVUrYcig0


sources on VQ:
https://en.wikipedia.org/wiki/Vector_quantization
I used this, the paper doesn't really talk about VQ in much detail and all
other sources just cite the paper. 
https://github.com/deepmind/sonnet/blob/v1/sonnet/python/modules/nets/vqvae.py

TODO:
make parameters pass-in-able

maybe need to one hot encode so have 4 channels?
"""

import tensorflow as tf
# TODO switch for readablity
from tensorflow.keras.layers import *

class ResidualBlock(tf.keras.Model):
    """
        One residual bloc consisting of a relu, a 3x3 convolution, 
        another relu and then a 1x1 convolution

        Why did I make this it's own class:
            I think these actually need to be their own classes because we need 
            2 of everything. So if I was to put them all in one class like i had
            planned I'd need: residual1_ con1_, residual1_conv_2, 
                                residual2_conv1, residual2_conv2
            and then again for the decoder.
            so either make these own class or use structural way
            If we were to do it structurally we'd need to repeat the same code 
                4 times as well unless it was generated by a function
            I think the classes with the call function are overkill and
            this architecture might be dummer but it's easier to read imo.
        
        https://towardsdatascience.com/residual-blocks-building-blocks-of-resnet-fd90ca15d6ec

        Shapes (from adding prints and running): (batch, H, W, channels) (filters 256)
        Res: shape pre running (10, 256, 256, 1)
        Res: shape post relu (10, 256, 256, 1)
        Res: shape post conv1 (10, 256, 256, 256)
        Res: shape post con2 (10, 256, 256, 256)
        Res: shape post add (10, 256, 256, 256)
    """
    def __init__(self, inputs=None):
        super(ResidualBlock, self).__init__(inputs)
        self.relu = tf.keras.layers.ReLU()

        self.conv1 = tf.keras.layers.Conv2D(filters=256, kernel_size=(3,3), 
                            strides=(1,1), padding='same',
                            activation='relu', kernel_initializer='he_uniform')
        # is the activation enough or do we need another relu and activation none
        self.conv2 = tf.keras.layers.Conv2D(filters=256, kernel_size=(1,1), 
                            strides=(1,1), padding='same',
                            activation=None, kernel_initializer='he_uniform')

    def call(self, X):
        new_x = self.relu(X)
        new_x = self.conv1(new_x)
        new_x = self.conv2(new_x)
        new_x += X
        return new_x

class Encoder(tf.keras.Model):
    """
    The encoder consists of:
        2x conv layers with stride 2 and kernel 4x4
        2x residual blocks

    Shapes:
    Enc: shape pre running (10, 256, 256, 1)
    Enc: shape post conv 1 (10, 127, 127, 256)
    Enc: shape post conv 2 (10, 62, 62, 256)
    Enc: shape post resdi 1 (10, 62, 62, 256)
    Enc: shape post resid 2 (10, 62, 62, 256)
    """
    def __init__(self, inputs=None):
        super(Encoder, self).__init__(inputs)
        # Q: no activation? probably yes but not explicitly mentioned in paper
        self.conv1 = tf.keras.layers.Conv2D(filters=256, kernel_size=(4,4), strides=(2,2), 
                            activation='relu', kernel_initializer='he_uniform')
        # activation in resdiaul?
        self.conv2 = tf.keras.layers.Conv2D(filters=256, kernel_size=(4,4), strides=(2,2), 
                            activation=None, kernel_initializer='he_uniform')

        self.resid1 = ResidualBlock()

        self.resid2 = ResidualBlock()

    def call(self, X):
        X = self.conv1(X)
        X = self.conv2(X)
        X = self.resid1(X)
        X = self.resid2(X)
        return X

class Decoder(tf.keras.Model):
    def __init__(self, inputs=None):
        super(Decoder, self).__init__(inputs)

        self.resid1 = ResidualBlock()
        self.resid2 = ResidualBlock()

        self.conv_t_1 = tf.keras.layers.Conv2DTranspose(filters=256, 
                            kernel_size=(4,4), strides=(2,2), activation='relu')

        self.conv_t_2 = tf.keras.layers.Conv2DTranspose(filters=256, 
                            kernel_size=(4,4), strides=(2,2), activation=None)

    def call(self, X):
        X = self.resid1(X)
        X = self.resid2(X)
        X = self.conv_t_1(X)
        X = self.conv_t_2(X)
        return X

class VQ(tf.keras.Model):
    """
    Used to learn the embedding spcae
    Might get own loss?

    Need to go from batch, height, width, channel to batch*height*width, channels

    Input and output dims depend on dataset

    Shapes:
        ---Shape of data as it comes into VQ: (10, 62, 62, 256)
        ---Shape of data after reshape: (38440, 256)

    source: https://github.com/deepmind/sonnet/blob/v1/sonnet/python/modules/nets/vqvae.py
    TODO: write what you got from here/how used it
    """
    def __init__(self, indim, outdim, inputs=None):
        super(VQ, self).__init__(inputs)
        # instead of getting as a varaible
        self.emb = tf.keras.layers.Embedding(indim, outdim)

        # from paper, need to make sure encoder commits to an embedding so output
        # does not grow. 
        self.commitment_loss = 0.1 # TODO: i picked this randomly

    def call(self, X):
        print("---Shape of data as it comes into VQ: {}".format(X.shape))
        X_shape = X.shape
        # Change shape to batch*height*width, channels
        X_reshaped = tf.reshape(X, (X_shape[0]*X_shape[1]*X_shape[2], X_shape[3]))
        print("---Shape of data after reshape: {}".format(X_reshaped.shape))

class VQVAE(tf.keras.Model):
    """
    Information about VQVAE from:
        The paper:  https://arxiv.org/pdf/1711.00937.pdf
    """
    def __init__(self, inputs=None):
        super(VQVAE, self).__init__(inputs)
        
        # encoder compoentnts
        # do i need to expose this so i can get the latent space?
        self.encoder = Encoder()

        # decoder components
        self.decoder = Decoder()

        # VQ
        self.vq = VQ(64, 512)

    def call(self, X):
        X = self.encoder(X)
        self.vq(X) # not correct but just to test
        X = self.decoder(X)
