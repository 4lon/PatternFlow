"""
Model
Should i make a class or functional
How should it train? using the train function it has or a train loop like i 
wrote for demos


To do:
VAE first (from lecture)
encode and decoder separately as per lecture

modify to vq part

actually seems theyre not related
but there is an encode and decode so do one first then other?

SOURCES/Notes:
VQVAE:
vaes
https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73

from the paper:
grad copy form enc to decoder
encoder: 
        2 conv layrs
            stride 2
            kernel 4x4
        2 residual blocks
            relu
            conv 3x3
            reulu
            conv 1x1
        (all 256 hidden)
decoder:
        2 residual
        2 transposed conv
adam optimiser
lr 2e^-4
batch size 128


Where in the code I have marked that I wasn't sure whether to activate or not
I have referenced this code:
https://github.com/deepmind/sonnet/blob/v1/sonnet/examples/vqvae_example.ipynb
The model seems to be a bit different, it has more than 2 conv layers ect.
Also it has implemented it by using classes and build and then a functional way,
which i didn't do.

Video explaining the paper on VQVAE: (this really helped me undesrtand what
ahppened between encoder and decoder)
https://www.youtube.com/watch?v=VZFVUrYcig0


sources on VQ:
https://en.wikipedia.org/wiki/Vector_quantization
I used this, the paper doesn't really talk about VQ in much detail and all
other sources just cite the paper. 
https://github.com/deepmind/sonnet/blob/v1/sonnet/python/modules/nets/vqvae.py

TODO:
make parameters pass-in-able

maybe need to one hot encode so have 4 channels?

clean up comments, maybe over explaining
"""

import tensorflow as tf
# TODO switch for readablity
from tensorflow.keras.layers import *

class ResidualBlock(tf.keras.Model):
    """
        One residual bloc consisting of a relu, a 3x3 convolution, 
        another relu and then a 1x1 convolution

        Why did I make this it's own class:
            I think these actually need to be their own classes because we need 
            2 of everything. So if I was to put them all in one class like i had
            planned I'd need: residual1_ con1_, residual1_conv_2, 
                                residual2_conv1, residual2_conv2
            and then again for the decoder.
            so either make these own class or use structural way
            If we were to do it structurally we'd need to repeat the same code 
                4 times as well unless it was generated by a function
            I think the classes with the call function are overkill and
            this architecture might be dummer but it's easier to read imo.
        
        https://towardsdatascience.com/residual-blocks-building-blocks-of-resnet-fd90ca15d6ec

        Shapes (from adding prints and running): (batch, H, W, channels) (filters 256)
        Res: shape pre running (10, 256, 256, 1)
        Res: shape post relu (10, 256, 256, 1)
        Res: shape post conv1 (10, 256, 256, 256)
        Res: shape post con2 (10, 256, 256, 256)
        Res: shape post add (10, 256, 256, 256)
    """
    def __init__(self, inputs=None):
        super(ResidualBlock, self).__init__(inputs)
        self.relu = tf.keras.layers.ReLU()

        self.conv1 = tf.keras.layers.Conv2D(filters=256, kernel_size=(3,3), 
                            strides=(1,1), padding='same',
                            activation='relu', kernel_initializer='he_uniform')
        # is the activation enough or do we need another relu and activation none
        self.conv2 = tf.keras.layers.Conv2D(filters=256, kernel_size=(1,1), 
                            strides=(1,1), padding='same',
                            activation=None, kernel_initializer='he_uniform')

    def call(self, X):
        new_x = self.relu(X)
        new_x = self.conv1(new_x)
        new_x = self.conv2(new_x)
        new_x += X
        return new_x

class Encoder(tf.keras.Model):
    """
    The encoder consists of:
        2x conv layers with stride 2 and kernel 4x4
        2x residual blocks

    Shapes:
    Enc: shape pre running (10, 256, 256, 1)
    Enc: shape post conv 1 (10, 127, 127, 256)
    Enc: shape post conv 2 (10, 62, 62, 256)
    Enc: shape post resdi 1 (10, 62, 62, 256)
    Enc: shape post resid 2 (10, 62, 62, 256)
    """
    def __init__(self, inputs=None):
        super(Encoder, self).__init__(inputs)
        # Q: no activation? probably yes but not explicitly mentioned in paper
        self.conv1 = tf.keras.layers.Conv2D(filters=256, kernel_size=(4,4), strides=(2,2), padding='same',
                            activation='relu', kernel_initializer='he_uniform')
        # activation in resdiaul?
        self.conv2 = tf.keras.layers.Conv2D(filters=256, kernel_size=(4,4), strides=(2,2), padding='same',
                            activation=None, kernel_initializer='he_uniform')

        self.resid1 = ResidualBlock()

        self.resid2 = ResidualBlock()

    def call(self, X):
        X = self.conv1(X)
        X = self.conv2(X)
        X = self.resid1(X)
        X = self.resid2(X)
        return X

class Decoder(tf.keras.Model):
    """
    Decoder consists of:
        2x residual layers
        2x conv transform layers

    Shapes:
    Dec: shape pre running (10, 62, 62, 256)
    Dec: shape post resid 1 (10, 62, 62, 256)
    Dec: shape post resid 2 (10, 62, 62, 256)
    Dec: shape post conv 1 (10, 126, 126, 256)
    Dec: shape post conv 2 (10, 254, 254, 256)  one missing
    fixed
    """
    def __init__(self, inputs=None):
        super(Decoder, self).__init__(inputs)

        self.resid1 = ResidualBlock()
        self.resid2 = ResidualBlock()

        self.conv_t_1 = tf.keras.layers.Conv2DTranspose(filters=256, padding='same',
                            kernel_size=(4,4), strides=(2,2), activation='relu')

        self.conv_t_2 = tf.keras.layers.Conv2DTranspose(filters=1, padding='same',
                            kernel_size=(4,4), strides=(2,2), activation=None)

    def call(self, X):
        X = self.resid1(X)
        X = self.resid2(X)
        X = self.conv_t_1(X)
        X = self.conv_t_2(X)
        return X

class VQ(tf.keras.Model):
    """
    Used to learn the embedding spcae
    Might get own loss?

    Need to go from batch, height, width, channel to batch*height*width, channels

    Input and output dims depend on dataset

    Shapes:
        ---Shape of data as it comes into VQ: (10, 62, 62, 256)
        ---Shape of data after reshape: (38440, 256)
        Shape of weights (256, 512)
        Shape of distances (38440, 512)
        Shape of distance encodings (38440, 256)
        Shape after quantisisation (10, 62, 62, 256) # yay!!

    source: https://github.com/deepmind/sonnet/blob/v1/sonnet/python/modules/nets/vqvae.py
    TODO: write what you got from here/how used it
    """
    def __init__(self, indim, outdim, inputs=None):
        super(VQ, self).__init__(inputs)
        # we need the set of weights we can train as the embedding with 
        # uniform initilisation
        # intiially i tried to fidn a layer to hold these, but i couldn't make it
        # work with the Embedding layer. So instead just creating a variable
        # and initialising by hand, making sure its trainble
        # this might not be correctly scoped, might need a build instead
        # should be ok to do here since know the shapes, but really i should do in a build
        # once the input has come in
        # https://www.tensorflow.org/guide/keras/custom_layers_and_models
        # add weight, or just make a variable? 
        self.emb = tf.Variable(initial_value=tf.random_uniform_initializer()(shape=(indim, outdim)), 
                        trainable=True, name="emb")

        # self.emb =  add_weight(name="emb", shape=[indim, outdim],  
        #         initializer=tf.random_uniform_initializer(),  trainable=True)

        # self.emb = tf.keras.layers.Embedding(indim, outdim)

        # from paper, need to make sure encoder commits to an embedding so output
        # does not grow. 
        self.commitment_loss = 0.1 # TODO: i picked this randomly
        self.input_dims = indim

    def call(self, X):
        # print("---Shape of data as it comes into VQ: {}".format(X.shape))
        X_shape = X.shape
        # Change shape to batch*height*width, channels
        X_reshaped = tf.reshape(X, (X_shape[0]*X_shape[1]*X_shape[2], X_shape[3]))
        # print("---Shape of data after reshape: {}".format(X_reshaped.shape))

        # calualte distances
        # this is the distance between one row of the encoded output to whats
        # in the embedded space. We need to multiply by the closest in 
        # embedded space, for first we need to calculate what the distance
        # to each of of the e_i in embedded space is. The measure of distance
        # is normal euclidiean distance (L2 norm (normal sum of squares in 2d))
        # Except don't need to take sqrt since square minimises anyway so
        # we just do (a-b)^2 and leave that. Alsow we want to avoid doing the 
        # sum as a for loop so we expand it into a^2+b^2-2ab and do this along
        # one dimention (ab=ba?)
        # This is the same as the math from the source listed in the doc for
        # this class. 
        # TODO: surely there's a funciton for this in tf? also clean this comment
        # so in the end we have sum of squares across rows of X + sum
        # of squares accross columns of weights - 2*rows X * columns weights
        # keepdims=True means we keep these sums in a spare dimention
        distances = tf.math.reduce_sum(X_reshaped**2, axis=1, keepdims=True) +\
                    tf.math.reduce_sum(self.emb**2, axis=0, keepdims=True) -\
                    2*tf.linalg.matmul(X_reshaped, self.emb)
        # print("Shape of distances {}".format(distances.shape))

        # Now we need to pick the e_i with the closest distance for each 
        # line in X, look it up in the table and apply it
        # For some reasons ource does argmin -dis, but its the same?
        indeces = tf.argmin(distances,axis=1)
        #expand to one hot encoding
        encoded = tf.one_hot(indeces, self.input_dims)
        # print("Shape of distance encodings {}".format(encoded.shape))
        indeces = tf.reshape(indeces, X.shape[:-1])
        # look up the indces in the embeddings
        quantised = tf.nn.embedding_lookup(tf.transpose(self.emb, [1, 0]), indeces)
        # I should be able to multiply instead right? Idk why it does the embedding
        # look up its the same thing
        # quantised = tf.linalg.matmul(encoded, self.emb)
        # print("Shape after quantisisation {}".format(quantised.shape))

        # so this has three loss compoenets which are all added to give L (3)
        # the loss for the enc/decoder loss, the loss for the embedding weights
        # and the commitment loss so the output doenst get out of hadn
        # so we need to do our own special sum loss function
        #
        # keep embed loss constant, only learn enc/dec loss 
        # thes ource does this differntly but as far as i can tele its just mse error
        # so why not?
        # enc_dec_loss = tf.keras.metrics.mean_squared_error(tf.stop_gradient(quantised), X)
        # # keep enc/dec loss constnat only learn embed loss
        # embed_loss = tf.keras.metrics.mean_squared_error(quantised, tf.stop_gradient(X))
        # I had these the wrong way around and also mse loss wasn't reducing
        embed_loss = tf.reduce_mean((tf.stop_gradient(quantised) - X) ** 2)
        enc_dec_loss = tf.reduce_mean((quantised - tf.stop_gradient(X)) ** 2)
        total_loss = self.commitment_loss * embed_loss +enc_dec_loss

        # now we need to feed the weights straight through without this operation
        # being optimised for (so no grad during this)
        q = X + tf.stop_gradient(quantised-X)

        #TODO some sources also have prepelxity which seems to be a measure but
        #idk what its for so leave out for now
        # worked out what perplexity is, it checs how many of the e_i are active. 
        # just a metric

        # converst shapes back
        return total_loss, q, encoded, indeces



class VQVAE(tf.keras.Model):
    """
    Information about VQVAE from:
        The paper:  https://arxiv.org/pdf/1711.00937.pdf
    """
    def __init__(self, inputs=None):
        super(VQVAE, self).__init__(inputs)
        
        # encoder compoentnts
        # do i need to expose this so i can get the latent space?
        self.encoder = Encoder()
        # they all have a cnn here but its not mentioned in the paper

        # decoder components
        self.decoder = Decoder()

        # VQ
        self.vq = VQ(256, 512)

    def call(self, X):
        latent = self.encoder(X)
        loss, q, encoded, indeces = self.vq(latent) # not correct but just to test
        recon = self.decoder(q)

        return loss, recon
