"""
Model
Should i make a class or functional
How should it train? using the train function it has or a train loop like i 
wrote for demos


To do:
VAE first (from lecture)
encode and decoder separately as per lecture

modify to vq part

actually seems theyre not related
but there is an encode and decode so do one first then other?

SOURCES/Notes:
VQVAE:
vaes
https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73

from the paper:
grad copy form enc to decoder
encoder: 
        2 conv layrs
            stride 2
            kernel 4x4
        2 residual blocks
            relu
            conv 3x3
            reulu
            conv 1x1
        (all 256 hidden)
decoder:
        2 residual
        2 transposed conv
adam optimiser
lr 2e^-4
batch size 128


Where in the code I have marked that I wasn't sure whether to activate or not
I have referenced this code:
https://github.com/deepmind/sonnet/blob/v1/sonnet/examples/vqvae_example.ipynb
The model seems to be a bit different, it has more than 2 conv layers ect.
Also it has implemented it by using classes and build and then a functional way,
which i didn't do.


TODO:
make parameters pass-in-able
"""

import tensorflow as tf
# TODO switch for readablity
from tensorflow.keras.layers import *

class ResidualBlock(tf.keras.Model):
    """
        One residual bloc consisting of a relu, a 3x3 convolution, 
        another relu and then a 1x1 convolution

        Why did I make this it's own class:
            I think these actually need to be their own classes because we need 
            2 of everything. So if I was to put them all in one class like i had
            planned I'd need: residual1_ con1_, residual1_conv_2, 
                                residual2_conv1, residual2_conv2
            and then again for the decoder.
            so either make these own class or use structural way
            If we were to do it structurally we'd need to repeat the same code 
                4 times as well unless it was generated by a function
            I think the classes with the call function are overkill and
            this architecture might be dummer but it's easier to read imo.
    """
    def __init__(self, inputs=None):
        super(ResidualBlock, self).__init__(inputs)
        self.relu = tf.keras.layers.ReLU()

        self.conv1 = tf.keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), 
                            activation='relu', kernel_initializer='he_uniform')
        # is the activation enough or do we need another relu and activation none
        self.conv2 = tf.keras.layers.Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), 
                            activation=None, kernel_initializer='he_uniform')

    def call(self, X):
        new_x = self.relu(X)
        new_x = self.conv1(new_x)
        new_x = self.conv2(new_x)
        new_x += X
        return new_x

class Encoder(tf.keras.Model):
    """
    The encoder consists of:
        2x conv layers with stride 2 and kernel 4x4
        2x residual blocks
    """
    def __init__(self, inputs=None):
        super(Encoder, self).__init__(inputs)
        # Q: no activation? probably yes but not explicitly mentioned in paper
        self.conv1 = tf.keras.layers.Conv2D(filters=256, kernel_size=(4,4), strides=(2,2), 
                            activation='relu', kernel_initializer='he_uniform')
        # activation in resdiaul?
        self.conv2 = tf.keras.layers.Conv2D(filters=256, kernel_size=(4,4), strides=(2,2), 
                            activation=None, kernel_initializer='he_uniform')

        self.resid1 = ResidualBlock()

        self.resid2 = ResidualBlock()

    def call(self, X):
        X = self.conv1(X)
        X = self.conv2(X)
        X = self.resid1(X)
        X = self.resid2(X)
        return X

class Decoder(tf.keras.Model):
    def __init__(self, inputs=None):
        super(Decoder, self).__init__(inputs)

        self.resid1 = ResidualBlock()
        self.resid2 = ResidualBlock()

        self.conv_t_1 = tf.keras.layers.Conv2DTranspose(filters=256, 
                            kernel_size=(4,4), strides=(2,2), activation='relu')

        self.conv_t_2 = tf.keras.layers.Conv2DTranspose(filters=256, 
                            kernel_size=(4,4), strides=(2,2), activation=None)

    def call(self, X):
        X = self.resid1(X)
        X = self.resid2(X)
        X = self.conv_t_1(X)
        X = self.conv_t_2(X)
        return X

class VQVAE(tf.keras.Model):
    """
    Information about VQVAE from:
        The paper:  https://arxiv.org/pdf/1711.00937.pdf
    """
    def __init__(self, inputs=None):
        super(VQVAE, self).__init__(inputs)
        
        # encoder compoentnts
        # do i need to expose this so i can get the latent space?
        self.encoder = Encoder()

        # decoder components
        self.decoder = Decoder()

    def call(self, X):
        pass