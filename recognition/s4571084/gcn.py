"""gcn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19CHiNNkww39vm_KI_Ov0GMJekRuEg3BF
"""

import numpy as np
import scipy.sparse as sp
import torch
from sklearn.preprocessing import LabelBinarizer


def normalize_adj(adjacency):
    adjacency += sp.eye(adjacency.shape[0])
    degree = np.array(adjacency.sum(1))
    d_hat = sp.diags(np.power(degree, -0.5).flatten())
    return d_hat.dot(adjacency).dot(d_hat).tocoo()


def normalize_features(features):
    return features / features.sum(1)


def load_data():
    dataset = np.load('/content/drive/My Drive/Dataset/facebook.npz')
    edges = dataset['edges']
    features = dataset['features']
    target = dataset['target']

    # set adjacency
    n = len(target)
    x = np.zeros((n, n), dtype=np.float32)
    for i in edges:
        x[i[0]][i[1]] = 1

    edges = sp.csr_matrix(x)
    adjacency = normalize_adj(edges)

    # set features
    features = sp.csr_matrix(features, dtype=np.float32)
    features = normalize_features(features)
    features = torch.FloatTensor(np.array(features))

    # set labels
    encode_onehot = LabelBinarizer()
    labels = encode_onehot.fit_transform(target)
    labels = torch.LongTensor(np.where(labels)[1])

    num_nodes = features.shape[0]
    train_mask = np.zeros(num_nodes, dtype=np.bool)
    val_mask = np.zeros(num_nodes, dtype=np.bool)
    test_mask = np.zeros(num_nodes, dtype=np.bool)

    train_mask[0:140] = True
    val_mask[200:500] = True
    test_mask[500:1500] = True

    return adjacency, features, labels, train_mask, val_mask, test_mask
