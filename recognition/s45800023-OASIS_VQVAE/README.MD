# Generative Model for the OASIS Brain Dataset Using VQVAE

## Description
This algorithm aims to implement a generative model for the OASIS brain dataset. To achieve this we can use a Vector-quantized Variational Auto-encoder to learn an embedding space. We can then feed encoded data into a DCGAN network to then generate realistic images from random noise. Descriptions of each model follow.
### VQVAE
The VQVAE model was presented in 2017 as a new generative model that can learn discrete representations. We know how VAE's work, using an encoder and decoder network to project images into a latent space, then learn to reconstruct them. However continuous vector spaces are used to represent the latent space, which is typically not how we find data in the real world. VQ-VAE's solve this problem. The encoder outputs are discrete and priors are learnt. To alleviate posterior collapse, they introduce a vector-quantizer into the model, hence the name. The high level architecture is displayed below.

![vqvaeArchitecture](./Resources/vqvaeA.png)

We use a VQVAE here to learn a discrete embedding space representation of the OASIS dataset. 
### DCGAN
To then generate fake images, we need a generator network, thus a GAN became necessary. The DCGAN (deep-convolutional generative adversarial network) was introduced in 2015, producing strong results on various image datasets. GAN use a discriminator network to learn real/fake images, and the generate learns to generate fake images, competing each other in a zero-sum game. DCGAN's expand upon this idea by using convolutional layers, making them powerful for image learning. The high level architecture is displayed below:

![dcganArchitecture](./Resources/dcganA.png)

## Training/Results
For training, the two main networks had to be trained seperately. The process is as follows.
### VQVAE
For the VQVAE we set up our encoder, decoder and vector quantizer models, and train them on the original images transformed to RGB. Training loss is displayed below:

![dcganLoss](./Resources/vqvaeloss.png)

With the trained network, we can output an example of a real image and the reconstructed version:

![realVreconstructed](./Resources/realvrec.png)

We can also visualize an embedding slice from the network and the quantized output:

![embedding](./Resources/embeddingslice.png)

And a UMAP projection of what the overall embedding space looks like:

![projection](./Resources/umap.png)

The above results already show the network is trained and we have a viable model.

### DCGAN
To then generate images, we train on input from the embedding space generated by the VQVAE. Thus the discriminator network trains with encoded data as the real data, and the generator attempts to generate fake codebook indices. The training process is the same as other GAN networks, where we first hold the generator constant and train the discriminator and find loss, and then we train the generator while holding the discriminator and loss. Refer to references for a more detailed explanation.


## Discussion

## Instructions to replicate results
1. Change the train, test, val dir variables in datasets.py to where you have the image set stored on your computer. 
2. In trainVQVAE and trainDCGAN classes, change torch.save path to desired location.
3. Run train.py
4. In predict, torch.load for VQVAE and Generator to where you stored models in step 2.
5. Run predict.py

Please note this is subject to model tuning. The VQVAE hyperparameters are stored, and was trained over 5 epochs. THE DCGAN model, I recommend observing the loss outputted and tuning the model as required. Results here were achieved with hyperparameters stored and running over 2 epochs.

## Depedencies

## References
https://developers.google.com/machine-learning/gan

https://www.researchgate.net/figure/The-architecture-of-the-generator-and-the-discriminator-in-a-DCGAN-model-FSC-is-the_fig4_343597759

https://arxiv.org/abs/1511.06434 (DCGAN paper)

https://arxiv.org/abs/1711.00937 (VQVAE paper)

https://colab.research.google.com/github/zalandoresearch/pytorch-vq-vae/blob/master/vq-vae.ipynb

https://ml.berkeley.edu/blog/posts/vq-vae/

