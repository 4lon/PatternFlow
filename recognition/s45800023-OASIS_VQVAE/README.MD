# Generative Model for the OASIS Brain Dataset Using VQVAE

## Description
This algorithm aims to implement a generative model for the OASIS brain dataset. To achieve this we can use a Vector-quantized Variational Auto-encoder to learn an embedding space. We can then feed encoded data into a DCGAN network to then generate realistic images from random noise. Descriptions of each model follow.
### VQVAE
The VQVAE model was presented in 2017 as a new generative model that can learn discrete representations. We know how VAE's work, using an encoder and decoder network to project images into a latent space, then learn to reconstruct them. However continuous vector spaces are used to represent the latent space, which is typically not how we find data in the real world. VQ-VAE's solve this problem. The encoder outputs are discrete and priors are learnt. To alleviate posterior collapse, they introduce a vector-quantizer into the model, hence the name. The high level architecture is displayed below.

![vqvaeArchitecture](/Resources/vqvaeA.png)

We use a VQVAE here to learn a discrete embedding space representation of the OASIS dataset. 
### DCGAN
To then generate fake images, we need a generator network, thus a GAN became necessary. The DCGAN (deep-convolutional generative adversarial network) was introduced in 2015, producing strong results on various image datasets. GAN use a discriminator network to learn real/fake images, and the generate learns to generate fake images, competing each other in a zero-sum game. DCGAN's expand upon this idea by using convolutional layers, making them powerful for image learning. The high level architecture is displayed below:

![dcganArchitecture](/Resources/dcganA.png)

## Training/Results
For training, the two main networks had to be trained seperately. The process is as follows.
### VQVAE
For the VQVAE we set up our encoder, decoder and vector quantizer models, and train them on the original images transformed to RGB. Training loss is displayed below:

![dcganLoss](/Resources/vqvaeloss.png)

With the trained network, we can output an example of a real image and the reconstructed version:

![realVreconstructed](/Resources/realvrec.png)

We can also visualize an embedding slice from the network:

![embedding](/Resources/embeddingslice.png)

And a UMAP projection of what the overall embedding space looks like:

![projection](/Resources/umap.png)

One could further quantize the embedding slice to see the output, but the above results already show the network is trained and we have a viable model.

### DCGAN
To then generate images, we train on input from the embedding space generated by the VQVAE. Thus the discriminator network trains with encoded data as the real data, and the generator attempts to generate fakes. The training process is the same as other GAN networks, where we first hold the generator constant and train the discriminator and find loss, and then we train the generator while holding the discriminator and loss. Refer to references for a more detailed explanation.


## Discussion

## Instructions to run code

## Depedencies

## References
https://developers.google.com/machine-learning/gan

https://www.researchgate.net/figure/The-architecture-of-the-generator-and-the-discriminator-in-a-DCGAN-model-FSC-is-the_fig4_343597759

https://arxiv.org/abs/1511.06434 (DCGAN paper)

https://arxiv.org/abs/1711.00937 (VQVAE paper)

https://colab.research.google.com/github/zalandoresearch/pytorch-vq-vae/blob/master/vq-vae.ipynb

https://ml.berkeley.edu/blog/posts/vq-vae/

