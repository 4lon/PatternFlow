{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76380e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8703cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "load data from data set.\n",
    "\n",
    "return: adjacency matrix(sparse matrix), features(sparse) and labels\n",
    "\"\"\"\n",
    "def load_data():\n",
    "    cat_data = np.load(\"F:\\\\3710report\\\\facebook.npz\")\n",
    "    n = len(cat_data[\"target\"])\n",
    "    x = np.zeros((n, n), dtype=np.float32)\n",
    "    for i in cat_data[\"edges\"]:\n",
    "        x[i[0]][i[1]] = 1\n",
    "    return sp.csr_matrix(x), sp.csr_matrix(cat_data[\"features\"], dtype=np.float32), cat_data[\"target\"]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "normalize the data.\n",
    "\n",
    "Param: adjacency matrix.\n",
    "return: normalized adjacency matrix.\n",
    "\"\"\"\n",
    "def normalize_adj(adjacency):\n",
    "    adjacency += sp.eye(adjacency.shape[0])\n",
    "    degree = np.array(adjacency.sum(1))\n",
    "    d_hat = sp.diags(np.power(degree, -0.5).flatten())\n",
    "    return d_hat.dot(adjacency).dot(d_hat).tocoo()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "normalize features.\n",
    "\n",
    "Param: featrues.\n",
    "return: normalized features.\n",
    "\"\"\"\n",
    "def normalize_features(features):\n",
    "    return features / features.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac1dfa55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t0.5000000000000001\n",
      "  (0, 18427)\t0.09805806756909202\n",
      "  (1, 1)\t0.028571428571428567\n",
      "  (1, 2812)\t0.02492223931396134\n",
      "  (1, 4987)\t0.016903085094570332\n",
      "  (1, 5228)\t0.020965696734438363\n",
      "  (1, 5307)\t0.015240998561973751\n",
      "  (1, 5755)\t0.020348923188911988\n",
      "  (1, 6829)\t0.031943828249996996\n",
      "  (1, 7136)\t0.033149677206589796\n",
      "  (1, 8049)\t0.012135707849456652\n",
      "  (1, 8533)\t0.027788500718836418\n",
      "  (1, 8894)\t0.023218173010628604\n",
      "  (1, 9934)\t0.019920476822239894\n",
      "  (1, 10281)\t0.04517539514526256\n",
      "  (1, 10379)\t0.007805119495830757\n",
      "  (1, 10554)\t0.02238868314198225\n",
      "  (1, 11557)\t0.019783564706223267\n",
      "  (1, 12305)\t0.020498001542269693\n",
      "  (1, 13737)\t0.021295885499998\n",
      "  (1, 14344)\t0.02366905341655754\n",
      "  (1, 15026)\t0.02577696311132335\n",
      "  (1, 15785)\t0.013894250359418209\n",
      "  (1, 16260)\t0.03253000243161777\n",
      "  (1, 16590)\t0.018018749253911177\n",
      "  :\t:\n",
      "  (22467, 5339)\t0.038235955645093626\n",
      "  (22467, 6181)\t0.04233337566673017\n",
      "  (22467, 8565)\t0.03984095364447979\n",
      "  (22467, 9367)\t0.02703690352179376\n",
      "  (22467, 9986)\t0.03367175148507369\n",
      "  (22467, 10347)\t0.048112522432468816\n",
      "  (22467, 10775)\t0.04376881095324085\n",
      "  (22467, 11187)\t0.04622501635210243\n",
      "  (22467, 11942)\t0.050251890762960605\n",
      "  (22467, 13121)\t0.048112522432468816\n",
      "  (22467, 16023)\t0.04103049699311091\n",
      "  (22467, 16564)\t0.0445435403187374\n",
      "  (22467, 16813)\t0.040422604172722164\n",
      "  (22467, 22467)\t0.05555555555555555\n",
      "  (22468, 640)\t0.3333333333333333\n",
      "  (22468, 4459)\t0.2041241452319315\n",
      "  (22468, 22468)\t0.3333333333333333\n",
      "  (22469, 2009)\t0.14433756729740646\n",
      "  (22469, 2062)\t0.15811388300841897\n",
      "  (22469, 7699)\t0.14433756729740646\n",
      "  (22469, 10823)\t0.14433756729740646\n",
      "  (22469, 11417)\t0.25000000000000006\n",
      "  (22469, 14921)\t0.12500000000000003\n",
      "  (22469, 19369)\t0.12500000000000003\n",
      "  (22469, 22469)\t0.12500000000000003\n",
      "tensor([[ 0.0210,  0.0222,  0.0210,  ...,  0.0172,  0.0301,  0.0179],\n",
      "        [ 0.0104,  0.0109,  0.0103,  ...,  0.0085,  0.0144,  0.0051],\n",
      "        [-0.0101, -0.0102, -0.0101,  ..., -0.0083, -0.0144, -0.0086],\n",
      "        ...,\n",
      "        [ 0.0130,  0.0137,  0.0130,  ...,  0.0090,  0.0185,  0.0111],\n",
      "        [ 0.0119,  0.0126,  0.0119,  ...,  0.0098,  0.0171,  0.0099],\n",
      "        [ 0.0084,  0.0099,  0.0094,  ...,  0.0070,  0.0135,  0.0080]])\n",
      "tensor([0, 2, 1,  ..., 2, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "adjacency, features, labels = load_data()\n",
    "\n",
    "#encoded to one hot\n",
    "onehot = LabelBinarizer()\n",
    "labels = onehot.fit_transform(labels)\n",
    "\n",
    "#normalize adjacency matrix and features\n",
    "adjacency = normalize_adj(adjacency)\n",
    "features = normalize_features(features)\n",
    "\n",
    "#transform to tensor.\n",
    "features = torch.FloatTensor(np.array(features))\n",
    "labels = torch.LongTensor(np.where(labels)[1])\n",
    "\n",
    "print(adjacency)\n",
    "print(features)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd9177eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_number = features.shape[0]\n",
    "\n",
    "#get train mask validation mask and test mask. (split the data set)\n",
    "train_mask = np.zeros(node_number, dtype=bool)\n",
    "val_mask = np.zeros(node_number, dtype=bool)\n",
    "test_mask = np.zeros(node_number, dtype=bool)\n",
    "train_mask[:500] = True\n",
    "val_mask[500:1000] = True\n",
    "test_mask[1000:1500] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be52a7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Graph convolution network layer.\n",
    "\n",
    "\"\"\"\n",
    "class GCNLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    initialiser of graph convolution layer.\n",
    "    \n",
    "    Params:\n",
    "    input_dim: input dimension of this layer.\n",
    "    out_dim: output dimension of this layer.\n",
    "    use_bias: if use bias(optional).\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, use_bias=True):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.use_bias = use_bias\n",
    "        self.weight = torch.nn.Parameter(torch.Tensor(input_dim, output_dim))\n",
    "        if self.use_bias:\n",
    "            self.bias = torch.nn.Parameter(torch.Tensor(output_dim))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    \"\"\"\n",
    "    reset parameters.\n",
    "    \"\"\"\n",
    "    def reset_parameters(self):\n",
    "        #initialise weight.\n",
    "        init.kaiming_uniform_(self.weight)\n",
    "        if self.use_bias:\n",
    "            init.zeros_(self.bias)\n",
    "\n",
    "    \"\"\"\n",
    "    Define computation performed at every call.\n",
    "    \n",
    "    Params:\n",
    "    adjacency: adjacency matrix.\n",
    "    input_feature: features of every data in dataset.\n",
    "    \"\"\"\n",
    "    def forward(self, adjacency, input_feature):\n",
    "        device = \"cpu\"\n",
    "        support = torch.mm(input_feature, self.weight.to(device))\n",
    "        # adjacency is sparse matrix so it need torch.sparse.mm instead of torch.mm\n",
    "        output = torch.sparse.mm(adjacency, support)\n",
    "        if self.use_bias:\n",
    "            output += self.bias.to(device)\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
