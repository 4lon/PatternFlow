{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "76380e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "d8703cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "load data from data set.\n",
    "\n",
    "return: adjacency matrix(sparse matrix), features(sparse) and labels\n",
    "\"\"\"\n",
    "def load_data(path):\n",
    "    cat_data = np.load(path)\n",
    "    n = len(cat_data[\"target\"])\n",
    "    x = np.zeros((n, n), dtype=np.float32)\n",
    "    for i in cat_data[\"edges\"]:\n",
    "        x[i[0]][i[1]] = 1\n",
    "    return sp.csr_matrix(x), sp.csr_matrix(cat_data[\"features\"], dtype=np.float32), cat_data[\"target\"]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "normalize the data.\n",
    "\n",
    "Param: adjacency matrix.\n",
    "return: normalized adjacency matrix.\n",
    "\"\"\"\n",
    "def normalize_adj(adjacency):\n",
    "    adjacency += sp.eye(adjacency.shape[0])\n",
    "    degree = np.array(adjacency.sum(1))\n",
    "    d_hat = sp.diags(np.power(degree, -0.5).flatten())\n",
    "    return d_hat.dot(adjacency).dot(d_hat).tocoo()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "normalize features.\n",
    "\n",
    "Param: features.\n",
    "return: normalized features.\n",
    "\"\"\"\n",
    "def normalize_features(features):\n",
    "    return features / features.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "ac1dfa55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t0.5000000000000001\n",
      "  (0, 18427)\t0.09805806756909202\n",
      "  (1, 1)\t0.028571428571428567\n",
      "  (1, 2812)\t0.02492223931396134\n",
      "  (1, 4987)\t0.016903085094570332\n",
      "  (1, 5228)\t0.020965696734438363\n",
      "  (1, 5307)\t0.015240998561973751\n",
      "  (1, 5755)\t0.020348923188911988\n",
      "  (1, 6829)\t0.031943828249996996\n",
      "  (1, 7136)\t0.033149677206589796\n",
      "  (1, 8049)\t0.012135707849456652\n",
      "  (1, 8533)\t0.027788500718836418\n",
      "  (1, 8894)\t0.023218173010628604\n",
      "  (1, 9934)\t0.019920476822239894\n",
      "  (1, 10281)\t0.04517539514526256\n",
      "  (1, 10379)\t0.007805119495830757\n",
      "  (1, 10554)\t0.02238868314198225\n",
      "  (1, 11557)\t0.019783564706223267\n",
      "  (1, 12305)\t0.020498001542269693\n",
      "  (1, 13737)\t0.021295885499998\n",
      "  (1, 14344)\t0.02366905341655754\n",
      "  (1, 15026)\t0.02577696311132335\n",
      "  (1, 15785)\t0.013894250359418209\n",
      "  (1, 16260)\t0.03253000243161777\n",
      "  (1, 16590)\t0.018018749253911177\n",
      "  :\t:\n",
      "  (22467, 5339)\t0.038235955645093626\n",
      "  (22467, 6181)\t0.04233337566673017\n",
      "  (22467, 8565)\t0.03984095364447979\n",
      "  (22467, 9367)\t0.02703690352179376\n",
      "  (22467, 9986)\t0.03367175148507369\n",
      "  (22467, 10347)\t0.048112522432468816\n",
      "  (22467, 10775)\t0.04376881095324085\n",
      "  (22467, 11187)\t0.04622501635210243\n",
      "  (22467, 11942)\t0.050251890762960605\n",
      "  (22467, 13121)\t0.048112522432468816\n",
      "  (22467, 16023)\t0.04103049699311091\n",
      "  (22467, 16564)\t0.0445435403187374\n",
      "  (22467, 16813)\t0.040422604172722164\n",
      "  (22467, 22467)\t0.05555555555555555\n",
      "  (22468, 640)\t0.3333333333333333\n",
      "  (22468, 4459)\t0.2041241452319315\n",
      "  (22468, 22468)\t0.3333333333333333\n",
      "  (22469, 2009)\t0.14433756729740646\n",
      "  (22469, 2062)\t0.15811388300841897\n",
      "  (22469, 7699)\t0.14433756729740646\n",
      "  (22469, 10823)\t0.14433756729740646\n",
      "  (22469, 11417)\t0.25000000000000006\n",
      "  (22469, 14921)\t0.12500000000000003\n",
      "  (22469, 19369)\t0.12500000000000003\n",
      "  (22469, 22469)\t0.12500000000000003\n",
      "tensor([[ 0.0210,  0.0222,  0.0210,  ...,  0.0172,  0.0301,  0.0179],\n",
      "        [ 0.0104,  0.0109,  0.0103,  ...,  0.0085,  0.0144,  0.0051],\n",
      "        [-0.0101, -0.0102, -0.0101,  ..., -0.0083, -0.0144, -0.0086],\n",
      "        ...,\n",
      "        [ 0.0130,  0.0137,  0.0130,  ...,  0.0090,  0.0185,  0.0111],\n",
      "        [ 0.0119,  0.0126,  0.0119,  ...,  0.0098,  0.0171,  0.0099],\n",
      "        [ 0.0084,  0.0099,  0.0094,  ...,  0.0070,  0.0135,  0.0080]])\n",
      "tensor([0, 2, 1,  ..., 2, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "PATH = \"F:\\\\3710report\\\\facebook.npz\"\n",
    "\n",
    "adjacency, features, labels = load_data(PATH)\n",
    "\n",
    "#encoded the label to one hot\n",
    "onehot = LabelBinarizer()\n",
    "labels = onehot.fit_transform(labels)\n",
    "\n",
    "#normalize adjacency matrix and features\n",
    "adjacency = normalize_adj(adjacency)\n",
    "features = normalize_features(features)\n",
    "\n",
    "#transform to tensor.\n",
    "features = torch.FloatTensor(np.array(features))\n",
    "labels = torch.LongTensor(np.where(labels)[1])\n",
    "\n",
    "print(adjacency)\n",
    "print(features)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "cd9177eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_number = features.shape[0]\n",
    "\n",
    "#get train mask, validation mask and test mask. (split the data set)\n",
    "train_mask = np.zeros(node_number, dtype=bool)\n",
    "val_mask = np.zeros(node_number, dtype=bool)\n",
    "test_mask = np.zeros(node_number, dtype=bool)\n",
    "train_mask[:3000] = True\n",
    "val_mask[3000:4000] = True\n",
    "test_mask[4000:5000] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "7e38cf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Graph convolution network layer.\n",
    "\n",
    "\"\"\"\n",
    "class GCNLayer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    initialiser of graph convolution layer.\n",
    "    \n",
    "    Params:\n",
    "    input_dim: input dimension of this layer.\n",
    "    out_dim: output dimension of this layer.\n",
    "    use_bias: if use bias(optional).\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, use_bias=True):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.use_bias = use_bias\n",
    "        self.weight = torch.nn.Parameter(torch.Tensor(input_dim, output_dim))\n",
    "        if self.use_bias:\n",
    "            self.bias = torch.nn.Parameter(torch.Tensor(output_dim))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    \"\"\"\n",
    "    initialise parameters.\n",
    "    \"\"\"\n",
    "    def reset_parameters(self):\n",
    "        #initialise weight.\n",
    "        torch.nn.init.kaiming_uniform_(self.weight)\n",
    "        if self.use_bias:\n",
    "            torch.nn.init.zeros_(self.bias)\n",
    "\n",
    "    \"\"\"\n",
    "    Define computation performed at every call.\n",
    "    \n",
    "    Params:\n",
    "    adjacency: adjacency matrix.\n",
    "    input_feature: features of every data in dataset.\n",
    "    \"\"\"\n",
    "    def forward(self, adjacency, input_feature):\n",
    "        device = \"cpu\"\n",
    "        support = torch.mm(input_feature, self.weight.to(device))\n",
    "        # adjacency is sparse matrix so it need torch.sparse.mm instead of torch.mm\n",
    "        output = torch.sparse.mm(adjacency, support)\n",
    "        if self.use_bias:\n",
    "            output += self.bias.to(device)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "b71af010",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Graph convolution network.\n",
    "Consist of two gcn layers.\n",
    "\"\"\"\n",
    "class GCN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    initialise the gcn model.\n",
    "    Param:\n",
    "    input_dim: the input dimension of the features.(128 dimensions of facebook dataset.)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=128):\n",
    "        super(GCN, self).__init__()\n",
    "        self.gcn_layer1 = GCNLayer(input_dim, 16)\n",
    "        \n",
    "        #4 classes in this dataset.\n",
    "        self.gcn_layer2 = GCNLayer(16, 4)\n",
    "        \n",
    "        \n",
    "    \"\"\"\n",
    "    Define computation performed at every call.\n",
    "    \n",
    "    Params:\n",
    "    adjacency: adjacency matrix.\n",
    "    feature: features of every data in dataset.\n",
    "    \"\"\"\n",
    "    def forward(self, adjacency, feature):\n",
    "        hidden = torch.nn.functional.relu(self.gcn_layer1(adjacency, feature))\n",
    "        output = self.gcn_layer2(adjacency, hidden)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "24fc14be",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1 # learning rate.\n",
    "weight_decay = 5e-4 # weight decay to prevent overfitting.\n",
    "epochs = 200 # we train 200 epochs.\n",
    "\n",
    "device = \"cpu\" # we don't have gpu so we use cpu as our device.\n",
    "model = GCN().to(device) # Initialise the model and load it to device.\n",
    "\n",
    "# initialise loss function and optimizer.\n",
    "# we use CrossEntropyLoss as our loss function and adam as our optimizer/\n",
    "loss = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# load features and labels to device.\n",
    "tensor_features = features.to(device)\n",
    "tensor_labels = labels.to(device)\n",
    "\n",
    "# Transform masks to pytorch tensor and load them to cpu.\n",
    "tensor_train_mask = torch.from_numpy(train_mask).to(device)\n",
    "tensor_val_mask = torch.from_numpy(val_mask).to(device)\n",
    "tensor_test_mask = torch.from_numpy(test_mask).to(device)\n",
    "\n",
    "# initialise the adjacency matrix tensor and load it to cpu.\n",
    "indices = torch.from_numpy(np.asarray([adjacency.row, adjacency.col]).astype('int64')).long()\n",
    "values = torch.from_numpy(adjacency.data.astype(np.float32))\n",
    "# we have 22470 data in this dataset.\n",
    "tensor_adjacency = torch.sparse.FloatTensor(indices, values, (22470, 22470)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "589d6f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "test the model to know the accuarcy on corresponding mask.\n",
    "\"\"\"\n",
    "def test(mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(tensor_adjacency, tensor_features)\n",
    "        test_mask_logits = logits[mask]\n",
    "        predict_labels = test_mask_logits.max(1)[1]\n",
    "        accuarcy = torch.eq(predict_labels, tensor_labels[mask]).float().mean()\n",
    "    return accuarcy, test_mask_logits.cpu().numpy(), tensor_labels[mask].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "84f94373",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "main training process.\n",
    "\"\"\"\n",
    "def train():\n",
    "    loss_history = []\n",
    "    val_acc_history = []\n",
    "    model.train()\n",
    "    train_labels = tensor_labels[tensor_train_mask]\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        logits = model(tensor_adjacency, tensor_features) # Forward propagation.\n",
    "        train_mask_logits = logits[tensor_train_mask] # Only nodes on training set are selected for supervision.\n",
    "        loss_val = loss(train_mask_logits, train_labels) # Calculate loss value.\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward() # Back propagation to calculate the gradient of parameters.\n",
    "        optimizer.step() # Use optimizer to update gradient.\n",
    "        \n",
    "        train_acc, _, _ = test(tensor_train_mask) # Calculate the accuracy on the training set.\n",
    "        val_acc, _, _ = test(tensor_val_mask) # Calculate the accuracy on the validation set.\n",
    "        \n",
    "        \"\"\"\n",
    "        Record the changes of loss value and accuracy in \n",
    "        the training process for visualization.\n",
    "        \"\"\"\n",
    "        loss_history.append(loss_val.item()) \n",
    "        val_acc_history.append(val_acc.item())\n",
    "        # print training result every 10 epochs.\n",
    "        print(\"Epoch %3d/%d: Loss %.4f, Train_accuracy %.4f, Validation_accuracy %.4f, Time %.4f\"%(\n",
    "                epoch, epochs, loss_val.item(), train_acc.item(), val_acc.item(), time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "68f5fdc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/200: Loss 1.4825, Train_accuracy 0.3350, Validation_accuracy 0.3070, Time 0.1251\n",
      "Epoch   2/200: Loss 1.3448, Train_accuracy 0.3703, Validation_accuracy 0.3660, Time 0.1271\n",
      "Epoch   3/200: Loss 1.3166, Train_accuracy 0.4730, Validation_accuracy 0.4380, Time 0.1421\n",
      "Epoch   4/200: Loss 1.2728, Train_accuracy 0.4930, Validation_accuracy 0.4630, Time 0.1421\n",
      "Epoch   5/200: Loss 1.2348, Train_accuracy 0.5567, Validation_accuracy 0.5480, Time 0.1421\n",
      "Epoch   6/200: Loss 1.1916, Train_accuracy 0.6397, Validation_accuracy 0.6280, Time 0.1411\n",
      "Epoch   7/200: Loss 1.1470, Train_accuracy 0.6637, Validation_accuracy 0.6590, Time 0.1411\n",
      "Epoch   8/200: Loss 1.0929, Train_accuracy 0.6927, Validation_accuracy 0.6970, Time 0.1351\n",
      "Epoch   9/200: Loss 1.0360, Train_accuracy 0.6933, Validation_accuracy 0.6980, Time 0.1351\n",
      "Epoch  10/200: Loss 0.9861, Train_accuracy 0.6977, Validation_accuracy 0.7040, Time 0.1351\n",
      "Epoch  11/200: Loss 0.9355, Train_accuracy 0.7040, Validation_accuracy 0.7120, Time 0.1481\n",
      "Epoch  12/200: Loss 0.8817, Train_accuracy 0.7220, Validation_accuracy 0.7410, Time 0.1361\n",
      "Epoch  13/200: Loss 0.8283, Train_accuracy 0.7410, Validation_accuracy 0.7530, Time 0.1331\n",
      "Epoch  14/200: Loss 0.7845, Train_accuracy 0.7643, Validation_accuracy 0.7790, Time 0.1291\n",
      "Epoch  15/200: Loss 0.7436, Train_accuracy 0.7810, Validation_accuracy 0.7960, Time 0.1261\n",
      "Epoch  16/200: Loss 0.7006, Train_accuracy 0.7853, Validation_accuracy 0.8000, Time 0.1261\n",
      "Epoch  17/200: Loss 0.6577, Train_accuracy 0.7887, Validation_accuracy 0.8000, Time 0.1391\n",
      "Epoch  18/200: Loss 0.6206, Train_accuracy 0.8063, Validation_accuracy 0.8080, Time 0.1271\n",
      "Epoch  19/200: Loss 0.5864, Train_accuracy 0.8197, Validation_accuracy 0.8180, Time 0.1381\n",
      "Epoch  20/200: Loss 0.5545, Train_accuracy 0.8380, Validation_accuracy 0.8310, Time 0.1361\n",
      "Epoch  21/200: Loss 0.5271, Train_accuracy 0.8457, Validation_accuracy 0.8360, Time 0.1441\n",
      "Epoch  22/200: Loss 0.5018, Train_accuracy 0.8443, Validation_accuracy 0.8380, Time 0.1291\n",
      "Epoch  23/200: Loss 0.4788, Train_accuracy 0.8543, Validation_accuracy 0.8370, Time 0.1321\n",
      "Epoch  24/200: Loss 0.4597, Train_accuracy 0.8610, Validation_accuracy 0.8530, Time 0.1301\n",
      "Epoch  25/200: Loss 0.4415, Train_accuracy 0.8707, Validation_accuracy 0.8620, Time 0.1291\n",
      "Epoch  26/200: Loss 0.4252, Train_accuracy 0.8710, Validation_accuracy 0.8710, Time 0.1321\n",
      "Epoch  27/200: Loss 0.4111, Train_accuracy 0.8800, Validation_accuracy 0.8620, Time 0.1441\n",
      "Epoch  28/200: Loss 0.3992, Train_accuracy 0.8810, Validation_accuracy 0.8670, Time 0.1371\n",
      "Epoch  29/200: Loss 0.3892, Train_accuracy 0.8880, Validation_accuracy 0.8730, Time 0.1301\n",
      "Epoch  30/200: Loss 0.3801, Train_accuracy 0.8903, Validation_accuracy 0.8710, Time 0.1231\n",
      "Epoch  31/200: Loss 0.3741, Train_accuracy 0.8913, Validation_accuracy 0.8730, Time 0.1261\n",
      "Epoch  32/200: Loss 0.3693, Train_accuracy 0.8883, Validation_accuracy 0.8680, Time 0.1301\n",
      "Epoch  33/200: Loss 0.3663, Train_accuracy 0.8917, Validation_accuracy 0.8650, Time 0.1221\n",
      "Epoch  34/200: Loss 0.3624, Train_accuracy 0.8960, Validation_accuracy 0.8730, Time 0.1391\n",
      "Epoch  35/200: Loss 0.3575, Train_accuracy 0.8977, Validation_accuracy 0.8810, Time 0.1311\n",
      "Epoch  36/200: Loss 0.3527, Train_accuracy 0.8990, Validation_accuracy 0.8770, Time 0.1321\n",
      "Epoch  37/200: Loss 0.3511, Train_accuracy 0.8960, Validation_accuracy 0.8720, Time 0.1501\n",
      "Epoch  38/200: Loss 0.3509, Train_accuracy 0.8990, Validation_accuracy 0.8820, Time 0.1301\n",
      "Epoch  39/200: Loss 0.3513, Train_accuracy 0.8947, Validation_accuracy 0.8690, Time 0.1321\n",
      "Epoch  40/200: Loss 0.3494, Train_accuracy 0.8993, Validation_accuracy 0.8800, Time 0.1381\n",
      "Epoch  41/200: Loss 0.3469, Train_accuracy 0.9003, Validation_accuracy 0.8830, Time 0.1201\n",
      "Epoch  42/200: Loss 0.3452, Train_accuracy 0.8963, Validation_accuracy 0.8740, Time 0.1361\n",
      "Epoch  43/200: Loss 0.3460, Train_accuracy 0.9023, Validation_accuracy 0.8790, Time 0.1361\n",
      "Epoch  44/200: Loss 0.3475, Train_accuracy 0.8937, Validation_accuracy 0.8690, Time 0.1291\n",
      "Epoch  45/200: Loss 0.3489, Train_accuracy 0.9033, Validation_accuracy 0.8770, Time 0.1251\n",
      "Epoch  46/200: Loss 0.3486, Train_accuracy 0.9003, Validation_accuracy 0.8710, Time 0.1321\n",
      "Epoch  47/200: Loss 0.3462, Train_accuracy 0.9013, Validation_accuracy 0.8810, Time 0.1261\n",
      "Epoch  48/200: Loss 0.3440, Train_accuracy 0.9010, Validation_accuracy 0.8800, Time 0.1351\n",
      "Epoch  49/200: Loss 0.3438, Train_accuracy 0.8990, Validation_accuracy 0.8700, Time 0.1361\n",
      "Epoch  50/200: Loss 0.3451, Train_accuracy 0.9000, Validation_accuracy 0.8790, Time 0.1231\n",
      "Epoch  51/200: Loss 0.3464, Train_accuracy 0.8960, Validation_accuracy 0.8670, Time 0.1341\n",
      "Epoch  52/200: Loss 0.3466, Train_accuracy 0.9040, Validation_accuracy 0.8820, Time 0.1321\n",
      "Epoch  53/200: Loss 0.3447, Train_accuracy 0.9010, Validation_accuracy 0.8750, Time 0.1231\n",
      "Epoch  54/200: Loss 0.3413, Train_accuracy 0.9003, Validation_accuracy 0.8750, Time 0.1361\n",
      "Epoch  55/200: Loss 0.3399, Train_accuracy 0.9047, Validation_accuracy 0.8820, Time 0.1401\n",
      "Epoch  56/200: Loss 0.3404, Train_accuracy 0.9020, Validation_accuracy 0.8680, Time 0.1231\n",
      "Epoch  57/200: Loss 0.3403, Train_accuracy 0.9063, Validation_accuracy 0.8870, Time 0.1281\n",
      "Epoch  58/200: Loss 0.3387, Train_accuracy 0.9070, Validation_accuracy 0.8760, Time 0.1321\n",
      "Epoch  59/200: Loss 0.3359, Train_accuracy 0.9043, Validation_accuracy 0.8830, Time 0.1231\n",
      "Epoch  60/200: Loss 0.3345, Train_accuracy 0.9097, Validation_accuracy 0.8830, Time 0.1301\n",
      "Epoch  61/200: Loss 0.3343, Train_accuracy 0.9033, Validation_accuracy 0.8720, Time 0.1271\n",
      "Epoch  62/200: Loss 0.3347, Train_accuracy 0.9073, Validation_accuracy 0.8820, Time 0.1291\n",
      "Epoch  63/200: Loss 0.3342, Train_accuracy 0.9027, Validation_accuracy 0.8770, Time 0.1341\n",
      "Epoch  64/200: Loss 0.3329, Train_accuracy 0.9083, Validation_accuracy 0.8760, Time 0.1341\n",
      "Epoch  65/200: Loss 0.3314, Train_accuracy 0.9100, Validation_accuracy 0.8860, Time 0.1381\n",
      "Epoch  66/200: Loss 0.3315, Train_accuracy 0.9093, Validation_accuracy 0.8750, Time 0.1371\n",
      "Epoch  67/200: Loss 0.3319, Train_accuracy 0.9097, Validation_accuracy 0.8880, Time 0.1321\n",
      "Epoch  68/200: Loss 0.3294, Train_accuracy 0.9093, Validation_accuracy 0.8800, Time 0.1261\n",
      "Epoch  69/200: Loss 0.3267, Train_accuracy 0.9133, Validation_accuracy 0.8820, Time 0.1441\n",
      "Epoch  70/200: Loss 0.3256, Train_accuracy 0.9080, Validation_accuracy 0.8910, Time 0.1341\n",
      "Epoch  71/200: Loss 0.3272, Train_accuracy 0.9103, Validation_accuracy 0.8740, Time 0.1401\n",
      "Epoch  72/200: Loss 0.3283, Train_accuracy 0.9110, Validation_accuracy 0.8870, Time 0.1541\n",
      "Epoch  73/200: Loss 0.3269, Train_accuracy 0.9127, Validation_accuracy 0.8790, Time 0.1421\n",
      "Epoch  74/200: Loss 0.3249, Train_accuracy 0.9110, Validation_accuracy 0.8840, Time 0.1301\n",
      "Epoch  75/200: Loss 0.3239, Train_accuracy 0.9117, Validation_accuracy 0.8880, Time 0.1441\n",
      "Epoch  76/200: Loss 0.3244, Train_accuracy 0.9093, Validation_accuracy 0.8760, Time 0.1311\n",
      "Epoch  77/200: Loss 0.3250, Train_accuracy 0.9120, Validation_accuracy 0.8880, Time 0.1331\n",
      "Epoch  78/200: Loss 0.3247, Train_accuracy 0.9110, Validation_accuracy 0.8820, Time 0.1341\n",
      "Epoch  79/200: Loss 0.3233, Train_accuracy 0.9123, Validation_accuracy 0.8860, Time 0.1231\n",
      "Epoch  80/200: Loss 0.3227, Train_accuracy 0.9117, Validation_accuracy 0.8860, Time 0.1291\n",
      "Epoch  81/200: Loss 0.3230, Train_accuracy 0.9140, Validation_accuracy 0.8830, Time 0.1421\n",
      "Epoch  82/200: Loss 0.3234, Train_accuracy 0.9083, Validation_accuracy 0.8900, Time 0.1261\n",
      "Epoch  83/200: Loss 0.3235, Train_accuracy 0.9090, Validation_accuracy 0.8800, Time 0.1331\n",
      "Epoch  84/200: Loss 0.3238, Train_accuracy 0.9050, Validation_accuracy 0.8840, Time 0.1381\n",
      "Epoch  85/200: Loss 0.3247, Train_accuracy 0.9073, Validation_accuracy 0.8850, Time 0.1381\n",
      "Epoch  86/200: Loss 0.3272, Train_accuracy 0.8987, Validation_accuracy 0.8640, Time 0.1301\n",
      "Epoch  87/200: Loss 0.3324, Train_accuracy 0.8967, Validation_accuracy 0.8860, Time 0.1311\n",
      "Epoch  88/200: Loss 0.3354, Train_accuracy 0.8990, Validation_accuracy 0.8710, Time 0.1341\n",
      "Epoch  89/200: Loss 0.3282, Train_accuracy 0.9093, Validation_accuracy 0.8850, Time 0.1361\n",
      "Epoch  90/200: Loss 0.3210, Train_accuracy 0.9107, Validation_accuracy 0.8860, Time 0.1331\n",
      "Epoch  91/200: Loss 0.3247, Train_accuracy 0.9007, Validation_accuracy 0.8680, Time 0.1311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  92/200: Loss 0.3277, Train_accuracy 0.9083, Validation_accuracy 0.8860, Time 0.1361\n",
      "Epoch  93/200: Loss 0.3230, Train_accuracy 0.9107, Validation_accuracy 0.8930, Time 0.1461\n",
      "Epoch  94/200: Loss 0.3204, Train_accuracy 0.9053, Validation_accuracy 0.8760, Time 0.1471\n",
      "Epoch  95/200: Loss 0.3238, Train_accuracy 0.9107, Validation_accuracy 0.8890, Time 0.1481\n",
      "Epoch  96/200: Loss 0.3236, Train_accuracy 0.9100, Validation_accuracy 0.8910, Time 0.1451\n",
      "Epoch  97/200: Loss 0.3200, Train_accuracy 0.9113, Validation_accuracy 0.8830, Time 0.1531\n",
      "Epoch  98/200: Loss 0.3212, Train_accuracy 0.9130, Validation_accuracy 0.8890, Time 0.1451\n",
      "Epoch  99/200: Loss 0.3232, Train_accuracy 0.9113, Validation_accuracy 0.8800, Time 0.1431\n",
      "Epoch 100/200: Loss 0.3207, Train_accuracy 0.9157, Validation_accuracy 0.8880, Time 0.1451\n",
      "Epoch 101/200: Loss 0.3192, Train_accuracy 0.9110, Validation_accuracy 0.8880, Time 0.1381\n",
      "Epoch 102/200: Loss 0.3210, Train_accuracy 0.9090, Validation_accuracy 0.8770, Time 0.1311\n",
      "Epoch 103/200: Loss 0.3210, Train_accuracy 0.9137, Validation_accuracy 0.8840, Time 0.1421\n",
      "Epoch 104/200: Loss 0.3192, Train_accuracy 0.9123, Validation_accuracy 0.8850, Time 0.1351\n",
      "Epoch 105/200: Loss 0.3196, Train_accuracy 0.9060, Validation_accuracy 0.8780, Time 0.1381\n",
      "Epoch 106/200: Loss 0.3206, Train_accuracy 0.9137, Validation_accuracy 0.8880, Time 0.1411\n",
      "Epoch 107/200: Loss 0.3196, Train_accuracy 0.9143, Validation_accuracy 0.8860, Time 0.1331\n",
      "Epoch 108/200: Loss 0.3186, Train_accuracy 0.9140, Validation_accuracy 0.8800, Time 0.1411\n",
      "Epoch 109/200: Loss 0.3192, Train_accuracy 0.9147, Validation_accuracy 0.8870, Time 0.1291\n",
      "Epoch 110/200: Loss 0.3197, Train_accuracy 0.9157, Validation_accuracy 0.8820, Time 0.1371\n",
      "Epoch 111/200: Loss 0.3188, Train_accuracy 0.9157, Validation_accuracy 0.8860, Time 0.1321\n",
      "Epoch 112/200: Loss 0.3184, Train_accuracy 0.9150, Validation_accuracy 0.8890, Time 0.1331\n",
      "Epoch 113/200: Loss 0.3190, Train_accuracy 0.9127, Validation_accuracy 0.8780, Time 0.1311\n",
      "Epoch 114/200: Loss 0.3192, Train_accuracy 0.9160, Validation_accuracy 0.8910, Time 0.1261\n",
      "Epoch 115/200: Loss 0.3186, Train_accuracy 0.9160, Validation_accuracy 0.8860, Time 0.1321\n",
      "Epoch 116/200: Loss 0.3183, Train_accuracy 0.9123, Validation_accuracy 0.8850, Time 0.1331\n",
      "Epoch 117/200: Loss 0.3188, Train_accuracy 0.9140, Validation_accuracy 0.8860, Time 0.1241\n",
      "Epoch 118/200: Loss 0.3190, Train_accuracy 0.9140, Validation_accuracy 0.8880, Time 0.1301\n",
      "Epoch 119/200: Loss 0.3187, Train_accuracy 0.9140, Validation_accuracy 0.8850, Time 0.1321\n",
      "Epoch 120/200: Loss 0.3186, Train_accuracy 0.9133, Validation_accuracy 0.8900, Time 0.1311\n",
      "Epoch 121/200: Loss 0.3193, Train_accuracy 0.9117, Validation_accuracy 0.8770, Time 0.1541\n",
      "Epoch 122/200: Loss 0.3204, Train_accuracy 0.9120, Validation_accuracy 0.8910, Time 0.1471\n",
      "Epoch 123/200: Loss 0.3212, Train_accuracy 0.9110, Validation_accuracy 0.8750, Time 0.1501\n",
      "Epoch 124/200: Loss 0.3222, Train_accuracy 0.9050, Validation_accuracy 0.8840, Time 0.1371\n",
      "Epoch 125/200: Loss 0.3227, Train_accuracy 0.9090, Validation_accuracy 0.8760, Time 0.1441\n",
      "Epoch 126/200: Loss 0.3220, Train_accuracy 0.9053, Validation_accuracy 0.8850, Time 0.1381\n",
      "Epoch 127/200: Loss 0.3201, Train_accuracy 0.9153, Validation_accuracy 0.8810, Time 0.1261\n",
      "Epoch 128/200: Loss 0.3175, Train_accuracy 0.9160, Validation_accuracy 0.8820, Time 0.1321\n",
      "Epoch 129/200: Loss 0.3171, Train_accuracy 0.9120, Validation_accuracy 0.8900, Time 0.1371\n",
      "Epoch 130/200: Loss 0.3181, Train_accuracy 0.9123, Validation_accuracy 0.8810, Time 0.1361\n",
      "Epoch 131/200: Loss 0.3193, Train_accuracy 0.9100, Validation_accuracy 0.8850, Time 0.1441\n",
      "Epoch 132/200: Loss 0.3191, Train_accuracy 0.9167, Validation_accuracy 0.8810, Time 0.1321\n",
      "Epoch 133/200: Loss 0.3179, Train_accuracy 0.9157, Validation_accuracy 0.8880, Time 0.1411\n",
      "Epoch 134/200: Loss 0.3169, Train_accuracy 0.9150, Validation_accuracy 0.8840, Time 0.1401\n",
      "Epoch 135/200: Loss 0.3170, Train_accuracy 0.9137, Validation_accuracy 0.8880, Time 0.1321\n",
      "Epoch 136/200: Loss 0.3177, Train_accuracy 0.9127, Validation_accuracy 0.8870, Time 0.1271\n",
      "Epoch 137/200: Loss 0.3180, Train_accuracy 0.9157, Validation_accuracy 0.8820, Time 0.1201\n",
      "Epoch 138/200: Loss 0.3178, Train_accuracy 0.9150, Validation_accuracy 0.8900, Time 0.1321\n",
      "Epoch 139/200: Loss 0.3174, Train_accuracy 0.9147, Validation_accuracy 0.8830, Time 0.1281\n",
      "Epoch 140/200: Loss 0.3171, Train_accuracy 0.9170, Validation_accuracy 0.8890, Time 0.1351\n",
      "Epoch 141/200: Loss 0.3172, Train_accuracy 0.9133, Validation_accuracy 0.8830, Time 0.1381\n",
      "Epoch 142/200: Loss 0.3183, Train_accuracy 0.9153, Validation_accuracy 0.8810, Time 0.1371\n",
      "Epoch 143/200: Loss 0.3183, Train_accuracy 0.9137, Validation_accuracy 0.8890, Time 0.1361\n",
      "Epoch 144/200: Loss 0.3183, Train_accuracy 0.9137, Validation_accuracy 0.8800, Time 0.1261\n",
      "Epoch 145/200: Loss 0.3188, Train_accuracy 0.9113, Validation_accuracy 0.8890, Time 0.1371\n",
      "Epoch 146/200: Loss 0.3188, Train_accuracy 0.9133, Validation_accuracy 0.8770, Time 0.1301\n",
      "Epoch 147/200: Loss 0.3184, Train_accuracy 0.9140, Validation_accuracy 0.8900, Time 0.1351\n",
      "Epoch 148/200: Loss 0.3183, Train_accuracy 0.9123, Validation_accuracy 0.8800, Time 0.1321\n",
      "Epoch 149/200: Loss 0.3177, Train_accuracy 0.9143, Validation_accuracy 0.8880, Time 0.1341\n",
      "Epoch 150/200: Loss 0.3178, Train_accuracy 0.9150, Validation_accuracy 0.8840, Time 0.1341\n",
      "Epoch 151/200: Loss 0.3172, Train_accuracy 0.9147, Validation_accuracy 0.8860, Time 0.1281\n",
      "Epoch 152/200: Loss 0.3172, Train_accuracy 0.9157, Validation_accuracy 0.8860, Time 0.1251\n",
      "Epoch 153/200: Loss 0.3168, Train_accuracy 0.9163, Validation_accuracy 0.8840, Time 0.1381\n",
      "Epoch 154/200: Loss 0.3167, Train_accuracy 0.9157, Validation_accuracy 0.8880, Time 0.1331\n",
      "Epoch 155/200: Loss 0.3169, Train_accuracy 0.9170, Validation_accuracy 0.8840, Time 0.1311\n",
      "Epoch 156/200: Loss 0.3167, Train_accuracy 0.9163, Validation_accuracy 0.8880, Time 0.1201\n",
      "Epoch 157/200: Loss 0.3169, Train_accuracy 0.9177, Validation_accuracy 0.8830, Time 0.1261\n",
      "Epoch 158/200: Loss 0.3169, Train_accuracy 0.9173, Validation_accuracy 0.8880, Time 0.1311\n",
      "Epoch 159/200: Loss 0.3170, Train_accuracy 0.9160, Validation_accuracy 0.8840, Time 0.1351\n",
      "Epoch 160/200: Loss 0.3167, Train_accuracy 0.9150, Validation_accuracy 0.8910, Time 0.1241\n",
      "Epoch 161/200: Loss 0.3169, Train_accuracy 0.9150, Validation_accuracy 0.8820, Time 0.1281\n",
      "Epoch 162/200: Loss 0.3167, Train_accuracy 0.9137, Validation_accuracy 0.8880, Time 0.1271\n",
      "Epoch 163/200: Loss 0.3169, Train_accuracy 0.9163, Validation_accuracy 0.8840, Time 0.1251\n",
      "Epoch 164/200: Loss 0.3167, Train_accuracy 0.9143, Validation_accuracy 0.8880, Time 0.1281\n",
      "Epoch 165/200: Loss 0.3169, Train_accuracy 0.9177, Validation_accuracy 0.8840, Time 0.1421\n",
      "Epoch 166/200: Loss 0.3169, Train_accuracy 0.9153, Validation_accuracy 0.8890, Time 0.1291\n",
      "Epoch 167/200: Loss 0.3169, Train_accuracy 0.9147, Validation_accuracy 0.8800, Time 0.1331\n",
      "Epoch 168/200: Loss 0.3169, Train_accuracy 0.9143, Validation_accuracy 0.8910, Time 0.1361\n",
      "Epoch 169/200: Loss 0.3171, Train_accuracy 0.9140, Validation_accuracy 0.8770, Time 0.1321\n",
      "Epoch 170/200: Loss 0.3175, Train_accuracy 0.9130, Validation_accuracy 0.8910, Time 0.1191\n",
      "Epoch 171/200: Loss 0.3179, Train_accuracy 0.9153, Validation_accuracy 0.8770, Time 0.1311\n",
      "Epoch 172/200: Loss 0.3186, Train_accuracy 0.9110, Validation_accuracy 0.8890, Time 0.1301\n",
      "Epoch 173/200: Loss 0.3188, Train_accuracy 0.9127, Validation_accuracy 0.8780, Time 0.1331\n",
      "Epoch 174/200: Loss 0.3192, Train_accuracy 0.9063, Validation_accuracy 0.8890, Time 0.1291\n",
      "Epoch 175/200: Loss 0.3195, Train_accuracy 0.9103, Validation_accuracy 0.8800, Time 0.1201\n",
      "Epoch 176/200: Loss 0.3200, Train_accuracy 0.9027, Validation_accuracy 0.8770, Time 0.1361\n",
      "Epoch 177/200: Loss 0.3215, Train_accuracy 0.9077, Validation_accuracy 0.8870, Time 0.1321\n",
      "Epoch 178/200: Loss 0.3233, Train_accuracy 0.9027, Validation_accuracy 0.8640, Time 0.1241\n",
      "Epoch 179/200: Loss 0.3257, Train_accuracy 0.9053, Validation_accuracy 0.8850, Time 0.1331\n",
      "Epoch 180/200: Loss 0.3260, Train_accuracy 0.9043, Validation_accuracy 0.8680, Time 0.1291\n",
      "Epoch 181/200: Loss 0.3216, Train_accuracy 0.9153, Validation_accuracy 0.8900, Time 0.1351\n",
      "Epoch 182/200: Loss 0.3167, Train_accuracy 0.9120, Validation_accuracy 0.8830, Time 0.1241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 183/200: Loss 0.3162, Train_accuracy 0.9047, Validation_accuracy 0.8710, Time 0.1401\n",
      "Epoch 184/200: Loss 0.3200, Train_accuracy 0.9097, Validation_accuracy 0.8860, Time 0.1411\n",
      "Epoch 185/200: Loss 0.3210, Train_accuracy 0.9083, Validation_accuracy 0.8750, Time 0.1431\n",
      "Epoch 186/200: Loss 0.3179, Train_accuracy 0.9167, Validation_accuracy 0.8880, Time 0.1331\n",
      "Epoch 187/200: Loss 0.3151, Train_accuracy 0.9150, Validation_accuracy 0.8860, Time 0.1491\n",
      "Epoch 188/200: Loss 0.3162, Train_accuracy 0.9083, Validation_accuracy 0.8750, Time 0.1491\n",
      "Epoch 189/200: Loss 0.3189, Train_accuracy 0.9107, Validation_accuracy 0.8890, Time 0.1551\n",
      "Epoch 190/200: Loss 0.3189, Train_accuracy 0.9117, Validation_accuracy 0.8830, Time 0.1331\n",
      "Epoch 191/200: Loss 0.3165, Train_accuracy 0.9177, Validation_accuracy 0.8890, Time 0.1551\n",
      "Epoch 192/200: Loss 0.3153, Train_accuracy 0.9143, Validation_accuracy 0.8890, Time 0.1571\n",
      "Epoch 193/200: Loss 0.3169, Train_accuracy 0.9103, Validation_accuracy 0.8740, Time 0.1491\n",
      "Epoch 194/200: Loss 0.3181, Train_accuracy 0.9143, Validation_accuracy 0.8890, Time 0.1361\n",
      "Epoch 195/200: Loss 0.3174, Train_accuracy 0.9157, Validation_accuracy 0.8850, Time 0.1406\n",
      "Epoch 196/200: Loss 0.3159, Train_accuracy 0.9153, Validation_accuracy 0.8850, Time 0.1251\n",
      "Epoch 197/200: Loss 0.3160, Train_accuracy 0.9143, Validation_accuracy 0.8900, Time 0.1251\n",
      "Epoch 198/200: Loss 0.3171, Train_accuracy 0.9113, Validation_accuracy 0.8830, Time 0.1351\n",
      "Epoch 199/200: Loss 0.3174, Train_accuracy 0.9140, Validation_accuracy 0.8890, Time 0.1371\n",
      "Epoch 200/200: Loss 0.3167, Train_accuracy 0.9163, Validation_accuracy 0.8890, Time 0.1261\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6709309c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
