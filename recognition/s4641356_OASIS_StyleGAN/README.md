# StyleGAN for OASIS dataset

## Author
Jack Napier (46413569)

## Overview
### Background
#### Generative Adversarial Networks:
Generative Adversarial Networks (GAN's) are an approach to generative neural networks that conducts unsupervised learning by employing a secondary discriminatory network to supervise the and label the plausibility of the generator's outputs.
A GAN is typically comprised of a generator G, which takes in a latent vector z and returns generated information G(z), and a discriminator D, which takes in either a real sample X or a generated sample G(z), and (depending on implementation) returns either a classification of the input (real or generated) or the probability the input is real.

In this way, loss is calculated for the generator G by composing it with D, and taking the loss of the *inverse* of the output of D(G(z)). That is, a higher loss is computed the less real the discriminator determines G(z). As D is trained to evaluate only true samples as real, By backpropagating this calculated loss from D(G) through only the weights of G, G is trained to output data that closely mimicks the original sample data.

However, this training of G requires D to be trained to distinguish between real and generated samples. This discriminator cannot simply be pretrained to distinguish between a latent space and real samples, as it would then provide minimal/no information for the generator to approve (effectively dictating yes/no and leaving the generator to attempt to randomly walk onto a correct set of weightings). The discriminator then needs to be trained to discriminate between real samples, and samples generated by the current generator, in order to provide effective feedback during backpropagation as to what particular features generated samples are causing the discriminator to return fake. The generator will be constantly adapting during training and thus the discriminator must adapt with it to continue to provide feedback.

As such we set up D and G to train 'adversarily' along side one another. Iteratively: the discriminator will learn which features primarily distinguish between real and generated samples, and then the generator will train against a fixed iteration of this trained discriminator to learn how to mimick these features. The discriminator will then learn new, generally finer features, and the cycle continues.

We thus set up the GAN in the following configuration:
<p align="center">
    <img src="resources/GAN im1.png" alt = "GAN structure" title = "GAN structure"/> TODO rename image
</p>

The GAN training paradigm is as follows:

- Take a batch of real samples, and train the discriminator to report a high "realness" score when it sees such an image.
- Generate an equivilantly sized batch of fake image using the generator and its current set of weights. Then, train the discrimimator to report a low "realness" score upon seeing such images.
- Train the generator to produce images that make the discriminator, now with its newly trained weights, output a high "realness" score. The generator must adapt weights as the discriminator has just been trained on its current configuration.

#### StyleGAN:
GANs are currently considered one of the most powerful generational network techniques. However, the classical approach simply involves a generator G that takes a latent vector z and outputs a generated sample G(z). This can lead to issues; because the generator synthesises features from these latent vectors, and the distribution of latent vectors may not relate to the distribution of features in the sample images. This can lead to synthesising invalid or nonexistant feature combinations, feature combinations missing in the latent space, and/or a necessarily overcomplicated mapping from the latent space to output samples.

StyleGAN attempts to combat this issue in the context of image generation with a revolutionary new architecture. In styleGAN the input latent vector z is first processed through a feature selection network f such that the distribution of w = f(z) closely relates to the distribution of features in the sample images. The generator then utilises a synthesis network g, which starts with a stack of constant values (an image per channel) that are convolved and upscaled to eventually form an image, with w informing the scaling and biasing at each resolution through the process of Adaptive Instance Normalization (AdaIN) given by the following:
<p align="center">
    <img src="resources/adaIn.png" alt = "adaIn equation" title = "adaIn equation"/> TODO rename image
</p>
With this constuction, the vector w of features (which is distributed desirably) directly informs the selection of features, from the most macroscopic while the resolution is low, to more fine details as the image approaches its target resolution. Noise is also introduced at each stage of generation to control the finest details, such as freckles on faces.
The difference between a traditional GAN generator and a styleGAN generator is illustrated below:
<p align="center">
    <img src="resources/styleGAN.png" alt = "styleGAN architecture" title = "architecture"/> TODO rename image
</p>https://arxiv.org/pdf/1812.04948.pdf TODO source
An additional benefit of this generator architecture, is that multiple mapping networks can be set up, giving various w's assotiated with different training sets. These w can be distributed to different layers of the synthesis network, essentially giving the output image the macroscopic features of one training set, mixed with finer features from other training set(s). This blending of styles is where the styleGAN gets its name. However this particular feature was not required to solve the task at hand.

 

### Implementation
A styleGAN was implemented to generate images of the OASIS dataset of brain scans. The project was undertaken with the convenient training environment (local machine with RTX2070) having access 8GB of VRAM. As such, to produce the best results the sample dataset was heaviliy preprocessed in `dataset.py` before being input to the model. The original sample images were 256x256 RGBA images. However these images were for intents and purposes greyscale, and posessed a significant monochrome padded border around the brain. Thus in order to minimise the amount of redundant information the styleGAN had to learn, the Images were flattened to single channel greyscale and then cropped. Furthermore, the images were quite simple, composed primarily of macroscopic features. As the number of weights required to learn increases exponentially with image size, the images were compressed to 32x32 before training, with generator outputs later upsampled (with the monocrome border being padded back in) back to the original image size. This allowed the training to comfortably run with a satisfactory latent dimension and batch size on the availible training environment. The image compression was not significant enough that the styleGAN failed to learn the features of the sample images. The images were then converted into 2x2 arrays of pixel intensities, and normalised to the range [0,1]. After this, the mean of the dataset was taken and subtracted to center the data around 0. This does mean that this mean needs to be kept track of by the model through training in order to produce an image from the generator output.However it was found that if this was not done training would collapse as the discriminator would quickly become easily able to distinguish between generated and real images with 0 loss, likely due to the random generator inputs being centered at 0.

The styleGAN was implemented from scratch, with the discriminator and generator architecture definied in `modules.py` following the above architecture for the generator, and taking inspiration from https://keras.io/examples/generative/stylegan/ for the discriminator architecture TODO reference this?. Keras layers were used to define the architecture, with two custom layers defined to conduct learnt scaling weight noise addition and the adaIn procedure. However the resultant keras models were not compiled, as training was undertaken using a custom script described below. The styleGAN training was extremely chaotic and it was hard to achieve stability. In addition to constant refinement to the model architecture, the learning rates of the discriminator and generator played a key role in the stability of training. If the discriminator's learning rate is not sufficiently below that of the generator, it eventually overtook the generator to the extent of zero loss and training collapsed. On the other hand if the generator's learning rate was too great relative to that of the discriminator, the generator overtook the discriminator to the extent of zero loss and training collapsed. If learning rates overall were too slow the discriminator was unable to provide sufficient feedback to advance training, and if the learning rates were overall too high the generator's training diverged. Both of these cases resulted in nothing but noise. 
The final learning rates settled on were:
- Generator: 0.00025
- Discriminator: 0.0002

And these provided stable training.
Finally, it was found that training always collapsed around 30 epochs in. The constant base of the synthesis network was changed from 0's to 1's. This change appeared to provide indefinite stable training, with the generator almost collapsing at the same point, but recovering with no future episodes. This effect is evident in the training history plot below. It is theorised that the collapsing was due to 0's nullifying the effect of the initial few feature scaleing multiplications.

The styleGAN training paradigm was custom written and defined in `train.py`. The training procedure was as described for GAN's above. Initially the discriminator was designed to conduct straight classification of images as real (1) or fake (0), using binary-cross-entropy as a loss function. This allowed the keras train_on_batch method to be employed, however training performance was poor and resultant images failed to capture even the rough shape of the desired brains. It was found that this method of discrimination was insufficient and the training procedure was rewritten to manually conduct training, compute loss, and backpropagate using GradientTapes. This allowed for the use of soft-plus as a loss function for the model. softplus sums the results of a batch (in this case discriminator realness scores) but with a curve that assymtotes towards zero for negative values. This allows for a

## Dependencies
Python version: 3.10.4

In addition to the standard Python libraries, the following software packages are required:
|    Package   |  Version |                                        Installation                                       |
|:------------:|:--------:|:-----------------------------------------------------------------------------------------:|
| `Numpy`      | 1.22.3   | `pip install numpy`                                                                       |
| `TensorFlow` | 2.9.2    | `pip install tensorflow`                                                                  |
| `CUDA`       | 11.2     | [Installation Guide](https://docs.nvidia.com/cuda/index.html#installation-guides)         |
| `CUDNN`      | 8.4.1    | [Installation Guide](https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html) |

This project is no-longer being actively maintained for newer versions of these dependencies.

## Requirements
Training the styleGAN in its current configuration has the following hardware requirements:
|        |                 Required                | Development Environment |
|--------|:---------------------------------------:|:-----------------------:|
| Memory | 10GB RAM                                | 32GB DDR4 RAM           | 
| GPU    | CUDA/CUDNN Compatible GPU With 8GB VRAM | NVIDIA RTX 2070 8GB     | 
|        |                                         |                         | 

The VRAM requirement can be reduced by reducing the resolution of the compressed images (Achieved by lowering the `COMPRESSION_SIZE` constant in the train method), However the GPU used must still be CUDA and CUDNN compatible.

## Results


## Usage
The individual helper functions included are documented in their respective files.

 To train a styleGAN for the OASIS dataset, one should run the `train` function included in `train.py`. The `train` function takes 3 arguments:
 - `model`: the folder containing the StyleGAN
 - `image_source`: the folder containing the training images
 - `epochs`: the number of epochs to train for

 If you wish to intialise and train a new styleGAN model from scratch, pass in `model = None`. This will generate a new styleGAN, which will be saved in the working directory inside the folder `model\`. This new styleGAN will be configured to train on images compressed to 64x64, with a latent dimension of 512 and using a synthesis network that begins with 4x4 resolution. If you wish to initialise a styleGAN with alternate parameters, you can modify the `COMPRESSION_SIZE`, `LATENT_DIM`, and `GENERATOR_INIT_SIZE` constants provided at the beginning of the function. Note that `COMPRESSION_SIZE` will also need to be set apropriately if training on a pre-existing model with an alternate output size and a training image cache has not been built yet.

 Upon first running `train`, an image cache is constructed containing the normalised numerical data of the compressed images. This cache will be used as the training set while it exists in the working directory.

 Upon completion of the requested training epochs, a plot is generated of the training losses. Training appends history to a `history.csv` in the working directly so consecutive runs of `train` will generate plots containing the information from previous training sessions.
 If one wishes to plot a specific range of training epochs, one can run the `plot_history` function located in `GANUtils.py` directly, passing in the desired epoch range as the final argument.
 
 The generated training history plot is displayed in its own window and saved to the working directory as `training_loss.png`

 After each epoch, a sample of 5 generated images is saved to the `output` folder in the working directory. If one wishes to modify the number of samples taken per epoch, one can modify the `IMAGE_SAMPLE_COUNT` constant located at the beginning of the `train` function.

 If one wishes to configure a different default location for these working components, one can modify the `TRAINING_HISTORY_FILE`, `IMAGE_SAMPLE_FOLDER`, `DEFAULT_MODEL_LOC` and `HISTORY_PLOT_FILE` constants included in the `train` function.

Training is configured to run with a batch size of 64, as this was the maximum achievible with the VRAM availible (8GB). In order to configure this yourself, modify the `BATCH_SIZE` constant located at the top of the `train` function.

This will result in two plots being displayed, one at 10 epochs, the other containing the full 20. The latter will be saved to disk and the `output/` folder will contain 20 sets of samples.

To generate images from a trained styleGAN, one should run the ` generate_images` function provided in `predict.py`
The `generate_images` function takes 3 arguments:
 - `model_folder`: the folder containing the StyleGAN from which to generate the sample images
 - `num_images`: the number of images to generate
 - `save_folder`: the folder to save the generated images

 If `save_folder = None` is passed, the images are not saved to disk, but they are still generated and rendered in their own window to the user.
### Examples:
```
from train import train
from predict import generate_images

#Generate new styleGAN and train for 10 epochs
train( 
    model = None, 
    image_source = "images/", 
    epochs = 10
    )

#train the styleGAN for a further 10 epochs (20 total)
train( 
    model = "model/", 
    image_source = "images/", 
    epochs = 10
    )

#Generate and display 5 sample images from the trained model
generate_images(
    model_folder = "model/"
    num_images = 5
    save_folder = None
    )

#Generate and display 5 more samples, saving to "samples"
generate_images(
    model_folder = "model/"
    num_images = 5
    save_folder = "samples/"
    )
```
## References
Generational Adversarial Networks: https://arxiv.org/pdf/1406.2661.pdf TODO how to refrence
StyleGAN Theory: 
StyleGAN Architecture: https://learn.uq.edu.au/webapps/blackboard/execute/displayLearningUnit?course_id=_160415_1&content_id=_8288357_1&framesetWrapped=true
https://arxiv.org/pdf/1912.04958.pdf
https://arxiv.org/pdf/1812.04948.pdf
