{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViT4Brain\n",
    "\n",
    "### Problem definition\n",
    "\n",
    "This project utilize Vision <a href='https://arxiv.org/abs/2010.11929'>Transformer(ViT)</a> to classify Alzheimerâ€™s disease (normal and AD) of the <a href='http://adni.loni.usc.edu/'>ADNI brain data</a>. The model will take the ANDI images as input and predict whether they belongs to AD or NC.\n",
    "\n",
    "### Model exaplaination\n",
    "\n",
    "The input image will first be splitted into fixed-size pactches, with posional embeddings obtained by linear projection. And then, the patches will be fed into transformer encoder, afther that a multilayer perceptron(MLP) will handle the output to perfrom final clssification.\n",
    "\n",
    "<img src=\"./images/vit.gif\" width=\"500px\"></img>\n",
    "\n",
    "\n",
    "The original idea of <a href='https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf'>transfomer</a> can be illustrated as follow, it utilizes multi-headed self-attention which can handle sequence data effectively by focusing on a certain part of the input.\n",
    "\n",
    "\n",
    "<img src=\"./images/transformer.png\" width=\"500px\"></img>\n",
    "\n",
    "(The images are from the original paper.)\n",
    "\n",
    "### Data preparation\n",
    "Put the unzipped preprocessed image files in the `datasets` folder. The path of the dataset is supposed to be like `./datasets/AD_NC/train/AD`\n",
    "\n",
    "\n",
    "***Training/Validation/Testing Set Spliting***\n",
    "Please note that  people from the same ID would only appear in either the train or validation set during training.\n",
    "\n",
    "* The test images follows the original download file folder struture.\n",
    "* There are 10400 AD images and 11120 NC images in the train folder. \n",
    "* The original train images further split into into train and validation sets, using approximatly 20% of the images for validation.\n",
    "* The junction datapoints where training and validation set splitting is manualy checked , to make sure not cover one patient's images in both sets.\n",
    "\n",
    "***Preprecessing***\n",
    "\n",
    "The training image will be resized to 224 * 224 size, and then  trasformed with data augumentation. The valid and test images will be resized to 224 * 224 size.\n",
    "### Required Dependencies:\n",
    "\n",
    "* Python 3.6+\n",
    "* torch==1.11.0\n",
    "* torchvision==0.12.0\n",
    "* tensorboard==2.9.1\n",
    "* pandas==1.4.2\n",
    "* numpy==1.21.5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Usage\n",
    "```bash\n",
    "usage: main.py [-h] [--batch_size BATCH_SIZE] [--dim DIM] \n",
    "                [--lr LR] [--depth DEPTH][--heads HEADS] [--epochs EPOCHS] [--mlp_dim MLP_DIM]\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  --batch_size BATCH_SIZE\n",
    "                        batch size\n",
    "  --dim DIM             neural network dimension\n",
    "  --lr LR               learning rate of the optimizer\n",
    "  --depth DEPTH         depth of the network\n",
    "  --heads HEADS         number of heads in the multihead attention\n",
    "  --epochs EPOCHS       number of epochs to train\n",
    "  --mlp_dim MLP_DIM     dimension of the mlp on top of the attention block\n",
    "```\n",
    "\n",
    "Example scripts:\n",
    "```bash\n",
    "python main.py --mode=train --model=ViT --depth=8 --mlp_dim=1024\n",
    "python main.py --mode=test\n",
    "python main.py --mode=pred\n",
    "```\n",
    "\n",
    "### Results\n",
    "Use below script to see the result loged by tensorboard at http://localhost:6008/.\n",
    "```\n",
    "tensorboard --logdir=./runs   \n",
    "```\n",
    "The plot of loss and accuracy during training and validation can been seen as follow:\n",
    "\n",
    "<img src=\"./images/train_valid_plot.png\" width=\"500px\"></img>\n",
    "\n",
    "Unfortunately even the accuracy of training and validation is apporaching to 100%, the testing accuracy is only **$0.643$**. The first thought came to my mind is overfitting. However, to reduce the influence of overfittly as much as possible, data augmentation and dropout has been considered. For data augmentation, RandomHorizontalFlip, RandomVerticalFlip, and RandomRotation have been used. And the model is trained with setting dropout = $0.2$.\n",
    "\n",
    "### Better accuracy using existing pacakge\n",
    "A variant of ViT, ,has been tried and finally got above **$0.758$** accuracy. The innovation of this model is using depthwise-pointwise self-attention.\n",
    "By using package from https://github.com/lucidrains/vit-pytorch, fistly  'pip install vit-pytorch'. And use pretrained model from https://drive.google.com/file/d/1EZY9jvK5CnMFLIfqHZsuY9gZX5ERbQL7/view?usp=sharing , and put it in the same folder with main.py.\n",
    "Below script can be used and the higher accuracy **$0.758$** can be obtained.\n",
    "```\n",
    "python main.py --mode=test --model=SepViT\n",
    "```\n",
    "\n",
    "### More file description:\n",
    "* main.py: hyperparameter initilization, loading dataset, model construction, the train/valid/test process.\n",
    "* train.py: train/test/valid function.\n",
    "* predict.py: predict several samples from the test set.\n",
    "* module.py: model structure of ViT.\n",
    "* './images': images used in the README\n",
    "* './runs': training loss and accuracy report. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "be96a02d3811e2a19ea5e1dae83c39640313aaefdc0e822e9fd50ff5f0caa095"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
